{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AMMICO - AI-based Media and Misinformation Content Analysis Tool","text":"<p>This package extracts data from images such as social media posts that contain an image part and a text part. The analysis can generate a very large number of features, depending on the user input. See our paper for a more in-depth description.</p> <p>This project is currently under development!</p> <p>Use pre-processed image files such as social media posts with comments and process to collect information:</p> <ol> <li>Text extraction from the images<ul> <li>Language detection</li> <li>Translation into English or other languages</li> </ul> </li> <li>Content extraction from the images<ul> <li>Textual summary of the image content (\"image caption\")</li> <li>Question answering about image content</li> </ul> </li> <li>Content extraction from videos<ul> <li>Textual summary of the video content </li> <li>Question answering about video content</li> <li>Extraction and translation of audio from the video</li> </ul> </li> <li>Color analysis<ul> <li>Analyse hue and percentage of color on image</li> </ul> </li> <li>Multimodal analysis<ul> <li>Find best matches for image content or image similarity  </li> </ul> </li> </ol>"},{"location":"#installation","title":"Installation","text":"<p>The <code>AMMICO</code> package can be installed using pip:  <pre><code>pip install ammico\n</code></pre></p> <p>Or install the development version from GitHub (currently recommended for the new features):</p> <p><pre><code>pip install git+https://github.com/ssciwr/AMMICO.git\n</code></pre> This will install the package and its dependencies locally. </p>"},{"location":"#usage","title":"Usage","text":"<p>The main demonstration notebook can be found in the <code>docs/tutorials</code> folder and also on google colab: </p> <p></p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to the ammico project! If you'd like to help improve the tool, add new features, or report or fix bugs, please follow these guidelines.</p>"},{"location":"#reporting-issues","title":"Reporting Issues","text":"<p>Please use the issues tab to report bugs, request features, or start discussions.</p>"},{"location":"#license","title":"License","text":"<p>ammico is licensed under the MIT license.</p>"},{"location":"#citing-ammico","title":"Citing ammico","text":"<p>Ammico has been published in Comp. Comm. Res., please cite the paper as specified in the Citations file.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to ammico","text":"<p>Welcome to <code>ammico</code>! Contributions to the package are welcome. Please adhere to the following conventions:</p> <ul> <li>fork the repository, make your changes, and make sure your changes pass all the tests (Sonarcloud, unit and integration tests, codecoverage limits); then open a Pull Request for your changes. Tag one of <code>ammico</code>'s developers for review.</li> <li>install and use the pre-commit hooks by running <code>pre-commit install</code> in the repository directory so that all your changes adhere to the PEP8 style guide and black code formatting</li> <li>make sure to update the documentation if applicable</li> </ul> <p>The tests are located in <code>ammico/tests</code>. Unit tests are named <code>test</code> following an underscore and the name of the module; inside the unit test modules, each test function is named <code>test</code> followed by an underscore and the name of the function/method that is being tested.</p> <p>To report bugs and issues, please open an issue describing what you did, what you expected to happen, and what actually happened. Please provide information about the environment as well as OS.</p> <p>For any questions and comments, feel free to post to our Discussions forum.</p> <p>Thank you for contributing to <code>ammico</code>!</p>"},{"location":"CONTRIBUTING/#templates","title":"Templates","text":""},{"location":"CONTRIBUTING/#template-for-pull-requests","title":"Template for pull requests","text":"<ul> <li> <p>issues that are addressed by this PR: [For example, this closes #33 or this addresses #29]</p> </li> <li> <p>changes that were made: [For example, updated version of dependencies or added a file type for input reading]</p> </li> <li> <p>if applicable: Follow-up work that is required</p> </li> </ul>"},{"location":"CONTRIBUTING/#template-for-bug-report","title":"Template for bug report","text":"<ul> <li> <p>what I did:</p> </li> <li> <p>what I expected:</p> </li> <li> <p>what actually happened:</p> </li> <li> <p>Python version and environment:</p> </li> <li> <p>Operating system:</p> </li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#text","title":"Text","text":""},{"location":"api/#ammico.text.TextAnalyzer","title":"<code>TextAnalyzer</code>","text":"<p>Used to get text from a csv and then run the TextDetector on it.</p> Source code in <code>ammico/text.py</code> <pre><code>class TextAnalyzer:\n    \"\"\"Used to get text from a csv and then run the TextDetector on it.\"\"\"\n\n    def __init__(\n        self, csv_path: str, column_key: str = None, csv_encoding: str = \"utf-8\"\n    ) -&gt; None:\n        \"\"\"Init the TextTranslator class.\n\n        Args:\n            csv_path (str): Path to the CSV file containing the text entries.\n            column_key (str): Key for the column containing the text entries.\n                Defaults to None.\n            csv_encoding (str): Encoding of the CSV file. Defaults to \"utf-8\".\n        \"\"\"\n        self.csv_path = csv_path\n        self.column_key = column_key\n        self.csv_encoding = csv_encoding\n        self._check_valid_csv_path()\n        self._check_file_exists()\n        if not self.column_key:\n            print(\"No column key provided - using 'text' as default.\")\n            self.column_key = \"text\"\n        if not self.csv_encoding:\n            print(\"No encoding provided - using 'utf-8' as default.\")\n            self.csv_encoding = \"utf-8\"\n        if not isinstance(self.column_key, str):\n            raise ValueError(\"The provided column key is not a string.\")\n        if not isinstance(self.csv_encoding, str):\n            raise ValueError(\"The provided encoding is not a string.\")\n\n    def _check_valid_csv_path(self):\n        if not isinstance(self.csv_path, str):\n            raise ValueError(\"The provided path to the CSV file is not a string.\")\n        if not self.csv_path.endswith(\".csv\"):\n            raise ValueError(\"The provided file is not a CSV file.\")\n\n    def _check_file_exists(self):\n        try:\n            with open(self.csv_path, \"r\") as file:  # noqa\n                pass\n        except FileNotFoundError:\n            raise FileNotFoundError(\"The provided CSV file does not exist.\")\n\n    def read_csv(self) -&gt; dict:\n        \"\"\"Read the CSV file and return the dictionary with the text entries.\n\n        Returns:\n            dict: The dictionary with the text entries.\n        \"\"\"\n        df = pd.read_csv(self.csv_path, encoding=self.csv_encoding)\n\n        if self.column_key not in df:\n            raise ValueError(\n                \"The provided column key is not in the CSV file. Please check.\"\n            )\n        self.mylist = df[self.column_key].to_list()\n        self.mydict = {}\n        for i, text in enumerate(self.mylist):\n            self.mydict[self.csv_path + \"row-\" + str(i)] = {\n                \"filename\": self.csv_path,\n                \"text\": text,\n            }\n</code></pre>"},{"location":"api/#ammico.text.TextAnalyzer.__init__","title":"<code>__init__(csv_path, column_key=None, csv_encoding='utf-8')</code>","text":"<p>Init the TextTranslator class.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str</code> <p>Path to the CSV file containing the text entries.</p> required <code>column_key</code> <code>str</code> <p>Key for the column containing the text entries. Defaults to None.</p> <code>None</code> <code>csv_encoding</code> <code>str</code> <p>Encoding of the CSV file. Defaults to \"utf-8\".</p> <code>'utf-8'</code> Source code in <code>ammico/text.py</code> <pre><code>def __init__(\n    self, csv_path: str, column_key: str = None, csv_encoding: str = \"utf-8\"\n) -&gt; None:\n    \"\"\"Init the TextTranslator class.\n\n    Args:\n        csv_path (str): Path to the CSV file containing the text entries.\n        column_key (str): Key for the column containing the text entries.\n            Defaults to None.\n        csv_encoding (str): Encoding of the CSV file. Defaults to \"utf-8\".\n    \"\"\"\n    self.csv_path = csv_path\n    self.column_key = column_key\n    self.csv_encoding = csv_encoding\n    self._check_valid_csv_path()\n    self._check_file_exists()\n    if not self.column_key:\n        print(\"No column key provided - using 'text' as default.\")\n        self.column_key = \"text\"\n    if not self.csv_encoding:\n        print(\"No encoding provided - using 'utf-8' as default.\")\n        self.csv_encoding = \"utf-8\"\n    if not isinstance(self.column_key, str):\n        raise ValueError(\"The provided column key is not a string.\")\n    if not isinstance(self.csv_encoding, str):\n        raise ValueError(\"The provided encoding is not a string.\")\n</code></pre>"},{"location":"api/#ammico.text.TextAnalyzer.read_csv","title":"<code>read_csv()</code>","text":"<p>Read the CSV file and return the dictionary with the text entries.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary with the text entries.</p> Source code in <code>ammico/text.py</code> <pre><code>def read_csv(self) -&gt; dict:\n    \"\"\"Read the CSV file and return the dictionary with the text entries.\n\n    Returns:\n        dict: The dictionary with the text entries.\n    \"\"\"\n    df = pd.read_csv(self.csv_path, encoding=self.csv_encoding)\n\n    if self.column_key not in df:\n        raise ValueError(\n            \"The provided column key is not in the CSV file. Please check.\"\n        )\n    self.mylist = df[self.column_key].to_list()\n    self.mydict = {}\n    for i, text in enumerate(self.mylist):\n        self.mydict[self.csv_path + \"row-\" + str(i)] = {\n            \"filename\": self.csv_path,\n            \"text\": text,\n        }\n</code></pre>"},{"location":"api/#ammico.text.TextDetector","title":"<code>TextDetector</code>","text":"<p>               Bases: <code>AnalysisMethod</code></p> Source code in <code>ammico/text.py</code> <pre><code>class TextDetector(AnalysisMethod):\n    def __init__(\n        self,\n        subdict: dict,\n        skip_extraction: bool = False,\n        accept_privacy: str = \"PRIVACY_AMMICO\",\n    ) -&gt; None:\n        \"\"\"Init text detection class.\n\n        Args:\n            subdict (dict): Dictionary containing file name/path, and possibly previous\n                analysis results from other modules.\n            skip_extraction (bool, optional): Decide if text will be extracted from images or\n                is already provided via a csv. Defaults to False.\n            accept_privacy (str, optional): Environment variable to accept the privacy\n                statement for the Google Cloud processing of the data. Defaults to\n                \"PRIVACY_AMMICO\".\n        \"\"\"\n        super().__init__(subdict)\n        # disable this for now\n        # maybe it would be better to initialize the keys differently\n        # the reason is that they are inconsistent depending on the selected\n        # options, and also this may not be really necessary and rather restrictive\n        # self.subdict.update(self.set_keys())\n        self.accepted = privacy_disclosure(accept_privacy)\n        if not self.accepted:\n            raise ValueError(\n                \"Privacy disclosure not accepted - skipping text detection.\"\n            )\n        self.translator = Translator(raise_exception=True)\n        self.skip_extraction = skip_extraction\n        if not isinstance(skip_extraction, bool):\n            raise ValueError(\"skip_extraction needs to be set to true or false\")\n        if self.skip_extraction:\n            print(\"Skipping text extraction from image.\")\n            print(\"Reading text directly from provided dictionary.\")\n        self._initialize_spacy()\n\n    def set_keys(self) -&gt; dict:\n        \"\"\"Set the default keys for text analysis.\n\n        Returns:\n            dict: The dictionary with default text keys.\n        \"\"\"\n        params = {\"text\": None, \"text_language\": None, \"text_english\": None}\n        return params\n\n    def _initialize_spacy(self):\n        \"\"\"Initialize the Spacy library for text analysis.\"\"\"\n        try:\n            self.nlp = spacy.load(\"en_core_web_md\")\n        except Exception:\n            spacy.cli.download(\"en_core_web_md\")\n            self.nlp = spacy.load(\"en_core_web_md\")\n\n    def _check_add_space_after_full_stop(self):\n        \"\"\"Add a space after a full stop. Required by googletrans.\"\"\"\n        # we have found text, now we check for full stops\n        index_stop = [i.start() for i in re.finditer(r\"\\.\", self.subdict[\"text\"])]\n        if not index_stop:  # no full stops found\n            return\n        # check if this includes the last string item\n        end_of_list = False\n        if len(self.subdict[\"text\"]) &lt;= (index_stop[-1] + 1):\n            # the last found full stop is at the end of the string\n            # but we can include all others\n            if len(index_stop) == 1:\n                end_of_list = True\n            else:\n                index_stop.pop()\n        if end_of_list:  # only one full stop at end of string\n            return\n        # if this is not the end of the list, check if there is a space after the full stop\n        no_space = [i for i in index_stop if self.subdict[\"text\"][i + 1] != \" \"]\n        if not no_space:  # all full stops have a space after them\n            return\n        # else, amend the text\n        add_one = 1\n        for i in no_space:\n            self.subdict[\"text\"] = (\n                self.subdict[\"text\"][: i + add_one]\n                + \" \"\n                + self.subdict[\"text\"][i + add_one :]\n            )\n            add_one += 1\n\n    def _truncate_text(self, max_length: int = 5000) -&gt; str:\n        \"\"\"Truncate the text if it is too long for googletrans.\"\"\"\n        if self.subdict[\"text\"] and len(self.subdict[\"text\"]) &gt; max_length:\n            print(\"Text is too long - truncating to {} characters.\".format(max_length))\n            self.subdict[\"text_truncated\"] = self.subdict[\"text\"][:max_length]\n\n    def analyse_image(self) -&gt; dict:\n        \"\"\"Perform text extraction and analysis of the text.\n\n        Returns:\n            dict: The updated dictionary with text analysis results.\n        \"\"\"\n        if not self.skip_extraction:\n            self.get_text_from_image()\n        # check that text was found\n        if not self.subdict[\"text\"]:\n            print(\"No text found - skipping analysis.\")\n        else:\n            # make sure all full stops are followed by whitespace\n            # otherwise googletrans breaks\n            self._check_add_space_after_full_stop()\n            self._truncate_text()\n            self.translate_text()\n            self.remove_linebreaks()\n            if self.subdict[\"text_english\"]:\n                self._run_spacy()\n        return self.subdict\n\n    def get_text_from_image(self):\n        \"\"\"Detect text on the image using Google Cloud Vision API.\"\"\"\n        if not self.accepted:\n            raise ValueError(\n                \"Privacy disclosure not accepted - skipping text detection.\"\n            )\n        path = self.subdict[\"filename\"]\n        try:\n            client = vision.ImageAnnotatorClient()\n        except DefaultCredentialsError:\n            raise DefaultCredentialsError(\n                \"Please provide credentials for google cloud vision API, see https://cloud.google.com/docs/authentication/application-default-credentials.\"\n            )\n        with io.open(path, \"rb\") as image_file:\n            content = image_file.read()\n        image = vision.Image(content=content)\n        # check for usual connection errors and retry if necessary\n        try:\n            response = client.text_detection(image=image)\n        except grpc.RpcError as exc:\n            print(\"Cloud vision API connection failed\")\n            print(\"Skipping this image ..{}\".format(path))\n            print(\"Connection failed with code {}: {}\".format(exc.code(), exc))\n        # here check if text was found on image\n        if response:\n            texts = response.text_annotations[0].description\n            self.subdict[\"text\"] = texts\n        else:\n            print(\"No text found on image.\")\n            self.subdict[\"text\"] = None\n        if response.error.message:\n            print(\"Google Cloud Vision Error\")\n            raise ValueError(\n                \"{}\\nFor more info on error messages, check: \"\n                \"https://cloud.google.com/apis/design/errors\".format(\n                    response.error.message\n                )\n            )\n\n    def translate_text(self):\n        \"\"\"Translate the detected text to English using the Translator object.\"\"\"\n        if not self.accepted:\n            raise ValueError(\n                \"Privacy disclosure not accepted - skipping text translation.\"\n            )\n        text_to_translate = (\n            self.subdict[\"text_truncated\"]\n            if \"text_truncated\" in self.subdict\n            else self.subdict[\"text\"]\n        )\n        try:\n            translated = self.translator.translate(text_to_translate)\n        except Exception:\n            print(\"Could not translate the text with error {}.\".format(Exception))\n            translated = None\n            print(\"Skipping translation for this text.\")\n        self.subdict[\"text_language\"] = translated.src if translated else None\n        self.subdict[\"text_english\"] = translated.text if translated else None\n\n    def remove_linebreaks(self):\n        \"\"\"Remove linebreaks from original and translated text.\"\"\"\n        if self.subdict[\"text\"] and self.subdict[\"text_english\"]:\n            self.subdict[\"text\"] = self.subdict[\"text\"].replace(\"\\n\", \" \")\n            self.subdict[\"text_english\"] = self.subdict[\"text_english\"].replace(\n                \"\\n\", \" \"\n            )\n\n    def _run_spacy(self):\n        \"\"\"Generate Spacy doc object for further text analysis.\"\"\"\n        self.doc = self.nlp(self.subdict[\"text_english\"])\n</code></pre>"},{"location":"api/#ammico.text.TextDetector.__init__","title":"<code>__init__(subdict, skip_extraction=False, accept_privacy='PRIVACY_AMMICO')</code>","text":"<p>Init text detection class.</p> <p>Parameters:</p> Name Type Description Default <code>subdict</code> <code>dict</code> <p>Dictionary containing file name/path, and possibly previous analysis results from other modules.</p> required <code>skip_extraction</code> <code>bool</code> <p>Decide if text will be extracted from images or is already provided via a csv. Defaults to False.</p> <code>False</code> <code>accept_privacy</code> <code>str</code> <p>Environment variable to accept the privacy statement for the Google Cloud processing of the data. Defaults to \"PRIVACY_AMMICO\".</p> <code>'PRIVACY_AMMICO'</code> Source code in <code>ammico/text.py</code> <pre><code>def __init__(\n    self,\n    subdict: dict,\n    skip_extraction: bool = False,\n    accept_privacy: str = \"PRIVACY_AMMICO\",\n) -&gt; None:\n    \"\"\"Init text detection class.\n\n    Args:\n        subdict (dict): Dictionary containing file name/path, and possibly previous\n            analysis results from other modules.\n        skip_extraction (bool, optional): Decide if text will be extracted from images or\n            is already provided via a csv. Defaults to False.\n        accept_privacy (str, optional): Environment variable to accept the privacy\n            statement for the Google Cloud processing of the data. Defaults to\n            \"PRIVACY_AMMICO\".\n    \"\"\"\n    super().__init__(subdict)\n    # disable this for now\n    # maybe it would be better to initialize the keys differently\n    # the reason is that they are inconsistent depending on the selected\n    # options, and also this may not be really necessary and rather restrictive\n    # self.subdict.update(self.set_keys())\n    self.accepted = privacy_disclosure(accept_privacy)\n    if not self.accepted:\n        raise ValueError(\n            \"Privacy disclosure not accepted - skipping text detection.\"\n        )\n    self.translator = Translator(raise_exception=True)\n    self.skip_extraction = skip_extraction\n    if not isinstance(skip_extraction, bool):\n        raise ValueError(\"skip_extraction needs to be set to true or false\")\n    if self.skip_extraction:\n        print(\"Skipping text extraction from image.\")\n        print(\"Reading text directly from provided dictionary.\")\n    self._initialize_spacy()\n</code></pre>"},{"location":"api/#ammico.text.TextDetector.analyse_image","title":"<code>analyse_image()</code>","text":"<p>Perform text extraction and analysis of the text.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary with text analysis results.</p> Source code in <code>ammico/text.py</code> <pre><code>def analyse_image(self) -&gt; dict:\n    \"\"\"Perform text extraction and analysis of the text.\n\n    Returns:\n        dict: The updated dictionary with text analysis results.\n    \"\"\"\n    if not self.skip_extraction:\n        self.get_text_from_image()\n    # check that text was found\n    if not self.subdict[\"text\"]:\n        print(\"No text found - skipping analysis.\")\n    else:\n        # make sure all full stops are followed by whitespace\n        # otherwise googletrans breaks\n        self._check_add_space_after_full_stop()\n        self._truncate_text()\n        self.translate_text()\n        self.remove_linebreaks()\n        if self.subdict[\"text_english\"]:\n            self._run_spacy()\n    return self.subdict\n</code></pre>"},{"location":"api/#ammico.text.TextDetector.get_text_from_image","title":"<code>get_text_from_image()</code>","text":"<p>Detect text on the image using Google Cloud Vision API.</p> Source code in <code>ammico/text.py</code> <pre><code>def get_text_from_image(self):\n    \"\"\"Detect text on the image using Google Cloud Vision API.\"\"\"\n    if not self.accepted:\n        raise ValueError(\n            \"Privacy disclosure not accepted - skipping text detection.\"\n        )\n    path = self.subdict[\"filename\"]\n    try:\n        client = vision.ImageAnnotatorClient()\n    except DefaultCredentialsError:\n        raise DefaultCredentialsError(\n            \"Please provide credentials for google cloud vision API, see https://cloud.google.com/docs/authentication/application-default-credentials.\"\n        )\n    with io.open(path, \"rb\") as image_file:\n        content = image_file.read()\n    image = vision.Image(content=content)\n    # check for usual connection errors and retry if necessary\n    try:\n        response = client.text_detection(image=image)\n    except grpc.RpcError as exc:\n        print(\"Cloud vision API connection failed\")\n        print(\"Skipping this image ..{}\".format(path))\n        print(\"Connection failed with code {}: {}\".format(exc.code(), exc))\n    # here check if text was found on image\n    if response:\n        texts = response.text_annotations[0].description\n        self.subdict[\"text\"] = texts\n    else:\n        print(\"No text found on image.\")\n        self.subdict[\"text\"] = None\n    if response.error.message:\n        print(\"Google Cloud Vision Error\")\n        raise ValueError(\n            \"{}\\nFor more info on error messages, check: \"\n            \"https://cloud.google.com/apis/design/errors\".format(\n                response.error.message\n            )\n        )\n</code></pre>"},{"location":"api/#ammico.text.TextDetector.remove_linebreaks","title":"<code>remove_linebreaks()</code>","text":"<p>Remove linebreaks from original and translated text.</p> Source code in <code>ammico/text.py</code> <pre><code>def remove_linebreaks(self):\n    \"\"\"Remove linebreaks from original and translated text.\"\"\"\n    if self.subdict[\"text\"] and self.subdict[\"text_english\"]:\n        self.subdict[\"text\"] = self.subdict[\"text\"].replace(\"\\n\", \" \")\n        self.subdict[\"text_english\"] = self.subdict[\"text_english\"].replace(\n            \"\\n\", \" \"\n        )\n</code></pre>"},{"location":"api/#ammico.text.TextDetector.set_keys","title":"<code>set_keys()</code>","text":"<p>Set the default keys for text analysis.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary with default text keys.</p> Source code in <code>ammico/text.py</code> <pre><code>def set_keys(self) -&gt; dict:\n    \"\"\"Set the default keys for text analysis.\n\n    Returns:\n        dict: The dictionary with default text keys.\n    \"\"\"\n    params = {\"text\": None, \"text_language\": None, \"text_english\": None}\n    return params\n</code></pre>"},{"location":"api/#ammico.text.TextDetector.translate_text","title":"<code>translate_text()</code>","text":"<p>Translate the detected text to English using the Translator object.</p> Source code in <code>ammico/text.py</code> <pre><code>def translate_text(self):\n    \"\"\"Translate the detected text to English using the Translator object.\"\"\"\n    if not self.accepted:\n        raise ValueError(\n            \"Privacy disclosure not accepted - skipping text translation.\"\n        )\n    text_to_translate = (\n        self.subdict[\"text_truncated\"]\n        if \"text_truncated\" in self.subdict\n        else self.subdict[\"text\"]\n    )\n    try:\n        translated = self.translator.translate(text_to_translate)\n    except Exception:\n        print(\"Could not translate the text with error {}.\".format(Exception))\n        translated = None\n        print(\"Skipping translation for this text.\")\n    self.subdict[\"text_language\"] = translated.src if translated else None\n    self.subdict[\"text_english\"] = translated.text if translated else None\n</code></pre>"},{"location":"api/#ammico.text.privacy_disclosure","title":"<code>privacy_disclosure(accept_privacy='PRIVACY_AMMICO')</code>","text":"<p>Asks the user to accept the privacy statement.</p> <p>Parameters:</p> Name Type Description Default <code>accept_privacy</code> <code>str</code> <p>The name of the disclosure variable (default: \"PRIVACY_AMMICO\").</p> <code>'PRIVACY_AMMICO'</code> Source code in <code>ammico/text.py</code> <pre><code>def privacy_disclosure(accept_privacy: str = \"PRIVACY_AMMICO\"):\n    \"\"\"\n    Asks the user to accept the privacy statement.\n\n    Args:\n        accept_privacy (str): The name of the disclosure variable (default: \"PRIVACY_AMMICO\").\n    \"\"\"\n    if not os.environ.get(accept_privacy):\n        accepted = _ask_for_privacy_acceptance(accept_privacy)\n    elif os.environ.get(accept_privacy) == \"False\":\n        accepted = False\n    elif os.environ.get(accept_privacy) == \"True\":\n        accepted = True\n    else:\n        print(\n            \"Could not determine privacy disclosure - skipping \\\n              text detection and translation.\"\n        )\n        accepted = False\n    return accepted\n</code></pre>"},{"location":"api/#image-summary","title":"Image Summary","text":""},{"location":"api/#ammico.image_summary.ImageSummaryDetector","title":"<code>ImageSummaryDetector</code>","text":"<p>               Bases: <code>AnalysisMethod</code></p> Source code in <code>ammico/image_summary.py</code> <pre><code>class ImageSummaryDetector(AnalysisMethod):\n    token_prompt_config = {\n        \"default\": {\n            \"summary\": {\"prompt\": \"Describe this image.\", \"max_new_tokens\": 256},\n            \"questions\": {\"prompt\": \"\", \"max_new_tokens\": 128},\n        },\n        \"concise\": {\n            \"summary\": {\n                \"prompt\": \"Describe this image in one concise caption.\",\n                \"max_new_tokens\": 64,\n            },\n            \"questions\": {\"prompt\": \"Answer concisely: \", \"max_new_tokens\": 128},\n        },\n    }\n    MAX_QUESTIONS_PER_IMAGE = 32\n    KEYS_BATCH_SIZE = 16\n\n    def __init__(\n        self,\n        summary_model: MultimodalSummaryModel,\n        subdict: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for analysing images using QWEN-2.5-VL model.\n        It provides methods for generating captions and answering questions about images.\n\n        Args:\n            summary_model ([type], optional): An instance of MultimodalSummaryModel to be used for analysis.\n            subdict (dict, optional): Dictionary containing the image to be analysed. Defaults to {}.\n\n        Returns:\n            None.\n        \"\"\"\n        if subdict is None:\n            subdict = {}\n\n        super().__init__(subdict)\n        self.summary_model = summary_model\n\n    def _load_pil_if_needed(\n        self, filename: Union[str, os.PathLike, Image.Image]\n    ) -&gt; Image.Image:\n        if isinstance(filename, (str, os.PathLike)):\n            return Image.open(filename).convert(\"RGB\")\n        elif isinstance(filename, Image.Image):\n            return filename.convert(\"RGB\")\n        else:\n            raise ValueError(\"filename must be a path or PIL.Image\")\n\n    @staticmethod\n    def _is_sequence_but_not_str(obj: Any) -&gt; bool:\n        \"\"\"True for sequence-like but not a string/bytes/PIL.Image.\"\"\"\n        return isinstance(obj, _Sequence) and not isinstance(\n            obj, (str, bytes, Image.Image)\n        )\n\n    def _prepare_inputs(\n        self, list_of_questions: list[str], entry: Optional[Dict[str, Any]] = None\n    ) -&gt; Dict[str, torch.Tensor]:\n        filename = entry.get(\"filename\")\n        if filename is None:\n            raise ValueError(\"entry must contain key 'filename'\")\n\n        if isinstance(filename, (str, os.PathLike, Image.Image)):\n            images_context = self._load_pil_if_needed(filename)\n        elif self._is_sequence_but_not_str(filename):\n            images_context = [self._load_pil_if_needed(i) for i in filename]\n        else:\n            raise ValueError(\n                \"Unsupported 'filename' entry: expected path, PIL.Image, or sequence.\"\n            )\n\n        images_only_messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    *(\n                        [{\"type\": \"image\", \"image\": img} for img in images_context]\n                        if isinstance(images_context, list)\n                        else [{\"type\": \"image\", \"image\": images_context}]\n                    )\n                ],\n            }\n        ]\n\n        try:\n            image_inputs, _ = process_vision_info(images_only_messages)\n        except Exception as e:\n            raise RuntimeError(f\"Image processing failed: {e}\")\n\n        texts: List[str] = []\n        for q in list_of_questions:\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        *(\n                            [\n                                {\"type\": \"image\", \"image\": image}\n                                for image in images_context\n                            ]\n                            if isinstance(images_context, list)\n                            else [{\"type\": \"image\", \"image\": images_context}]\n                        ),\n                        {\"type\": \"text\", \"text\": q},\n                    ],\n                }\n            ]\n            text = self.summary_model.processor.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            texts.append(text)\n\n        images_batch = [image_inputs] * len(texts)\n        inputs = self.summary_model.processor(\n            text=texts,\n            images=images_batch,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = {k: v.to(self.summary_model.device) for k, v in inputs.items()}\n\n        return inputs\n\n    def _validate_analysis_type(\n        self,\n        analysis_type: Union[\"AnalysisType\", str],\n        list_of_questions: Optional[List[str]],\n        max_questions_per_image: int,\n    ) -&gt; Tuple[str, List[str], bool, bool]:\n        if isinstance(analysis_type, AnalysisType):\n            analysis_type = analysis_type.value\n\n        allowed = {\"summary\", \"questions\", \"summary_and_questions\"}\n        if analysis_type not in allowed:\n            raise ValueError(f\"analysis_type must be one of {allowed}\")\n\n        if list_of_questions is None:\n            list_of_questions = [\n                \"Are there people in the image?\",\n                \"What is this picture about?\",\n            ]\n\n        if analysis_type in (\"questions\", \"summary_and_questions\"):\n            if len(list_of_questions) &gt; max_questions_per_image:\n                raise ValueError(\n                    f\"Number of questions per image ({len(list_of_questions)}) exceeds safety cap ({max_questions_per_image}). Reduce questions or increase max_questions_per_image.\"\n                )\n\n        is_summary = analysis_type in (\"summary\", \"summary_and_questions\")\n        is_questions = analysis_type in (\"questions\", \"summary_and_questions\")\n\n        return analysis_type, list_of_questions, is_summary, is_questions\n\n    def analyse_image(\n        self,\n        entry: dict,\n        analysis_type: Union[str, AnalysisType] = AnalysisType.SUMMARY_AND_QUESTIONS,\n        list_of_questions: Optional[List[str]] = None,\n        max_questions_per_image: int = MAX_QUESTIONS_PER_IMAGE,\n        is_concise_summary: bool = True,\n        is_concise_answer: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Analyse a single image entry. Returns dict with keys depending on analysis_type:\n            - 'caption' (str) if summary requested\n            - 'vqa' (dict) if questions requested\n        \"\"\"\n        self.subdict = entry\n        analysis_type, list_of_questions, is_summary, is_questions = (\n            self._validate_analysis_type(\n                analysis_type, list_of_questions, max_questions_per_image\n            )\n        )\n\n        if is_summary:\n            try:\n                caps = self.generate_caption(\n                    entry,\n                    num_return_sequences=1,\n                    is_concise_summary=is_concise_summary,\n                )\n                self.subdict[\"caption\"] = caps[0] if caps else \"\"\n            except Exception as e:\n                warnings.warn(f\"Caption generation failed: {e}\")\n\n        if is_questions:\n            try:\n                vqa_map = self.answer_questions(\n                    list_of_questions, entry, is_concise_answer\n                )\n                self.subdict[\"vqa\"] = vqa_map\n            except Exception as e:\n                warnings.warn(f\"VQA failed: {e}\")\n\n        return self.subdict\n\n    def analyse_images_from_dict(\n        self,\n        analysis_type: Union[AnalysisType, str] = AnalysisType.SUMMARY_AND_QUESTIONS,\n        list_of_questions: Optional[List[str]] = None,\n        max_questions_per_image: int = MAX_QUESTIONS_PER_IMAGE,\n        keys_batch_size: int = KEYS_BATCH_SIZE,\n        is_concise_summary: bool = True,\n        is_concise_answer: bool = True,\n    ) -&gt; Dict[str, dict]:\n        \"\"\"\n        Analyse image with  model.\n\n        Args:\n            analysis_type (str): type of the analysis.\n            list_of_questions (list[str]): list of questions.\n            max_questions_per_image (int): maximum number of questions per image.\n                We recommend to keep it low to avoid long processing times and high memory usage.\n            keys_batch_size (int): number of images to process in a batch.\n            is_concise_summary (bool): whether to generate concise summary.\n            is_concise_answer (bool): whether to generate concise answers.\n        Returns:\n            self.subdict (dict): dictionary with analysis results.\n        \"\"\"\n        # TODO: add option to ask multiple questions per image as one batch.\n        analysis_type, list_of_questions, is_summary, is_questions = (\n            self._validate_analysis_type(\n                analysis_type, list_of_questions, max_questions_per_image\n            )\n        )\n\n        keys = list(self.subdict.keys())\n        for batch_start in range(0, len(keys), keys_batch_size):\n            batch_keys = keys[batch_start : batch_start + keys_batch_size]\n            for key in batch_keys:\n                entry = self.subdict[key]\n                if is_summary:\n                    try:\n                        caps = self.generate_caption(\n                            entry,\n                            num_return_sequences=1,\n                            is_concise_summary=is_concise_summary,\n                        )\n                        entry[\"caption\"] = caps[0] if caps else \"\"\n                    except Exception as e:\n                        warnings.warn(f\"Caption generation failed: {e}\")\n\n                if is_questions:\n                    try:\n                        vqa_map = self.answer_questions(\n                            list_of_questions, entry, is_concise_answer\n                        )\n                        entry[\"vqa\"] = vqa_map\n                    except Exception as e:\n                        warnings.warn(f\"VQA failed: {e}\")\n\n                self.subdict[key] = entry\n        return self.subdict\n\n    def generate_caption(\n        self,\n        entry: Optional[Dict[str, Any]] = None,\n        num_return_sequences: int = 1,\n        is_concise_summary: bool = True,\n    ) -&gt; List[str]:\n        \"\"\"\n        Create caption for image. Depending on is_concise_summary it will be either concise or detailed.\n\n        Args:\n            entry (dict): dictionary containing the image to be captioned.\n            num_return_sequences (int): number of captions to generate.\n            is_concise_summary (bool): whether to generate concise summary.\n\n        Returns:\n            results (list[str]): list of generated captions.\n        \"\"\"\n        prompt = self.token_prompt_config[\n            \"concise\" if is_concise_summary else \"default\"\n        ][\"summary\"][\"prompt\"]\n        max_new_tokens = self.token_prompt_config[\n            \"concise\" if is_concise_summary else \"default\"\n        ][\"summary\"][\"max_new_tokens\"]\n        inputs = self._prepare_inputs([prompt], entry)\n\n        gen_conf = GenerationConfig(\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            num_return_sequences=num_return_sequences,\n        )\n\n        with torch.inference_mode():\n            try:\n                if self.summary_model.device == \"cuda\":\n                    with torch.amp.autocast(\"cuda\", enabled=True):\n                        generated_ids = self.summary_model.model.generate(\n                            **inputs, generation_config=gen_conf\n                        )\n                else:\n                    generated_ids = self.summary_model.model.generate(\n                        **inputs, generation_config=gen_conf\n                    )\n            except RuntimeError as e:\n                warnings.warn(\n                    f\"Retry without autocast failed: {e}. Attempting cudnn-disabled retry.\"\n                )\n                cudnn_was_enabled = (\n                    torch.backends.cudnn.is_available() and torch.backends.cudnn.enabled\n                )\n                if cudnn_was_enabled:\n                    torch.backends.cudnn.enabled = False\n                try:\n                    generated_ids = self.summary_model.model.generate(\n                        **inputs, generation_config=gen_conf\n                    )\n                except Exception as retry_error:\n                    raise RuntimeError(\n                        f\"Failed to generate ids after retry: {retry_error}\"\n                    ) from retry_error\n                finally:\n                    if cudnn_was_enabled:\n                        torch.backends.cudnn.enabled = True\n\n        decoded = None\n        if \"input_ids\" in inputs:\n            in_ids = inputs[\"input_ids\"]\n            trimmed = [\n                out_ids[len(inp_ids) :]\n                for inp_ids, out_ids in zip(in_ids, generated_ids)\n            ]\n            decoded = self.summary_model.tokenizer.batch_decode(\n                trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )\n        else:\n            decoded = self.summary_model.tokenizer.batch_decode(\n                generated_ids,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=False,\n            )\n\n        results = [d.strip() for d in decoded]\n        return results\n\n    def _clean_list_of_questions(\n        self, list_of_questions: list[str], prompt: str\n    ) -&gt; list[str]:\n        \"\"\"Clean the list of questions to contain correctly formatted strings.\"\"\"\n        # remove all None or empty questions\n        list_of_questions = [i for i in list_of_questions if i and i.strip()]\n        # ensure each question ends with a question mark\n        list_of_questions = [\n            i.strip() + \"?\" if not i.strip().endswith(\"?\") else i.strip()\n            for i in list_of_questions\n        ]\n        # ensure each question starts with the prompt\n        list_of_questions = [\n            i if i.lower().startswith(prompt.lower()) else prompt + i\n            for i in list_of_questions\n        ]\n        return list_of_questions\n\n    def answer_questions(\n        self,\n        list_of_questions: list[str],\n        entry: Optional[Dict[str, Any]] = None,\n        is_concise_answer: bool = True,\n    ) -&gt; List[str]:\n        \"\"\"\n        Create answers for list of questions about image.\n        Args:\n            list_of_questions (list[str]): list of questions.\n            entry (dict): dictionary containing the image to be captioned.\n            is_concise_answer (bool): whether to generate concise answers.\n        Returns:\n            answers (list[str]): list of answers.\n        \"\"\"\n        prompt = self.token_prompt_config[\n            \"concise\" if is_concise_answer else \"default\"\n        ][\"questions\"][\"prompt\"]\n        max_new_tokens = self.token_prompt_config[\n            \"concise\" if is_concise_answer else \"default\"\n        ][\"questions\"][\"max_new_tokens\"]\n\n        list_of_questions = self._clean_list_of_questions(list_of_questions, prompt)\n        gen_conf = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)\n\n        question_chunk_size = 8\n        answers: List[str] = []\n        n = len(list_of_questions)\n        for i in range(0, n, question_chunk_size):\n            chunk = list_of_questions[i : i + question_chunk_size]\n            inputs = self._prepare_inputs(chunk, entry)\n            with torch.inference_mode():\n                if self.summary_model.device == \"cuda\":\n                    with torch.amp.autocast(\"cuda\", enabled=True):\n                        out_ids = self.summary_model.model.generate(\n                            **inputs, generation_config=gen_conf\n                        )\n                else:\n                    out_ids = self.summary_model.model.generate(\n                        **inputs, generation_config=gen_conf\n                    )\n\n            if \"input_ids\" in inputs:\n                in_ids = inputs[\"input_ids\"]\n                trimmed_batch = [\n                    out_row[len(inp_row) :] for inp_row, out_row in zip(in_ids, out_ids)\n                ]\n                decoded = self.summary_model.tokenizer.batch_decode(\n                    trimmed_batch,\n                    skip_special_tokens=True,\n                    clean_up_tokenization_spaces=False,\n                )\n            else:\n                decoded = self.summary_model.tokenizer.batch_decode(\n                    out_ids,\n                    skip_special_tokens=True,\n                    clean_up_tokenization_spaces=False,\n                )\n\n            answers.extend([d.strip() for d in decoded])\n\n        if len(answers) != len(list_of_questions):\n            raise ValueError(\n                f\"Expected {len(list_of_questions)} answers, but got {len(answers)}, try varying amount of questions\"\n            )\n\n        return answers\n</code></pre>"},{"location":"api/#ammico.image_summary.ImageSummaryDetector.__init__","title":"<code>__init__(summary_model, subdict=None)</code>","text":"<p>Class for analysing images using QWEN-2.5-VL model. It provides methods for generating captions and answering questions about images.</p> <p>Parameters:</p> Name Type Description Default <code>summary_model</code> <code>[type]</code> <p>An instance of MultimodalSummaryModel to be used for analysis.</p> required <code>subdict</code> <code>dict</code> <p>Dictionary containing the image to be analysed. Defaults to {}.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>ammico/image_summary.py</code> <pre><code>def __init__(\n    self,\n    summary_model: MultimodalSummaryModel,\n    subdict: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Class for analysing images using QWEN-2.5-VL model.\n    It provides methods for generating captions and answering questions about images.\n\n    Args:\n        summary_model ([type], optional): An instance of MultimodalSummaryModel to be used for analysis.\n        subdict (dict, optional): Dictionary containing the image to be analysed. Defaults to {}.\n\n    Returns:\n        None.\n    \"\"\"\n    if subdict is None:\n        subdict = {}\n\n    super().__init__(subdict)\n    self.summary_model = summary_model\n</code></pre>"},{"location":"api/#ammico.image_summary.ImageSummaryDetector.analyse_image","title":"<code>analyse_image(entry, analysis_type=AnalysisType.SUMMARY_AND_QUESTIONS, list_of_questions=None, max_questions_per_image=MAX_QUESTIONS_PER_IMAGE, is_concise_summary=True, is_concise_answer=True)</code>","text":"<p>Analyse a single image entry. Returns dict with keys depending on analysis_type:     - 'caption' (str) if summary requested     - 'vqa' (dict) if questions requested</p> Source code in <code>ammico/image_summary.py</code> <pre><code>def analyse_image(\n    self,\n    entry: dict,\n    analysis_type: Union[str, AnalysisType] = AnalysisType.SUMMARY_AND_QUESTIONS,\n    list_of_questions: Optional[List[str]] = None,\n    max_questions_per_image: int = MAX_QUESTIONS_PER_IMAGE,\n    is_concise_summary: bool = True,\n    is_concise_answer: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Analyse a single image entry. Returns dict with keys depending on analysis_type:\n        - 'caption' (str) if summary requested\n        - 'vqa' (dict) if questions requested\n    \"\"\"\n    self.subdict = entry\n    analysis_type, list_of_questions, is_summary, is_questions = (\n        self._validate_analysis_type(\n            analysis_type, list_of_questions, max_questions_per_image\n        )\n    )\n\n    if is_summary:\n        try:\n            caps = self.generate_caption(\n                entry,\n                num_return_sequences=1,\n                is_concise_summary=is_concise_summary,\n            )\n            self.subdict[\"caption\"] = caps[0] if caps else \"\"\n        except Exception as e:\n            warnings.warn(f\"Caption generation failed: {e}\")\n\n    if is_questions:\n        try:\n            vqa_map = self.answer_questions(\n                list_of_questions, entry, is_concise_answer\n            )\n            self.subdict[\"vqa\"] = vqa_map\n        except Exception as e:\n            warnings.warn(f\"VQA failed: {e}\")\n\n    return self.subdict\n</code></pre>"},{"location":"api/#ammico.image_summary.ImageSummaryDetector.analyse_images_from_dict","title":"<code>analyse_images_from_dict(analysis_type=AnalysisType.SUMMARY_AND_QUESTIONS, list_of_questions=None, max_questions_per_image=MAX_QUESTIONS_PER_IMAGE, keys_batch_size=KEYS_BATCH_SIZE, is_concise_summary=True, is_concise_answer=True)</code>","text":"<p>Analyse image with  model.</p> <p>Parameters:</p> Name Type Description Default <code>analysis_type</code> <code>str</code> <p>type of the analysis.</p> <code>SUMMARY_AND_QUESTIONS</code> <code>list_of_questions</code> <code>list[str]</code> <p>list of questions.</p> <code>None</code> <code>max_questions_per_image</code> <code>int</code> <p>maximum number of questions per image. We recommend to keep it low to avoid long processing times and high memory usage.</p> <code>MAX_QUESTIONS_PER_IMAGE</code> <code>keys_batch_size</code> <code>int</code> <p>number of images to process in a batch.</p> <code>KEYS_BATCH_SIZE</code> <code>is_concise_summary</code> <code>bool</code> <p>whether to generate concise summary.</p> <code>True</code> <code>is_concise_answer</code> <code>bool</code> <p>whether to generate concise answers.</p> <code>True</code> <p>Returns:     self.subdict (dict): dictionary with analysis results.</p> Source code in <code>ammico/image_summary.py</code> <pre><code>def analyse_images_from_dict(\n    self,\n    analysis_type: Union[AnalysisType, str] = AnalysisType.SUMMARY_AND_QUESTIONS,\n    list_of_questions: Optional[List[str]] = None,\n    max_questions_per_image: int = MAX_QUESTIONS_PER_IMAGE,\n    keys_batch_size: int = KEYS_BATCH_SIZE,\n    is_concise_summary: bool = True,\n    is_concise_answer: bool = True,\n) -&gt; Dict[str, dict]:\n    \"\"\"\n    Analyse image with  model.\n\n    Args:\n        analysis_type (str): type of the analysis.\n        list_of_questions (list[str]): list of questions.\n        max_questions_per_image (int): maximum number of questions per image.\n            We recommend to keep it low to avoid long processing times and high memory usage.\n        keys_batch_size (int): number of images to process in a batch.\n        is_concise_summary (bool): whether to generate concise summary.\n        is_concise_answer (bool): whether to generate concise answers.\n    Returns:\n        self.subdict (dict): dictionary with analysis results.\n    \"\"\"\n    # TODO: add option to ask multiple questions per image as one batch.\n    analysis_type, list_of_questions, is_summary, is_questions = (\n        self._validate_analysis_type(\n            analysis_type, list_of_questions, max_questions_per_image\n        )\n    )\n\n    keys = list(self.subdict.keys())\n    for batch_start in range(0, len(keys), keys_batch_size):\n        batch_keys = keys[batch_start : batch_start + keys_batch_size]\n        for key in batch_keys:\n            entry = self.subdict[key]\n            if is_summary:\n                try:\n                    caps = self.generate_caption(\n                        entry,\n                        num_return_sequences=1,\n                        is_concise_summary=is_concise_summary,\n                    )\n                    entry[\"caption\"] = caps[0] if caps else \"\"\n                except Exception as e:\n                    warnings.warn(f\"Caption generation failed: {e}\")\n\n            if is_questions:\n                try:\n                    vqa_map = self.answer_questions(\n                        list_of_questions, entry, is_concise_answer\n                    )\n                    entry[\"vqa\"] = vqa_map\n                except Exception as e:\n                    warnings.warn(f\"VQA failed: {e}\")\n\n            self.subdict[key] = entry\n    return self.subdict\n</code></pre>"},{"location":"api/#ammico.image_summary.ImageSummaryDetector.answer_questions","title":"<code>answer_questions(list_of_questions, entry=None, is_concise_answer=True)</code>","text":"<p>Create answers for list of questions about image. Args:     list_of_questions (list[str]): list of questions.     entry (dict): dictionary containing the image to be captioned.     is_concise_answer (bool): whether to generate concise answers. Returns:     answers (list[str]): list of answers.</p> Source code in <code>ammico/image_summary.py</code> <pre><code>def answer_questions(\n    self,\n    list_of_questions: list[str],\n    entry: Optional[Dict[str, Any]] = None,\n    is_concise_answer: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Create answers for list of questions about image.\n    Args:\n        list_of_questions (list[str]): list of questions.\n        entry (dict): dictionary containing the image to be captioned.\n        is_concise_answer (bool): whether to generate concise answers.\n    Returns:\n        answers (list[str]): list of answers.\n    \"\"\"\n    prompt = self.token_prompt_config[\n        \"concise\" if is_concise_answer else \"default\"\n    ][\"questions\"][\"prompt\"]\n    max_new_tokens = self.token_prompt_config[\n        \"concise\" if is_concise_answer else \"default\"\n    ][\"questions\"][\"max_new_tokens\"]\n\n    list_of_questions = self._clean_list_of_questions(list_of_questions, prompt)\n    gen_conf = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)\n\n    question_chunk_size = 8\n    answers: List[str] = []\n    n = len(list_of_questions)\n    for i in range(0, n, question_chunk_size):\n        chunk = list_of_questions[i : i + question_chunk_size]\n        inputs = self._prepare_inputs(chunk, entry)\n        with torch.inference_mode():\n            if self.summary_model.device == \"cuda\":\n                with torch.amp.autocast(\"cuda\", enabled=True):\n                    out_ids = self.summary_model.model.generate(\n                        **inputs, generation_config=gen_conf\n                    )\n            else:\n                out_ids = self.summary_model.model.generate(\n                    **inputs, generation_config=gen_conf\n                )\n\n        if \"input_ids\" in inputs:\n            in_ids = inputs[\"input_ids\"]\n            trimmed_batch = [\n                out_row[len(inp_row) :] for inp_row, out_row in zip(in_ids, out_ids)\n            ]\n            decoded = self.summary_model.tokenizer.batch_decode(\n                trimmed_batch,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=False,\n            )\n        else:\n            decoded = self.summary_model.tokenizer.batch_decode(\n                out_ids,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=False,\n            )\n\n        answers.extend([d.strip() for d in decoded])\n\n    if len(answers) != len(list_of_questions):\n        raise ValueError(\n            f\"Expected {len(list_of_questions)} answers, but got {len(answers)}, try varying amount of questions\"\n        )\n\n    return answers\n</code></pre>"},{"location":"api/#ammico.image_summary.ImageSummaryDetector.generate_caption","title":"<code>generate_caption(entry=None, num_return_sequences=1, is_concise_summary=True)</code>","text":"<p>Create caption for image. Depending on is_concise_summary it will be either concise or detailed.</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>dict</code> <p>dictionary containing the image to be captioned.</p> <code>None</code> <code>num_return_sequences</code> <code>int</code> <p>number of captions to generate.</p> <code>1</code> <code>is_concise_summary</code> <code>bool</code> <p>whether to generate concise summary.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>results</code> <code>list[str]</code> <p>list of generated captions.</p> Source code in <code>ammico/image_summary.py</code> <pre><code>def generate_caption(\n    self,\n    entry: Optional[Dict[str, Any]] = None,\n    num_return_sequences: int = 1,\n    is_concise_summary: bool = True,\n) -&gt; List[str]:\n    \"\"\"\n    Create caption for image. Depending on is_concise_summary it will be either concise or detailed.\n\n    Args:\n        entry (dict): dictionary containing the image to be captioned.\n        num_return_sequences (int): number of captions to generate.\n        is_concise_summary (bool): whether to generate concise summary.\n\n    Returns:\n        results (list[str]): list of generated captions.\n    \"\"\"\n    prompt = self.token_prompt_config[\n        \"concise\" if is_concise_summary else \"default\"\n    ][\"summary\"][\"prompt\"]\n    max_new_tokens = self.token_prompt_config[\n        \"concise\" if is_concise_summary else \"default\"\n    ][\"summary\"][\"max_new_tokens\"]\n    inputs = self._prepare_inputs([prompt], entry)\n\n    gen_conf = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_return_sequences=num_return_sequences,\n    )\n\n    with torch.inference_mode():\n        try:\n            if self.summary_model.device == \"cuda\":\n                with torch.amp.autocast(\"cuda\", enabled=True):\n                    generated_ids = self.summary_model.model.generate(\n                        **inputs, generation_config=gen_conf\n                    )\n            else:\n                generated_ids = self.summary_model.model.generate(\n                    **inputs, generation_config=gen_conf\n                )\n        except RuntimeError as e:\n            warnings.warn(\n                f\"Retry without autocast failed: {e}. Attempting cudnn-disabled retry.\"\n            )\n            cudnn_was_enabled = (\n                torch.backends.cudnn.is_available() and torch.backends.cudnn.enabled\n            )\n            if cudnn_was_enabled:\n                torch.backends.cudnn.enabled = False\n            try:\n                generated_ids = self.summary_model.model.generate(\n                    **inputs, generation_config=gen_conf\n                )\n            except Exception as retry_error:\n                raise RuntimeError(\n                    f\"Failed to generate ids after retry: {retry_error}\"\n                ) from retry_error\n            finally:\n                if cudnn_was_enabled:\n                    torch.backends.cudnn.enabled = True\n\n    decoded = None\n    if \"input_ids\" in inputs:\n        in_ids = inputs[\"input_ids\"]\n        trimmed = [\n            out_ids[len(inp_ids) :]\n            for inp_ids, out_ids in zip(in_ids, generated_ids)\n        ]\n        decoded = self.summary_model.tokenizer.batch_decode(\n            trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n    else:\n        decoded = self.summary_model.tokenizer.batch_decode(\n            generated_ids,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=False,\n        )\n\n    results = [d.strip() for d in decoded]\n    return results\n</code></pre>"},{"location":"api/#video-summary","title":"Video Summary","text":""},{"location":"api/#ammico.video_summary.VideoSummaryDetector","title":"<code>VideoSummaryDetector</code>","text":"<p>               Bases: <code>AnalysisMethod</code></p> Source code in <code>ammico/video_summary.py</code> <pre><code>class VideoSummaryDetector(AnalysisMethod):\n    def __init__(\n        self,\n        summary_model: MultimodalSummaryModel = None,\n        audio_model: Optional[AudioToTextModel] = None,\n        subdict: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for analysing videos using QWEN-2.5-VL model.\n        It provides methods for generating captions and answering questions about videos.\n\n        Args:\n            summary_model ([type], optional): An instance of MultimodalSummaryModel to be used for analysis.\n            subdict (dict, optional): Dictionary containing the video to be analysed. Defaults to {}.\n\n        Returns:\n            None.\n        \"\"\"\n        if subdict is None:\n            subdict = {}\n\n        super().__init__(subdict)\n        _validate_subdict(subdict)\n        self.summary_model = summary_model or None\n        self.audio_model = audio_model\n        self.prompt_builder = PromptBuilder()\n\n    def _decode_trimmed_outputs(\n        self,\n        generated_ids: torch.Tensor,\n        inputs: Dict[str, torch.Tensor],\n        tokenizer,\n        prompt_texts: List[str],\n    ) -&gt; List[str]:\n        \"\"\"\n        Trim prompt tokens using attention_mask/input_ids when available and decode to strings.\n        Then remove any literal prompt prefix using prompt_texts (one per batch element).\n        Args:\n            generated_ids (torch.Tensor): Generated token IDs from the model.\n            inputs (Dict[str, torch.Tensor]): Original input tensors used for generation.\n            tokenizer: Tokenizer used for decoding the generated outputs.\n            prompt_texts (List[str]): List of prompt texts corresponding to each input in the batch.\n        Returns:\n            List[str]: Decoded generated texts after trimming and cleaning.\n        \"\"\"\n\n        batch_size = generated_ids.shape[0]\n\n        if \"input_ids\" in inputs:\n            token_for_padding = (\n                tokenizer.pad_token_id\n                if getattr(tokenizer, \"pad_token_id\", None) is not None\n                else getattr(tokenizer, \"eos_token_id\", None)\n            )\n            if token_for_padding is None:\n                lengths = [int(inputs[\"input_ids\"].shape[1])] * batch_size\n            else:\n                lengths = inputs[\"input_ids\"].ne(token_for_padding).sum(dim=1).tolist()\n        else:\n            lengths = [0] * batch_size\n\n        trimmed_ids = []\n        for i in range(batch_size):\n            out_ids = generated_ids[i]\n            in_len = int(lengths[i]) if i &lt; len(lengths) else 0\n            if out_ids.size(0) &gt; in_len:\n                t = out_ids[in_len:]\n            else:\n                t = out_ids.new_empty((0,), dtype=out_ids.dtype)\n            t_cpu = t.to(\"cpu\")\n            trimmed_ids.append(t_cpu.tolist())\n\n        decoded = tokenizer.batch_decode(\n            trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n        )\n        decoded_results = []\n        for ptext, raw in zip(prompt_texts, decoded):\n            cleaned = _strip_prompt_prefix_literal(raw, ptext)\n            decoded_results.append(cleaned)\n        return decoded_results\n\n    def _generate_from_processor_inputs(\n        self,\n        processor_inputs: Dict[str, torch.Tensor],\n        prompt_texts: List[str],\n        tokenizer,\n        len_objects: Optional[int] = None,\n    ) -&gt; List[str]:\n        \"\"\"\n        Run model.generate on already-processed processor_inputs (tensors moved to device),\n        then decode and trim prompt tokens &amp; remove literal prompt prefixes using prompt_texts.\n        Args:\n            processor_inputs (Dict[str, torch.Tensor]): Inputs prepared by the processor.\n            prompt_texts (List[str]): List of prompt texts corresponding to each input in the batch.\n            tokenizer: Tokenizer used for decoding the generated outputs.\n            len_objects (Optional[int], optional): Number of objects/frames to adjust max_new_tokens. Defaults to None.\n        Returns:\n            List[str]: Decoded generated texts after trimming and cleaning.\n        \"\"\"\n        # In case of many frames, allow more max_new_tokens # TODO recheck the logic\n        if len_objects is not None:\n            max_new_tokens = len_objects * 128\n        else:\n            max_new_tokens = 128\n        gen_conf = GenerationConfig(\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            num_return_sequences=1,\n        )\n\n        for k, v in processor_inputs.items():\n            if isinstance(v, torch.Tensor):\n                processor_inputs[k] = v.to(self.summary_model.device)\n\n        with torch.inference_mode():\n            try:\n                if self.summary_model.device == \"cuda\":\n                    with torch.amp.autocast(\"cuda\", enabled=True):\n                        generated_ids = self.summary_model.model.generate(\n                            **processor_inputs, generation_config=gen_conf\n                        )\n                else:\n                    generated_ids = self.summary_model.model.generate(\n                        **processor_inputs, generation_config=gen_conf\n                    )\n            except RuntimeError as e:\n                warnings.warn(\n                    f\"Generation failed with error: {e}. Retrying with cuDNN disabled.\",\n                    RuntimeWarning,\n                )\n                cudnn_was_enabled = (\n                    torch.backends.cudnn.is_available() and torch.backends.cudnn.enabled\n                )\n                if cudnn_was_enabled:\n                    torch.backends.cudnn.enabled = False\n                try:\n                    generated_ids = self.summary_model.model.generate(\n                        **processor_inputs, generation_config=gen_conf\n                    )\n                except Exception as retry_error:\n                    raise RuntimeError(\n                        f\"Failed to generate ids after retry: {retry_error}\"\n                    ) from retry_error\n                finally:\n                    if cudnn_was_enabled:\n                        torch.backends.cudnn.enabled = True\n\n        decoded = self._decode_trimmed_outputs(\n            generated_ids, processor_inputs, tokenizer, prompt_texts\n        )\n        return decoded\n\n    def _audio_to_text(self, audio_path: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Convert audio file to text using an whisper model.\n        Args:\n            audio_path (str): Path to the audio file.\n        Returns:\n            List[Dict[str, Any]]: List of transcribed audio segments with start_time, end_time, text, and duration.\n        \"\"\"\n\n        if not os.path.exists(audio_path):\n            raise ValueError(f\"Audio file {audio_path} does not exist.\")\n\n        try:\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n            audio = whisperx.load_audio(audio_path)\n            transcribe_result = self.audio_model.model.transcribe(audio)\n            model_a, metadata = whisperx.load_align_model(\n                language_code=transcribe_result[\"language\"],\n                device=self.audio_model.device,\n            )\n            aligned_result = whisperx.align(\n                transcribe_result[\"segments\"],\n                model_a,\n                metadata,\n                audio,\n                self.audio_model.device,\n            )\n            audio_descriptions = []\n            for segment in aligned_result[\"segments\"]:\n                audio_descriptions.append(\n                    {\n                        \"start_time\": segment[\"start\"],\n                        \"end_time\": segment[\"end\"],\n                        \"text\": segment[\"text\"].strip(),\n                        \"duration\": segment[\"end\"] - segment[\"start\"],\n                    }\n                )\n            return audio_descriptions\n        except Exception as e:\n            raise RuntimeError(f\"Failed to transcribe audio: {e}\")\n\n    def _check_audio_stream(self, filename: str) -&gt; bool:\n        \"\"\"\n        Check if the video file has an audio stream.\n        Args:\n            filename (str): Path to the video file.\n        Returns:\n            bool: True if audio stream exists, False otherwise.\n        \"\"\"\n        try:\n            cmd = [\n                \"ffprobe\",\n                \"-v\",\n                \"error\",\n                \"-select_streams\",\n                \"a\",\n                \"-show_entries\",\n                \"stream=codec_type\",\n                \"-of\",\n                \"default=noprint_wrappers=1:nokey=1\",\n                filename,\n            ]\n            result = subprocess.run(\n                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n            )\n            output = result.stdout.strip()\n            return bool(output)\n        except Exception as e:\n            warnings.warn(\n                f\"Failed to check audio stream in video {filename}: {e}\",\n                RuntimeWarning,\n            )\n            return False\n\n    def _extract_transcribe_audio_part(\n        self,\n        filename: str,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extract audio part from the video file and generate captions using an audio whisperx model.\n        Args:\n            filename (str): Path to the video file.\n        Returns:\n            List[Dict[str, Any]]: List of transcribed audio segments with start_time, end_time, text, and duration.\n        \"\"\"\n\n        if not self._check_audio_stream(filename):\n            self.audio_model.close()\n            self.audio_model = None\n            return []\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            audio_output_path = os.path.join(tmpdir, \"audio_extracted.wav\")\n            try:\n                subprocess.run(\n                    [\n                        \"ffmpeg\",\n                        \"-i\",\n                        filename,\n                        \"-vn\",\n                        \"-acodec\",\n                        \"pcm_s16le\",\n                        \"-ar\",\n                        \"16000\",\n                        \"-ac\",\n                        \"1\",\n                        \"-y\",\n                        audio_output_path,\n                    ],\n                    check=True,\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                )\n            except subprocess.CalledProcessError as e:\n                raise RuntimeError(f\"Failed to extract audio from video: {e}\")\n\n            audio_descriptions = self._audio_to_text(audio_output_path)\n\n        # and close the audio model to free up resources\n        self.audio_model.close()\n        self.audio_model = None\n\n        return audio_descriptions\n\n    def _detect_scene_cuts(\n        self,\n        filename: str,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Detect scene cuts in the video using frame differencing method.\n        Args:\n            filename: Path to the video file\n        Returns:\n            List of segments with 'start_time' and 'end_time'\n        \"\"\"\n\n        cap = cv2.VideoCapture(filename)\n        fps = cap.get(cv2.CAP_PROP_FPS)\n\n        frames = []\n        img_height, img_width = None, None\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            img_height, img_width = frame.shape[:2]\n\n            try:\n                if img_width / img_height &gt; 1.2:\n                    frame_small = cv2.resize(frame, (320, 240))\n                elif img_width / img_height &lt; 0.8:\n                    frame_small = cv2.resize(frame, (240, 320))\n                else:\n                    frame_small = cv2.resize(frame, (320, 320))\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to resize frame for scene cut detection: {e}\"\n                )\n\n            gray = cv2.cvtColor(\n                frame_small, cv2.COLOR_BGR2GRAY\n            )  # TODO check if it is ok, maybe we can use color info as well\n            frames.append(gray)\n\n        cap.release()\n        if img_height is None or img_width is None:\n            raise ValueError(\n                \"Failed to read frames from video for scene cut detection.\"\n            )\n\n        # Compute frame differences to keep memory usage low\n        frame_diffs = []\n        for i in range(1, len(frames)):\n            diff = cv2.absdiff(frames[i], frames[i - 1])\n            mean_diff = np.mean(diff)\n            frame_diffs.append(mean_diff)\n\n        # Find peaks in differences (scene cuts) via adaptive threshold based on median\n        threshold = 25.0\n        median_diff = np.median(frame_diffs)\n        cut_threshold = median_diff + threshold\n\n        cut_frames = signal.find_peaks(\n            frame_diffs,\n            height=cut_threshold,\n            distance=int(fps * 0.5),  # At least 0.5s between cuts\n        )[0]\n\n        video_segments = []\n        cut_frames_with_starts = [0] + list(cut_frames) + [len(frames)]\n\n        for i in range(len(cut_frames_with_starts) - 1):\n            start_frame = cut_frames_with_starts[i]\n            end_frame = cut_frames_with_starts[i + 1]\n\n            video_segments.append(\n                {\n                    \"type\": \"video_scene\",\n                    \"start_time\": start_frame / fps,\n                    \"end_time\": end_frame / fps,\n                    \"duration\": (end_frame - start_frame) / fps,\n                }\n            )\n\n        # Since there may be issues with last frame detection, slightly adjust the end_time of the last segment\n        last_segment = video_segments[-1]\n        last_segment[\"end_time\"] -= 0.5\n        # Ensure the end_time does not go below the start_time in case of very short last segment/video\n        if last_segment[\"end_time\"] &lt; last_segment[\"start_time\"]:\n            last_segment[\"end_time\"] = last_segment[\"start_time\"]\n\n        return {\n            \"segments\": video_segments,\n            \"video_meta\": {\n                \"width\": img_width,\n                \"height\": img_height,\n            },\n        }\n\n    def _extract_frame_timestamps_from_clip(\n        self,\n        filename: str,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract frame timestamps for each detected video segment.\n        Args:\n            filename: Path to the video file\n        Returns:\n            List of segments with 'start_time', 'end_time', and 'frame_timestamps'\n        \"\"\"\n        base_frames_per_clip = 4.0\n        result = self._detect_scene_cuts(filename)\n        segments = result[\"segments\"]\n        video_meta = result[\"video_meta\"]\n        for seg in segments:\n            if seg[\"duration\"] &lt; 2.0:\n                frame_rate_per_clip = 2.0\n            elif seg[\"duration\"] &gt; 20.0:\n                frame_rate_per_clip = 6.0\n            else:\n                frame_rate_per_clip = base_frames_per_clip\n\n            start_time = seg[\"start_time\"]\n            end_time = seg[\"end_time\"]\n            n_samples = max(1, int(frame_rate_per_clip))\n            sample_times = torch.linspace(\n                start_time, end_time, steps=n_samples, dtype=torch.float32\n            )\n            seg[\"frame_timestamps\"] = sample_times.tolist()\n\n        return {\n            \"segments\": segments,\n            \"video_meta\": video_meta,\n        }\n\n    def _reassign_video_timestamps_to_segments(\n        self,\n        segments: List[Dict[str, Any]],\n        video_segs: List[Dict[str, Any]],\n    ) -&gt; None:\n        \"\"\"\n        Reassign video frame timestamps to each new segment based on overlapping video scenes.\n        Args:\n            segments: List of segments to assign timestamps to.\n            video_segs: List of video scenes with original frame timestamps.\n        Returns:\n            None\n        \"\"\"\n\n        boundary_margin = 0.5\n        eps = 1e-6\n\n        video_list = list(video_segs)\n        for seg in segments:\n            seg_start = seg[\"start_time\"]\n            seg_end = seg[\"end_time\"]\n\n            merged_timestamps: List[float] = []\n            for vscene in video_list:\n                if \"frame_timestamps\" not in vscene:\n                    raise ValueError(\"Video scene missing 'frame_timestamps' key.\")\n\n                contrib = [\n                    float(t)\n                    for t in vscene[\"frame_timestamps\"]\n                    if (t + eps) &gt;= (seg_start - boundary_margin)\n                    and (t - eps) &lt;= (seg_end + boundary_margin)\n                    and (t + eps) &gt;= seg_start\n                    and (t - eps) &lt;= seg_end\n                ]\n                if contrib:\n                    merged_timestamps.extend(contrib)\n\n            # dedupe &amp; sort\n            seg[\"video_frame_timestamps\"] = sorted(set(merged_timestamps))\n\n    def _combine_visual_frames_by_time(\n        self,\n        video_segs: List[Dict[str, Any]],\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Split too-long video segments (&gt;25s).\n        Args:\n            video_segs: List of video segments with 'start_time' and 'end_time'\n        Returns:\n            List of combined segments\n        \"\"\"\n\n        if not video_segs:\n            raise ValueError(\"No video segments to combine.\")\n        out = []\n        for vs in video_segs:\n            st, ed, dur = (\n                float(vs[\"start_time\"]),\n                float(vs[\"end_time\"]),\n                float(vs[\"duration\"]),\n            )\n            if dur &gt; 25.0:\n                parts = int(math.ceil(dur / 25.0))\n                part_dur = dur / parts\n                for p in range(parts):\n                    ps = st + p * part_dur\n                    pe = st + (p + 1) * part_dur if p &lt; parts - 1 else ed\n                    out.append(\n                        {\n                            \"start_time\": ps,\n                            \"end_time\": pe,\n                            \"duration\": pe - ps,\n                            \"audio_phrases\": [],\n                            \"video_scenes\": [vs],\n                        }\n                    )\n            else:\n                out.append(\n                    {\n                        \"start_time\": st,\n                        \"end_time\": ed,\n                        \"duration\": dur,\n                        \"audio_phrases\": [],\n                        \"video_scenes\": [vs],\n                    }\n                )\n\n        self._reassign_video_timestamps_to_segments(out, video_segs)\n        return out\n\n    def merge_audio_visual_boundaries(\n        self,\n        audio_segs: List[Dict[str, Any]],\n        video_segs: List[Dict[str, Any]],\n        segment_threshold_duration: int = 8,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Merge audio phrase boundaries and video scene cuts into coherent temporal segments for the model\n        Args:\n            audio_segs: List of audio segments with 'start_time' and 'end_time'\n            video_segs: List of video segments with 'start_time' and 'end_time'\n            segment_threshold_duration: Duration to create a new segment boundary\n        Returns:\n            List of merged segments\n        \"\"\"\n        if not audio_segs:\n            new_vid = self._combine_visual_frames_by_time(video_segs)\n            return new_vid\n\n        events = [\n            (\"audio\", seg[\"start_time\"], seg[\"end_time\"], seg) for seg in audio_segs\n        ] + [(\"video\", seg[\"start_time\"], seg[\"end_time\"], seg) for seg in video_segs]\n\n        if not events:\n            raise ValueError(\"No audio and video segments to merge.\")\n\n        events.sort(key=lambda x: x[1])\n        global_last_end = max(e[2] for e in events)\n        # Create merged segments respecting both boundaries\n        merged = []\n        current_segment_start = 0\n        current_audio_phrases = []\n        current_video_scenes = []\n\n        for event_type, start, _, data in events:\n            current_duration = start - current_segment_start\n            if current_duration &gt; segment_threshold_duration:\n                segment_end = start\n\n                if segment_end &lt; current_segment_start:\n                    segment_end = current_segment_start\n\n                merged.append(\n                    {\n                        \"start_time\": current_segment_start,\n                        \"end_time\": segment_end,\n                        \"audio_phrases\": current_audio_phrases,\n                        \"video_scenes\": current_video_scenes,\n                        \"duration\": segment_end - current_segment_start,\n                    }\n                )\n                # start a new segment at the current event's start\n                current_segment_start = segment_end\n                current_audio_phrases = []\n                current_video_scenes = []\n\n            if event_type == \"audio\":\n                current_audio_phrases.append(data)\n            else:\n                current_video_scenes.append(data)\n\n        if current_audio_phrases or current_video_scenes:\n            final_end = max(global_last_end, events[-1][2], current_segment_start)\n            if final_end &lt; current_segment_start:\n                final_end = current_segment_start\n\n            merged.append(\n                {\n                    \"start_time\": current_segment_start,\n                    \"end_time\": final_end,\n                    \"audio_phrases\": current_audio_phrases,\n                    \"video_scenes\": current_video_scenes,\n                    \"duration\": final_end - current_segment_start,\n                }\n            )\n\n        self._reassign_video_timestamps_to_segments(merged, video_segs)\n        return merged\n\n    def _run_ffmpeg(\n        self, cmd_args: List[str], timeout: Optional[float]\n    ) -&gt; subprocess.CompletedProcess:\n        \"\"\"\n        Execute ffmpeg command and return the completed process.\n        Args:\n            cmd_args: List of ffmpeg command arguments.\n            timeout: Timeout for the subprocess.\n        Returns:\n            CompletedProcess: Result of the subprocess execution.\n        \"\"\"\n\n        cmd = [\"ffmpeg\", \"-hide_banner\", \"-loglevel\", \"error\"] + cmd_args\n        return subprocess.run(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout\n        )\n\n    def _build_extract_command(\n        self, filename: str, timestamp: float, accurate: bool, codec: str = \"png\"\n    ) -&gt; List[str]:\n        \"\"\"\n        Build ffmpeg command for frame extraction.\n\n        Args:\n            filename: Path to video file\n            timestamp: Time in seconds\n            accurate: If True, seek after input (slow but accurate)\n            codec: Output codec ('png' or 'mjpeg')\n        Returns:\n            List of ffmpeg command arguments\n        \"\"\"\n        ss_arg = f\"{timestamp:.6f}\"\n        cmd = []\n        # Position -ss based on accuracy requirement\n        if accurate:\n            cmd = [\"-i\", filename, \"-ss\", ss_arg]  # accurate mode\n        else:\n            cmd = [\"-ss\", ss_arg, \"-i\", filename]  # fast mode\n\n        # Common extraction parameters\n        cmd += [\"-frames:v\", \"1\", \"-f\", \"image2pipe\"]\n\n        # Codec-specific settings\n        if codec == \"png\":\n            cmd += [\"-vcodec\", \"png\", \"-pix_fmt\", \"rgb24\"]\n        elif codec == \"mjpeg\":\n            cmd += [\"-vcodec\", \"mjpeg\", \"-pix_fmt\", \"yuvj420p\"]\n\n        cmd.append(\"pipe:1\")\n        return cmd\n\n    def _run_ffmpeg_extraction(\n        self,\n        filename: str,\n        timestamp: float,\n        out_w: int,\n        out_h: int,\n        timeout: Optional[float] = 30.0,\n    ) -&gt; Image.Image:\n        \"\"\"\n        Extract a single frame at the specified timestamp.\n\n        Args:\n            filename: Path to video file\n            timestamp: Time in seconds\n            out_w: Optional output width\n            out_h: Optional output height\n            timeout: Subprocess timeout in seconds\n\n        Returns:\n            PIL Image in RGB format\n\n        Raises:\n            RuntimeError: If frame extraction fails\n        \"\"\"\n\n        strategies = [\n            (\"mjpeg\", False),\n            (\"png\", True),\n        ]\n\n        last_error = None\n\n        for codec, use_accurate in strategies:\n            try:\n                cmd = self._build_extract_command(\n                    filename, timestamp, use_accurate, codec\n                )\n                proc = self._run_ffmpeg(cmd, timeout)\n\n                if proc.returncode == 0 and proc.stdout:\n                    img = Image.open(BytesIO(proc.stdout)).convert(\"RGB\")\n                    img = img.resize((out_w, out_h), resample=Image.BILINEAR)\n                    return img\n                else:\n                    last_error = proc.stderr.decode(\"utf-8\", errors=\"replace\")\n\n            except Exception as e:\n                last_error = str(e)\n                warnings.warn(\n                    f\"Frame extraction failed at {timestamp:.3f}s with codec {codec} \"\n                    f\"({'accurate' if use_accurate else 'fast'}): {last_error}\",\n                    RuntimeWarning,\n                )\n\n        raise RuntimeError(\n            f\"Failed to extract frame at {timestamp:.3f}s from {filename}. \"\n            f\"Last error: {last_error[:500]}\"\n        )\n\n    def _calculate_output_dimensions(\n        self, original_w: int, original_h: int\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        Calculate output dimensions in a fully adaptive way, preserving aspect ratio, but decreasing size.\n        It works both for landscape and portrait videos.\n        Args:\n            original_w: Original width\n            original_h: Original height\n        Returns:\n            Tuple of (out_w, out_h)\n        \"\"\"\n        aspect_ratio = original_w / original_h\n        max_dimension = 720\n\n        if aspect_ratio &gt; 1.2:\n            out_w = max_dimension\n            out_h = int(max_dimension / aspect_ratio)\n        elif aspect_ratio &lt; 0.8:\n            out_h = max_dimension\n            out_w = int(max_dimension * aspect_ratio)\n        else:\n            out_w = max_dimension\n            out_h = max_dimension\n        return out_w, out_h\n\n    def _extract_frames_ffmpeg(\n        self,\n        filename: str,\n        timestamps: List[float],\n        original_w: int,\n        original_h: int,\n        workers: int = 4,\n    ) -&gt; List[Tuple[float, Image.Image]]:\n        \"\"\"\n        Extract multiple frames using a thread pool (parallel ffmpeg processes).\n        Args:\n            filename: Path to video file\n            timestamps: List of times in seconds\n            original_w: Original width of the video\n            original_h: Original height of the video\n            workers: Number of parallel threads\n        Returns:\n          List of (timestamp, PIL.Image) preserving order of timestamps.\n        \"\"\"\n        results = {}\n        out_w, out_h = self._calculate_output_dimensions(original_w, original_h)\n        with ThreadPoolExecutor(max_workers=workers) as ex:\n            futures = {\n                ex.submit(self._run_ffmpeg_extraction, filename, t, out_w, out_h): i\n                for i, t in enumerate(timestamps)\n            }\n            for fut in as_completed(futures):\n                idx = futures[fut]\n                try:\n                    img = fut.result()\n                    results[idx] = img\n                except Exception as e:\n                    raise RuntimeError(\n                        f\"Failed to extract frame for {timestamps[idx]}s: {e}\"\n                    ) from e\n\n        return [(timestamps[i], results[i]) for i in range(len(timestamps))]\n\n    def _make_captions_from_extracted_frames(\n        self,\n        filename: str,\n        merged_segments: List[Dict[str, Any]],\n        video_meta: Dict[str, Any],\n        list_of_questions: Optional[List[str]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Generate captions for all extracted frames and then produce a concise summary of the video.\n        Args:\n            filename (str): Path to the video file.\n            merged_segments (List[Dict[str, Any]]): List of merged segments with frame timestamps.\n            list_of_questions (Optional[List[str]]): List of questions for VQA.\n        Returns:\n            None. Modifies merged_segments in place to add 'summary_bullets' and 'vqa_bullets'.\n        \"\"\"\n        proc = self.summary_model.processor\n\n        img_width = video_meta.get(\"width\")\n        img_height = video_meta.get(\"height\")\n        if img_width is None or img_height is None:\n            raise ValueError(\n                \"Frame dimensions not found in the last segment for extraction.\"\n            )\n\n        for seg in merged_segments:  # TODO might be generator faster, so changes to ffmpeg extraction may be needed\n            collected: List[Tuple[float, str]] = []\n            frame_timestamps = seg.get(\"video_frame_timestamps\", [])\n            if not frame_timestamps:\n                raise ValueError(\n                    f\"No frame timestamps found for segment {seg['start_time']:.2f}s to {seg['end_time']:.2f}s\"\n                )\n            include_questions = bool(list_of_questions)\n            caption_instruction = self.prompt_builder.build_frame_prompt(\n                include_vqa=include_questions,\n                questions=list_of_questions,\n            )\n            pairs = self._extract_frames_ffmpeg(\n                filename,\n                frame_timestamps,\n                original_w=img_width,\n                original_h=img_height,\n                workers=min(8, (os.cpu_count() or 1) // 2),\n            )\n            prompt_texts = []\n\n            for ts, img in pairs:\n                messages = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"image\", \"image\": img},\n                            {\"type\": \"text\", \"text\": caption_instruction},\n                        ],\n                    }\n                ]\n\n                prompt_text = proc.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True\n                )\n\n                prompt_texts.append(prompt_text)\n\n            processor_inputs = proc(\n                text=prompt_texts,\n                images=[img for _, img in pairs],\n                return_tensors=\"pt\",\n                padding=True,\n            )\n            len_objects = len(pairs)\n            if include_questions:\n                len_objects *= 2  # because we expect two outputs per input when questions are included\n            captions = self._generate_from_processor_inputs(\n                processor_inputs,\n                prompt_texts,\n                self.summary_model.tokenizer,\n                len_objects=len_objects,\n            )\n            for t, c in zip(frame_timestamps, captions):\n                collected.append((float(t), c))\n\n            collected.sort(key=lambda x: x[0])\n            bullets_summary, bullets_vqa = _categorize_outputs(\n                collected, include_questions\n            )\n\n            seg[\"summary_bullets\"] = bullets_summary\n            seg[\"vqa_bullets\"] = bullets_vqa\n\n    def make_captions_for_subclips(\n        self,\n        entry: Dict[str, Any],\n        list_of_questions: Optional[List[str]] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Generate captions for video subclips using both audio and visual information, for a further full video summary/VQA.\n        Args:\n            entry (Dict[str, Any]): Dictionary containing the video file information.\n            list_of_questions (Optional[List[str]]): List of questions for VQA.\n        Returns:\n            List[Dict[str, Any]]: List of dictionaries containing timestamps and generated captions.\n        \"\"\"\n\n        filename = entry.get(\"filename\")\n        if not filename:\n            raise ValueError(\"entry must contain key 'filename'\")\n\n        if not os.path.exists(filename):\n            raise ValueError(f\"Video file {filename} does not exist.\")\n\n        audio_generated_captions = []\n        if self.audio_model is not None:\n            audio_generated_captions = self._extract_transcribe_audio_part(filename)\n            entry[\"audio_descriptions\"] = audio_generated_captions\n\n        video_result_segments = self._extract_frame_timestamps_from_clip(filename)\n        video_segments_w_timestamps = video_result_segments[\"segments\"]\n        video_meta = video_result_segments[\"video_meta\"]\n        merged_segments = self.merge_audio_visual_boundaries(\n            audio_generated_captions,\n            video_segments_w_timestamps,\n        )\n\n        self._make_captions_from_extracted_frames(\n            filename,\n            merged_segments,\n            video_meta,\n            list_of_questions=list_of_questions,\n        )\n        results = []\n        proc = self.summary_model.processor\n        for seg in merged_segments:\n            frame_timestamps = seg.get(\"video_frame_timestamps\", [])\n\n            collected: List[Tuple[float, str]] = []\n            include_audio = False\n            audio_lines = seg[\"audio_phrases\"]\n            if audio_lines:\n                include_audio = True\n\n            include_questions = bool(list_of_questions)\n            caption_instruction = self.prompt_builder.build_clip_prompt(\n                frame_bullets=seg.get(\"summary_bullets\", []),\n                include_audio=include_audio,\n                audio_transcription=seg.get(\"audio_phrases\", []),\n                include_vqa=include_questions,\n                questions=list_of_questions,\n                vqa_bullets=seg.get(\"vqa_bullets\", []),\n            )\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [{\"type\": \"text\", \"text\": caption_instruction}],\n                }\n            ]\n            prompt_text = proc.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            processor_inputs = proc(\n                text=[prompt_text],\n                return_tensors=\"pt\",\n                padding=True,\n            )\n            final_outputs = self._generate_from_processor_inputs(\n                processor_inputs,\n                [prompt_text],\n                self.summary_model.tokenizer,\n            )\n            for t, c in zip(frame_timestamps, final_outputs):\n                collected.append((float(t), c))\n\n            collected.sort(key=lambda x: x[0])\n            bullets_summary, bullets_vqa = _categorize_outputs(\n                collected, include_questions\n            )\n\n            results.append(\n                {\n                    \"start_time\": seg[\"start_time\"],\n                    \"end_time\": seg[\"end_time\"],\n                    \"summary_bullets\": bullets_summary,\n                    \"vqa_bullets\": bullets_vqa,\n                }\n            )\n\n        return results\n\n    def final_summary(self, summary_dict: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Produce a concise summary of the video, based on generated captions for all extracted frames.\n        Args:\n            summary_dict (Dict[str, Any]): Dictionary containing captions for the frames.\n        Returns:\n            Dict[str, Any]: A dictionary containing the list of captions with timestamps and the final summary.\n        \"\"\"\n        proc = self.summary_model.processor\n\n        bullets = []\n        for seg in summary_dict:\n            seg_bullets = seg.get(\"summary_bullets\", [])\n            bullets.extend(seg_bullets)\n        if not bullets:\n            raise ValueError(\"No captions available for summary generation.\")\n\n        summary_user_prompt = self.prompt_builder.build_video_prompt(\n            include_vqa=False,\n            clip_summaries=bullets,\n        )\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"text\", \"text\": summary_user_prompt}],\n            }\n        ]\n\n        summary_prompt_text = proc.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        summary_inputs = proc(\n            text=[summary_prompt_text], return_tensors=\"pt\", padding=True\n        )\n\n        summary_inputs = {\n            k: v.to(self.summary_model.device) if isinstance(v, torch.Tensor) else v\n            for k, v in summary_inputs.items()\n        }\n        final_summary_list = self._generate_from_processor_inputs(\n            summary_inputs,\n            [summary_prompt_text],\n            self.summary_model.tokenizer,\n        )\n        final_summary = final_summary_list[0].strip() if final_summary_list else \"\"\n\n        return {\n            \"summary\": final_summary,\n        }\n\n    def final_answers(\n        self,\n        answers_dict: List[Dict[str, Any]],\n        list_of_questions: List[str],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Answer the list of questions for the video based on the VQA bullets from the frames.\n        Args:\n            answers_dict (Dict[str, Any]): Dictionary containing the VQA bullets.\n        Returns:\n            Dict[str, Any]: A dictionary containing the list of answers to the questions.\n        \"\"\"\n        vqa_bullets = []\n        summary_bullets = []\n        for seg in answers_dict:\n            summary_bullets.extend(seg.get(\"summary_bullets\", []))\n            seg_bullets = seg.get(\"vqa_bullets\", [])\n            vqa_bullets.extend(seg_bullets)\n\n        if not vqa_bullets:\n            raise ValueError(\n                \"No VQA bullets generated for single frames available for answering questions.\"\n            )\n\n        include_questions = bool(list_of_questions)\n        if include_questions:\n            prompt = self.prompt_builder.build_video_prompt(\n                include_vqa=include_questions,\n                questions=list_of_questions,\n                vqa_bullets=vqa_bullets,\n                clip_summaries=summary_bullets,\n            )\n        else:\n            raise ValueError(\n                \"list_of_questions must be provided for making final answers.\"\n            )\n\n        proc = self.summary_model.processor\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"text\", \"text\": prompt}],\n            }\n        ]\n        final_vqa_prompt_text = proc.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        final_vqa_inputs = proc(\n            text=[final_vqa_prompt_text], return_tensors=\"pt\", padding=True\n        )\n        final_vqa_inputs = {\n            k: v.to(self.summary_model.device) if isinstance(v, torch.Tensor) else v\n            for k, v in final_vqa_inputs.items()\n        }\n\n        final_vqa_list = self._generate_from_processor_inputs(\n            final_vqa_inputs,\n            [final_vqa_prompt_text],\n            self.summary_model.tokenizer,\n        )\n\n        final_vqa_output = final_vqa_list[0].strip() if final_vqa_list else \"\"\n        vqa_answers = []\n        answer_matches = re.findall(\n            r\"\\d+\\.\\s+(.+?)(?=\\n\\d+\\.|$)\", final_vqa_output, flags=re.DOTALL\n        )\n        for answer in answer_matches:\n            vqa_answers.append(answer.strip())\n        return {\n            \"vqa_answers\": vqa_answers,\n        }\n\n    def analyse_videos_from_dict(\n        self,\n        analysis_type: Union[AnalysisType, str] = AnalysisType.SUMMARY,\n        list_of_questions: Optional[List[str]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Analyse the video specified in self.subdict using frame extraction and captioning.\n        Args:\n            analysis_type (Union[AnalysisType, str], optional): Type of analysis to perform. Defaults to AnalysisType.SUMMARY.\n            list_of_questions (List[str], optional): List of questions to answer about the video. Required if analysis_type includes questions.\n        Returns:\n            Dict[str, Any]: A dictionary containing the analysis results, including summary and answers for provided questions(if any).\n        \"\"\"\n        if list_of_questions is not None and not isinstance(list_of_questions, list):\n            raise TypeError(\"Expected list_of_questions to be a list of strings.\")\n        if list_of_questions and any(not isinstance(q, str) for q in list_of_questions):\n            raise ValueError(\"All items in list_of_questions must be strings.\")\n\n        analysis_type, is_summary, is_questions = AnalysisType._validate_analysis_type(\n            analysis_type, list_of_questions\n        )\n\n        for video_key, entry in self.subdict.items():\n            answers_dict = self.make_captions_for_subclips(\n                entry,\n                list_of_questions=list_of_questions,\n            )\n            if is_summary:\n                answer = self.final_summary(answers_dict)\n                entry[\"summary\"] = answer[\"summary\"]\n            if is_questions:\n                answer = self.final_answers(answers_dict, list_of_questions)\n                entry[\"vqa_answers\"] = answer[\"vqa_answers\"]\n\n            self.subdict[video_key] = entry\n\n        return self.subdict\n</code></pre>"},{"location":"api/#ammico.video_summary.VideoSummaryDetector.__init__","title":"<code>__init__(summary_model=None, audio_model=None, subdict=None)</code>","text":"<p>Class for analysing videos using QWEN-2.5-VL model. It provides methods for generating captions and answering questions about videos.</p> <p>Parameters:</p> Name Type Description Default <code>summary_model</code> <code>[type]</code> <p>An instance of MultimodalSummaryModel to be used for analysis.</p> <code>None</code> <code>subdict</code> <code>dict</code> <p>Dictionary containing the video to be analysed. Defaults to {}.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>ammico/video_summary.py</code> <pre><code>def __init__(\n    self,\n    summary_model: MultimodalSummaryModel = None,\n    audio_model: Optional[AudioToTextModel] = None,\n    subdict: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Class for analysing videos using QWEN-2.5-VL model.\n    It provides methods for generating captions and answering questions about videos.\n\n    Args:\n        summary_model ([type], optional): An instance of MultimodalSummaryModel to be used for analysis.\n        subdict (dict, optional): Dictionary containing the video to be analysed. Defaults to {}.\n\n    Returns:\n        None.\n    \"\"\"\n    if subdict is None:\n        subdict = {}\n\n    super().__init__(subdict)\n    _validate_subdict(subdict)\n    self.summary_model = summary_model or None\n    self.audio_model = audio_model\n    self.prompt_builder = PromptBuilder()\n</code></pre>"},{"location":"api/#ammico.video_summary.VideoSummaryDetector.analyse_videos_from_dict","title":"<code>analyse_videos_from_dict(analysis_type=AnalysisType.SUMMARY, list_of_questions=None)</code>","text":"<p>Analyse the video specified in self.subdict using frame extraction and captioning. Args:     analysis_type (Union[AnalysisType, str], optional): Type of analysis to perform. Defaults to AnalysisType.SUMMARY.     list_of_questions (List[str], optional): List of questions to answer about the video. Required if analysis_type includes questions. Returns:     Dict[str, Any]: A dictionary containing the analysis results, including summary and answers for provided questions(if any).</p> Source code in <code>ammico/video_summary.py</code> <pre><code>def analyse_videos_from_dict(\n    self,\n    analysis_type: Union[AnalysisType, str] = AnalysisType.SUMMARY,\n    list_of_questions: Optional[List[str]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Analyse the video specified in self.subdict using frame extraction and captioning.\n    Args:\n        analysis_type (Union[AnalysisType, str], optional): Type of analysis to perform. Defaults to AnalysisType.SUMMARY.\n        list_of_questions (List[str], optional): List of questions to answer about the video. Required if analysis_type includes questions.\n    Returns:\n        Dict[str, Any]: A dictionary containing the analysis results, including summary and answers for provided questions(if any).\n    \"\"\"\n    if list_of_questions is not None and not isinstance(list_of_questions, list):\n        raise TypeError(\"Expected list_of_questions to be a list of strings.\")\n    if list_of_questions and any(not isinstance(q, str) for q in list_of_questions):\n        raise ValueError(\"All items in list_of_questions must be strings.\")\n\n    analysis_type, is_summary, is_questions = AnalysisType._validate_analysis_type(\n        analysis_type, list_of_questions\n    )\n\n    for video_key, entry in self.subdict.items():\n        answers_dict = self.make_captions_for_subclips(\n            entry,\n            list_of_questions=list_of_questions,\n        )\n        if is_summary:\n            answer = self.final_summary(answers_dict)\n            entry[\"summary\"] = answer[\"summary\"]\n        if is_questions:\n            answer = self.final_answers(answers_dict, list_of_questions)\n            entry[\"vqa_answers\"] = answer[\"vqa_answers\"]\n\n        self.subdict[video_key] = entry\n\n    return self.subdict\n</code></pre>"},{"location":"api/#ammico.video_summary.VideoSummaryDetector.final_answers","title":"<code>final_answers(answers_dict, list_of_questions)</code>","text":"<p>Answer the list of questions for the video based on the VQA bullets from the frames. Args:     answers_dict (Dict[str, Any]): Dictionary containing the VQA bullets. Returns:     Dict[str, Any]: A dictionary containing the list of answers to the questions.</p> Source code in <code>ammico/video_summary.py</code> <pre><code>def final_answers(\n    self,\n    answers_dict: List[Dict[str, Any]],\n    list_of_questions: List[str],\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Answer the list of questions for the video based on the VQA bullets from the frames.\n    Args:\n        answers_dict (Dict[str, Any]): Dictionary containing the VQA bullets.\n    Returns:\n        Dict[str, Any]: A dictionary containing the list of answers to the questions.\n    \"\"\"\n    vqa_bullets = []\n    summary_bullets = []\n    for seg in answers_dict:\n        summary_bullets.extend(seg.get(\"summary_bullets\", []))\n        seg_bullets = seg.get(\"vqa_bullets\", [])\n        vqa_bullets.extend(seg_bullets)\n\n    if not vqa_bullets:\n        raise ValueError(\n            \"No VQA bullets generated for single frames available for answering questions.\"\n        )\n\n    include_questions = bool(list_of_questions)\n    if include_questions:\n        prompt = self.prompt_builder.build_video_prompt(\n            include_vqa=include_questions,\n            questions=list_of_questions,\n            vqa_bullets=vqa_bullets,\n            clip_summaries=summary_bullets,\n        )\n    else:\n        raise ValueError(\n            \"list_of_questions must be provided for making final answers.\"\n        )\n\n    proc = self.summary_model.processor\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n        }\n    ]\n    final_vqa_prompt_text = proc.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    final_vqa_inputs = proc(\n        text=[final_vqa_prompt_text], return_tensors=\"pt\", padding=True\n    )\n    final_vqa_inputs = {\n        k: v.to(self.summary_model.device) if isinstance(v, torch.Tensor) else v\n        for k, v in final_vqa_inputs.items()\n    }\n\n    final_vqa_list = self._generate_from_processor_inputs(\n        final_vqa_inputs,\n        [final_vqa_prompt_text],\n        self.summary_model.tokenizer,\n    )\n\n    final_vqa_output = final_vqa_list[0].strip() if final_vqa_list else \"\"\n    vqa_answers = []\n    answer_matches = re.findall(\n        r\"\\d+\\.\\s+(.+?)(?=\\n\\d+\\.|$)\", final_vqa_output, flags=re.DOTALL\n    )\n    for answer in answer_matches:\n        vqa_answers.append(answer.strip())\n    return {\n        \"vqa_answers\": vqa_answers,\n    }\n</code></pre>"},{"location":"api/#ammico.video_summary.VideoSummaryDetector.final_summary","title":"<code>final_summary(summary_dict)</code>","text":"<p>Produce a concise summary of the video, based on generated captions for all extracted frames. Args:     summary_dict (Dict[str, Any]): Dictionary containing captions for the frames. Returns:     Dict[str, Any]: A dictionary containing the list of captions with timestamps and the final summary.</p> Source code in <code>ammico/video_summary.py</code> <pre><code>def final_summary(self, summary_dict: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Produce a concise summary of the video, based on generated captions for all extracted frames.\n    Args:\n        summary_dict (Dict[str, Any]): Dictionary containing captions for the frames.\n    Returns:\n        Dict[str, Any]: A dictionary containing the list of captions with timestamps and the final summary.\n    \"\"\"\n    proc = self.summary_model.processor\n\n    bullets = []\n    for seg in summary_dict:\n        seg_bullets = seg.get(\"summary_bullets\", [])\n        bullets.extend(seg_bullets)\n    if not bullets:\n        raise ValueError(\"No captions available for summary generation.\")\n\n    summary_user_prompt = self.prompt_builder.build_video_prompt(\n        include_vqa=False,\n        clip_summaries=bullets,\n    )\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": summary_user_prompt}],\n        }\n    ]\n\n    summary_prompt_text = proc.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    summary_inputs = proc(\n        text=[summary_prompt_text], return_tensors=\"pt\", padding=True\n    )\n\n    summary_inputs = {\n        k: v.to(self.summary_model.device) if isinstance(v, torch.Tensor) else v\n        for k, v in summary_inputs.items()\n    }\n    final_summary_list = self._generate_from_processor_inputs(\n        summary_inputs,\n        [summary_prompt_text],\n        self.summary_model.tokenizer,\n    )\n    final_summary = final_summary_list[0].strip() if final_summary_list else \"\"\n\n    return {\n        \"summary\": final_summary,\n    }\n</code></pre>"},{"location":"api/#ammico.video_summary.VideoSummaryDetector.make_captions_for_subclips","title":"<code>make_captions_for_subclips(entry, list_of_questions=None)</code>","text":"<p>Generate captions for video subclips using both audio and visual information, for a further full video summary/VQA. Args:     entry (Dict[str, Any]): Dictionary containing the video file information.     list_of_questions (Optional[List[str]]): List of questions for VQA. Returns:     List[Dict[str, Any]]: List of dictionaries containing timestamps and generated captions.</p> Source code in <code>ammico/video_summary.py</code> <pre><code>def make_captions_for_subclips(\n    self,\n    entry: Dict[str, Any],\n    list_of_questions: Optional[List[str]] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Generate captions for video subclips using both audio and visual information, for a further full video summary/VQA.\n    Args:\n        entry (Dict[str, Any]): Dictionary containing the video file information.\n        list_of_questions (Optional[List[str]]): List of questions for VQA.\n    Returns:\n        List[Dict[str, Any]]: List of dictionaries containing timestamps and generated captions.\n    \"\"\"\n\n    filename = entry.get(\"filename\")\n    if not filename:\n        raise ValueError(\"entry must contain key 'filename'\")\n\n    if not os.path.exists(filename):\n        raise ValueError(f\"Video file {filename} does not exist.\")\n\n    audio_generated_captions = []\n    if self.audio_model is not None:\n        audio_generated_captions = self._extract_transcribe_audio_part(filename)\n        entry[\"audio_descriptions\"] = audio_generated_captions\n\n    video_result_segments = self._extract_frame_timestamps_from_clip(filename)\n    video_segments_w_timestamps = video_result_segments[\"segments\"]\n    video_meta = video_result_segments[\"video_meta\"]\n    merged_segments = self.merge_audio_visual_boundaries(\n        audio_generated_captions,\n        video_segments_w_timestamps,\n    )\n\n    self._make_captions_from_extracted_frames(\n        filename,\n        merged_segments,\n        video_meta,\n        list_of_questions=list_of_questions,\n    )\n    results = []\n    proc = self.summary_model.processor\n    for seg in merged_segments:\n        frame_timestamps = seg.get(\"video_frame_timestamps\", [])\n\n        collected: List[Tuple[float, str]] = []\n        include_audio = False\n        audio_lines = seg[\"audio_phrases\"]\n        if audio_lines:\n            include_audio = True\n\n        include_questions = bool(list_of_questions)\n        caption_instruction = self.prompt_builder.build_clip_prompt(\n            frame_bullets=seg.get(\"summary_bullets\", []),\n            include_audio=include_audio,\n            audio_transcription=seg.get(\"audio_phrases\", []),\n            include_vqa=include_questions,\n            questions=list_of_questions,\n            vqa_bullets=seg.get(\"vqa_bullets\", []),\n        )\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"text\", \"text\": caption_instruction}],\n            }\n        ]\n        prompt_text = proc.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        processor_inputs = proc(\n            text=[prompt_text],\n            return_tensors=\"pt\",\n            padding=True,\n        )\n        final_outputs = self._generate_from_processor_inputs(\n            processor_inputs,\n            [prompt_text],\n            self.summary_model.tokenizer,\n        )\n        for t, c in zip(frame_timestamps, final_outputs):\n            collected.append((float(t), c))\n\n        collected.sort(key=lambda x: x[0])\n        bullets_summary, bullets_vqa = _categorize_outputs(\n            collected, include_questions\n        )\n\n        results.append(\n            {\n                \"start_time\": seg[\"start_time\"],\n                \"end_time\": seg[\"end_time\"],\n                \"summary_bullets\": bullets_summary,\n                \"vqa_bullets\": bullets_vqa,\n            }\n        )\n\n    return results\n</code></pre>"},{"location":"api/#ammico.video_summary.VideoSummaryDetector.merge_audio_visual_boundaries","title":"<code>merge_audio_visual_boundaries(audio_segs, video_segs, segment_threshold_duration=8)</code>","text":"<p>Merge audio phrase boundaries and video scene cuts into coherent temporal segments for the model Args:     audio_segs: List of audio segments with 'start_time' and 'end_time'     video_segs: List of video segments with 'start_time' and 'end_time'     segment_threshold_duration: Duration to create a new segment boundary Returns:     List of merged segments</p> Source code in <code>ammico/video_summary.py</code> <pre><code>def merge_audio_visual_boundaries(\n    self,\n    audio_segs: List[Dict[str, Any]],\n    video_segs: List[Dict[str, Any]],\n    segment_threshold_duration: int = 8,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Merge audio phrase boundaries and video scene cuts into coherent temporal segments for the model\n    Args:\n        audio_segs: List of audio segments with 'start_time' and 'end_time'\n        video_segs: List of video segments with 'start_time' and 'end_time'\n        segment_threshold_duration: Duration to create a new segment boundary\n    Returns:\n        List of merged segments\n    \"\"\"\n    if not audio_segs:\n        new_vid = self._combine_visual_frames_by_time(video_segs)\n        return new_vid\n\n    events = [\n        (\"audio\", seg[\"start_time\"], seg[\"end_time\"], seg) for seg in audio_segs\n    ] + [(\"video\", seg[\"start_time\"], seg[\"end_time\"], seg) for seg in video_segs]\n\n    if not events:\n        raise ValueError(\"No audio and video segments to merge.\")\n\n    events.sort(key=lambda x: x[1])\n    global_last_end = max(e[2] for e in events)\n    # Create merged segments respecting both boundaries\n    merged = []\n    current_segment_start = 0\n    current_audio_phrases = []\n    current_video_scenes = []\n\n    for event_type, start, _, data in events:\n        current_duration = start - current_segment_start\n        if current_duration &gt; segment_threshold_duration:\n            segment_end = start\n\n            if segment_end &lt; current_segment_start:\n                segment_end = current_segment_start\n\n            merged.append(\n                {\n                    \"start_time\": current_segment_start,\n                    \"end_time\": segment_end,\n                    \"audio_phrases\": current_audio_phrases,\n                    \"video_scenes\": current_video_scenes,\n                    \"duration\": segment_end - current_segment_start,\n                }\n            )\n            # start a new segment at the current event's start\n            current_segment_start = segment_end\n            current_audio_phrases = []\n            current_video_scenes = []\n\n        if event_type == \"audio\":\n            current_audio_phrases.append(data)\n        else:\n            current_video_scenes.append(data)\n\n    if current_audio_phrases or current_video_scenes:\n        final_end = max(global_last_end, events[-1][2], current_segment_start)\n        if final_end &lt; current_segment_start:\n            final_end = current_segment_start\n\n        merged.append(\n            {\n                \"start_time\": current_segment_start,\n                \"end_time\": final_end,\n                \"audio_phrases\": current_audio_phrases,\n                \"video_scenes\": current_video_scenes,\n                \"duration\": final_end - current_segment_start,\n            }\n        )\n\n    self._reassign_video_timestamps_to_segments(merged, video_segs)\n    return merged\n</code></pre>"},{"location":"api/#colors","title":"Colors","text":""},{"location":"api/#ammico.colors.ColorDetector","title":"<code>ColorDetector</code>","text":"<p>               Bases: <code>AnalysisMethod</code></p> Source code in <code>ammico/colors.py</code> <pre><code>class ColorDetector(AnalysisMethod):\n    def __init__(\n        self,\n        subdict: dict,\n        delta_e_method: str = \"CIE 1976\",\n    ) -&gt; None:\n        \"\"\"Color Analysis class, analyse hue and identify named colors.\n\n        Args:\n            subdict (dict): The dictionary containing the image path.\n            delta_e_method (str): The calculation method used for assigning the\n                closest color name, defaults to \"CIE 1976\".\n                The available options are: 'CIE 1976', 'CIE 1994', 'CIE 2000',\n                'CMC', 'ITP', 'CAM02-LCD', 'CAM02-SCD', 'CAM02-UCS', 'CAM16-LCD',\n                'CAM16-SCD', 'CAM16-UCS', 'DIN99'\n        \"\"\"\n        super().__init__(subdict)\n        self.subdict.update(self.set_keys())\n        self.merge_color = True\n        self.n_colors = 100\n        if delta_e_method not in COLOR_SCHEMES:\n            raise ValueError(\n                \"Invalid selection for assigning the color name. Please select one of {}\".format(\n                    COLOR_SCHEMES\n                )\n            )\n        self.delta_e_method = delta_e_method\n\n    def set_keys(self) -&gt; dict:\n        colors = {\n            \"red\": 0,\n            \"green\": 0,\n            \"blue\": 0,\n            \"yellow\": 0,\n            \"cyan\": 0,\n            \"orange\": 0,\n            \"purple\": 0,\n            \"pink\": 0,\n            \"brown\": 0,\n            \"grey\": 0,\n            \"white\": 0,\n            \"black\": 0,\n        }\n        return colors\n\n    def analyse_image(self) -&gt; dict:\n        \"\"\"\n        Uses the colorgram library to extract the n most common colors from the images.\n        One problem is, that the most common colors are taken before beeing categorized,\n        so for small values it might occur that the ten most common colors are shades of grey,\n        while other colors are present but will be ignored. Because of this n_colors=100 was chosen as default.\n\n        The colors are then matched to the closest color in the CSS3 color list using the delta-e metric.\n        They are then merged into one data frame.\n        The colors can be reduced to a smaller list of colors using the get_color_table function.\n        These colors are: \"red\", \"green\", \"blue\", \"yellow\",\"cyan\", \"orange\", \"purple\", \"pink\", \"brown\", \"grey\", \"white\", \"black\".\n\n        Returns:\n            dict: Dictionary with color names as keys and percentage of color in image as values.\n        \"\"\"\n        filename = self.subdict[\"filename\"]\n\n        colors = colorgram.extract(filename, self.n_colors)\n        for color in colors:\n            rgb_name = self.rgb2name(\n                color.rgb,\n                merge_color=self.merge_color,\n                delta_e_method=self.delta_e_method,\n            )\n            self.subdict[rgb_name] += color.proportion\n\n        # ensure color rounding\n        for key in self.set_keys().keys():\n            if self.subdict[key]:\n                self.subdict[key] = round(self.subdict[key], 2)\n\n        return self.subdict\n\n    def rgb2name(\n        self, c, merge_color: bool = True, delta_e_method: str = \"CIE 1976\"\n    ) -&gt; str:\n        \"\"\"Take an rgb color as input and return the closest color name from the CSS3 color list.\n\n        Args:\n            c (Union[List,tuple]): RGB value.\n            merge_color (bool, Optional): Whether color name should be reduced, defaults to True.\n        Returns:\n            str: Closest matching color name.\n        \"\"\"\n        if len(c) != 3:\n            raise ValueError(\"Input color must be a list or tuple of length 3 (RGB).\")\n\n        h_color = \"#{:02x}{:02x}{:02x}\".format(int(c[0]), int(c[1]), int(c[2]))\n        try:\n            output_color = webcolors.hex_to_name(h_color, spec=\"css3\")\n            output_color = output_color.lower().replace(\"grey\", \"gray\")\n        except ValueError:\n            delta_e_lst = []\n            filtered_colors = webcolors._definitions._CSS3_NAMES_TO_HEX\n\n            for _, img_hex in filtered_colors.items():\n                cur_clr = webcolors.hex_to_rgb(img_hex)\n                # calculate color Delta-E\n                delta_e = colour.delta_E(c, cur_clr, method=delta_e_method)\n                delta_e_lst.append(delta_e)\n            # find lowest delta-e\n            min_diff = np.argsort(delta_e_lst)[0]\n            output_color = (\n                str(list(filtered_colors.items())[min_diff][0])\n                .lower()\n                .replace(\"grey\", \"gray\")\n            )\n\n        # match color to reduced list:\n        if merge_color:\n            for reduced_key, reduced_color_sub_list in get_color_table().items():\n                if str(output_color).lower() in [\n                    str(color_name).lower()\n                    for color_name in reduced_color_sub_list[\"ColorName\"]\n                ]:\n                    output_color = reduced_key.lower()\n                    break\n        return output_color\n</code></pre>"},{"location":"api/#ammico.colors.ColorDetector.__init__","title":"<code>__init__(subdict, delta_e_method='CIE 1976')</code>","text":"<p>Color Analysis class, analyse hue and identify named colors.</p> <p>Parameters:</p> Name Type Description Default <code>subdict</code> <code>dict</code> <p>The dictionary containing the image path.</p> required <code>delta_e_method</code> <code>str</code> <p>The calculation method used for assigning the closest color name, defaults to \"CIE 1976\". The available options are: 'CIE 1976', 'CIE 1994', 'CIE 2000', 'CMC', 'ITP', 'CAM02-LCD', 'CAM02-SCD', 'CAM02-UCS', 'CAM16-LCD', 'CAM16-SCD', 'CAM16-UCS', 'DIN99'</p> <code>'CIE 1976'</code> Source code in <code>ammico/colors.py</code> <pre><code>def __init__(\n    self,\n    subdict: dict,\n    delta_e_method: str = \"CIE 1976\",\n) -&gt; None:\n    \"\"\"Color Analysis class, analyse hue and identify named colors.\n\n    Args:\n        subdict (dict): The dictionary containing the image path.\n        delta_e_method (str): The calculation method used for assigning the\n            closest color name, defaults to \"CIE 1976\".\n            The available options are: 'CIE 1976', 'CIE 1994', 'CIE 2000',\n            'CMC', 'ITP', 'CAM02-LCD', 'CAM02-SCD', 'CAM02-UCS', 'CAM16-LCD',\n            'CAM16-SCD', 'CAM16-UCS', 'DIN99'\n    \"\"\"\n    super().__init__(subdict)\n    self.subdict.update(self.set_keys())\n    self.merge_color = True\n    self.n_colors = 100\n    if delta_e_method not in COLOR_SCHEMES:\n        raise ValueError(\n            \"Invalid selection for assigning the color name. Please select one of {}\".format(\n                COLOR_SCHEMES\n            )\n        )\n    self.delta_e_method = delta_e_method\n</code></pre>"},{"location":"api/#ammico.colors.ColorDetector.analyse_image","title":"<code>analyse_image()</code>","text":"<p>Uses the colorgram library to extract the n most common colors from the images. One problem is, that the most common colors are taken before beeing categorized, so for small values it might occur that the ten most common colors are shades of grey, while other colors are present but will be ignored. Because of this n_colors=100 was chosen as default.</p> <p>The colors are then matched to the closest color in the CSS3 color list using the delta-e metric. They are then merged into one data frame. The colors can be reduced to a smaller list of colors using the get_color_table function. These colors are: \"red\", \"green\", \"blue\", \"yellow\",\"cyan\", \"orange\", \"purple\", \"pink\", \"brown\", \"grey\", \"white\", \"black\".</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with color names as keys and percentage of color in image as values.</p> Source code in <code>ammico/colors.py</code> <pre><code>def analyse_image(self) -&gt; dict:\n    \"\"\"\n    Uses the colorgram library to extract the n most common colors from the images.\n    One problem is, that the most common colors are taken before beeing categorized,\n    so for small values it might occur that the ten most common colors are shades of grey,\n    while other colors are present but will be ignored. Because of this n_colors=100 was chosen as default.\n\n    The colors are then matched to the closest color in the CSS3 color list using the delta-e metric.\n    They are then merged into one data frame.\n    The colors can be reduced to a smaller list of colors using the get_color_table function.\n    These colors are: \"red\", \"green\", \"blue\", \"yellow\",\"cyan\", \"orange\", \"purple\", \"pink\", \"brown\", \"grey\", \"white\", \"black\".\n\n    Returns:\n        dict: Dictionary with color names as keys and percentage of color in image as values.\n    \"\"\"\n    filename = self.subdict[\"filename\"]\n\n    colors = colorgram.extract(filename, self.n_colors)\n    for color in colors:\n        rgb_name = self.rgb2name(\n            color.rgb,\n            merge_color=self.merge_color,\n            delta_e_method=self.delta_e_method,\n        )\n        self.subdict[rgb_name] += color.proportion\n\n    # ensure color rounding\n    for key in self.set_keys().keys():\n        if self.subdict[key]:\n            self.subdict[key] = round(self.subdict[key], 2)\n\n    return self.subdict\n</code></pre>"},{"location":"api/#ammico.colors.ColorDetector.rgb2name","title":"<code>rgb2name(c, merge_color=True, delta_e_method='CIE 1976')</code>","text":"<p>Take an rgb color as input and return the closest color name from the CSS3 color list.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Union[List, tuple]</code> <p>RGB value.</p> required <code>merge_color</code> <code>(bool, Optional)</code> <p>Whether color name should be reduced, defaults to True.</p> <code>True</code> <p>Returns:     str: Closest matching color name.</p> Source code in <code>ammico/colors.py</code> <pre><code>def rgb2name(\n    self, c, merge_color: bool = True, delta_e_method: str = \"CIE 1976\"\n) -&gt; str:\n    \"\"\"Take an rgb color as input and return the closest color name from the CSS3 color list.\n\n    Args:\n        c (Union[List,tuple]): RGB value.\n        merge_color (bool, Optional): Whether color name should be reduced, defaults to True.\n    Returns:\n        str: Closest matching color name.\n    \"\"\"\n    if len(c) != 3:\n        raise ValueError(\"Input color must be a list or tuple of length 3 (RGB).\")\n\n    h_color = \"#{:02x}{:02x}{:02x}\".format(int(c[0]), int(c[1]), int(c[2]))\n    try:\n        output_color = webcolors.hex_to_name(h_color, spec=\"css3\")\n        output_color = output_color.lower().replace(\"grey\", \"gray\")\n    except ValueError:\n        delta_e_lst = []\n        filtered_colors = webcolors._definitions._CSS3_NAMES_TO_HEX\n\n        for _, img_hex in filtered_colors.items():\n            cur_clr = webcolors.hex_to_rgb(img_hex)\n            # calculate color Delta-E\n            delta_e = colour.delta_E(c, cur_clr, method=delta_e_method)\n            delta_e_lst.append(delta_e)\n        # find lowest delta-e\n        min_diff = np.argsort(delta_e_lst)[0]\n        output_color = (\n            str(list(filtered_colors.items())[min_diff][0])\n            .lower()\n            .replace(\"grey\", \"gray\")\n        )\n\n    # match color to reduced list:\n    if merge_color:\n        for reduced_key, reduced_color_sub_list in get_color_table().items():\n            if str(output_color).lower() in [\n                str(color_name).lower()\n                for color_name in reduced_color_sub_list[\"ColorName\"]\n            ]:\n                output_color = reduced_key.lower()\n                break\n    return output_color\n</code></pre>"},{"location":"api/#utils","title":"Utils","text":""},{"location":"api/#ammico.utils.AnalysisMethod","title":"<code>AnalysisMethod</code>","text":"<p>Base class to be inherited by all analysis methods.</p> Source code in <code>ammico/utils.py</code> <pre><code>class AnalysisMethod:\n    \"\"\"Base class to be inherited by all analysis methods.\"\"\"\n\n    def __init__(self, subdict: dict) -&gt; None:\n        self.subdict = subdict\n        # define keys that will be set by the analysis\n\n    def set_keys(self):\n        raise NotImplementedError()\n\n    def analyse_image(self):\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/#ammico.utils.DownloadResource","title":"<code>DownloadResource</code>","text":"<p>A remote resource that needs on demand downloading.</p> <p>We use this as a wrapper to the pooch library. The wrapper registers each data file and allows prefetching through the CLI entry point ammico_prefetch_models.</p> Source code in <code>ammico/utils.py</code> <pre><code>class DownloadResource:\n    \"\"\"A remote resource that needs on demand downloading.\n\n    We use this as a wrapper to the pooch library. The wrapper registers\n    each data file and allows prefetching through the CLI entry point\n    ammico_prefetch_models.\n    \"\"\"\n\n    # We store a list of defined resouces in a class variable, allowing\n    # us prefetching from a CLI e.g. to bundle into a Docker image\n    resources = []\n\n    def __init__(self, **kwargs):\n        DownloadResource.resources.append(self)\n        self.kwargs = kwargs\n\n    def get(self):\n        return pooch.retrieve(**self.kwargs)\n</code></pre>"},{"location":"api/#ammico.utils.ammico_prefetch_models","title":"<code>ammico_prefetch_models()</code>","text":"<p>Prefetch all the download resources</p> Source code in <code>ammico/utils.py</code> <pre><code>def ammico_prefetch_models():\n    \"\"\"Prefetch all the download resources\"\"\"\n    for res in DownloadResource.resources:\n        res.get()\n</code></pre>"},{"location":"api/#ammico.utils.append_data_to_dict","title":"<code>append_data_to_dict(mydict)</code>","text":"<p>Append entries from nested dictionaries to keys in a global dict.</p> Source code in <code>ammico/utils.py</code> <pre><code>def append_data_to_dict(mydict: dict) -&gt; dict:\n    \"\"\"Append entries from nested dictionaries to keys in a global dict.\"\"\"\n\n    # first initialize empty list for each key that is present\n    outdict = {key: [] for key in list(mydict.values())[0].keys()}\n    # now append the values to each key in a list\n    for subdict in mydict.values():\n        for key in subdict.keys():\n            outdict[key].append(subdict[key])\n    return outdict\n</code></pre>"},{"location":"api/#ammico.utils.dump_df","title":"<code>dump_df(mydict)</code>","text":"<p>Utility to dump the dictionary into a dataframe.</p> Source code in <code>ammico/utils.py</code> <pre><code>def dump_df(mydict: dict) -&gt; DataFrame:\n    \"\"\"Utility to dump the dictionary into a dataframe.\"\"\"\n    return DataFrame.from_dict(mydict)\n</code></pre>"},{"location":"api/#ammico.utils.find_files","title":"<code>find_files(path=None, pattern=None, recursive=True, limit=20, random_seed=None, return_as_list=False)</code>","text":"<p>Find image files on the file system.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The base directory where we are looking for the images. Defaults to None, which uses the ammico data directory if set or the current working directory otherwise.</p> <code>None</code> <code>pattern</code> <code>str | list</code> <p>The naming pattern that the filename should match.     Use either '.ext' or just 'ext'     Defaults to [\"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\",\"tiff\"]. Can be used to allow other patterns or to only include     specific prefixes or suffixes.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Whether to recurse into subdirectories. Default is set to True.</p> <code>True</code> <code>limit</code> <code>int / list</code> <p>The maximum number of images to be found. Provide a list or tuple of length 2 to batch the images. Defaults to 20. To return all images, set to None or -1.</p> <code>20</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for shuffling the images. If None is provided the data will not be shuffeled. Defaults to None.</p> <code>None</code> <code>return_as_list</code> <code>bool</code> <p>Whether to return the list of files instead of a dict. Defaults to False.</p> <code>False</code> <p>Returns:     dict: A nested dictionary with file ids and all filenames including the path.     Or     list: A list of file paths if return_as_list is set to True.</p> Source code in <code>ammico/utils.py</code> <pre><code>def find_files(\n    path: Optional[Union[str, Path, None]] = None,\n    pattern: Optional[Iterable[str]] = None,\n    recursive: bool = True,\n    limit=20,\n    random_seed: int = None,\n    return_as_list: bool = False,\n) -&gt; Union[dict, list]:\n    \"\"\"Find image files on the file system.\n\n    Args:\n        path (str, optional): The base directory where we are looking for the images. Defaults\n            to None, which uses the ammico data directory if set or the current\n            working directory otherwise.\n        pattern (str|list, optional): The naming pattern that the filename should match.\n                Use either '.ext' or just 'ext'\n                Defaults to [\"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\",\"tiff\"]. Can be used to allow other patterns or to only include\n                specific prefixes or suffixes.\n        recursive (bool, optional): Whether to recurse into subdirectories. Default is set to True.\n        limit (int/list, optional): The maximum number of images to be found.\n            Provide a list or tuple of length 2 to batch the images.\n            Defaults to 20. To return all images, set to None or -1.\n        random_seed (int, optional): The random seed to use for shuffling the images.\n            If None is provided the data will not be shuffeled. Defaults to None.\n        return_as_list (bool, optional): Whether to return the list of files instead of a dict.\n            Defaults to False.\n    Returns:\n        dict: A nested dictionary with file ids and all filenames including the path.\n        Or\n        list: A list of file paths if return_as_list is set to True.\n    \"\"\"\n\n    if path is None:\n        path = os.environ.get(\"AMMICO_DATA_HOME\", \".\")\n    if pattern is None:\n        pattern = [\"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\", \"tiff\"]\n\n    if isinstance(pattern, str):\n        pattern = [pattern]\n    results = []\n    for p in pattern:\n        results.extend(_match_pattern(path, p, recursive=recursive))\n\n    if len(results) == 0:\n        raise FileNotFoundError(f\"No files found in {path} with pattern '{pattern}'\")\n\n    if random_seed is not None:\n        random.seed(random_seed)\n        random.shuffle(results)\n\n    images = _limit_results(results, limit)\n\n    if return_as_list:\n        return images\n\n    return initialize_dict(images)\n</code></pre>"},{"location":"api/#ammico.utils.find_videos","title":"<code>find_videos(path=None, pattern=['mp4', 'mov', 'avi', 'mkv', 'webm'], recursive=True, limit=5, random_seed=None)</code>","text":"<p>Find video files on the file system.</p> Source code in <code>ammico/utils.py</code> <pre><code>def find_videos(\n    path: str = None,\n    pattern=[\"mp4\", \"mov\", \"avi\", \"mkv\", \"webm\"],\n    recursive: bool = True,\n    limit=5,\n    random_seed: int = None,\n) -&gt; dict:\n    \"\"\"Find video files on the file system.\"\"\"\n    if path is None:\n        path = os.environ.get(\"AMMICO_DATA_HOME\", \".\")\n    if isinstance(pattern, str):\n        pattern = [pattern]\n    results = []\n    for p in pattern:\n        results.extend(_match_pattern(path, p, recursive=recursive))\n    if len(results) == 0:\n        raise FileNotFoundError(f\"No files found in {path} with pattern '{pattern}'\")\n    if random_seed is not None:\n        random.seed(random_seed)\n        random.shuffle(results)\n    videos = _limit_results(results, limit)\n    return initialize_dict(videos)\n</code></pre>"},{"location":"api/#ammico.utils.get_supported_whisperx_languages","title":"<code>get_supported_whisperx_languages()</code>","text":"<p>Get the list of supported whisperx languages.</p> Source code in <code>ammico/utils.py</code> <pre><code>def get_supported_whisperx_languages() -&gt; List[str]:\n    \"\"\"Get the list of supported whisperx languages.\"\"\"\n    supported_languages = set(DEFAULT_ALIGN_MODELS_TORCH.keys()) | set(\n        DEFAULT_ALIGN_MODELS_HF.keys()\n    )\n    return sorted(supported_languages)\n</code></pre>"},{"location":"api/#ammico.utils.initialize_dict","title":"<code>initialize_dict(filelist)</code>","text":"<p>Initialize the nested dictionary for all the found images.</p> <p>Parameters:</p> Name Type Description Default <code>filelist</code> <code>list</code> <p>The list of files to be analyzed, including their paths.</p> required <p>Returns:     dict: The nested dictionary with all image ids and their paths.</p> Source code in <code>ammico/utils.py</code> <pre><code>def initialize_dict(filelist: list) -&gt; dict:\n    \"\"\"Initialize the nested dictionary for all the found images.\n\n    Args:\n        filelist (list): The list of files to be analyzed, including their paths.\n    Returns:\n        dict: The nested dictionary with all image ids and their paths.\"\"\"\n    mydict = {}\n    for img_path in filelist:\n        id_ = os.path.splitext(os.path.basename(img_path))[0]\n        mydict[id_] = {\"filename\": img_path}\n    return mydict\n</code></pre>"},{"location":"api/#ammico.utils.is_interactive","title":"<code>is_interactive()</code>","text":"<p>Check if we are running in an interactive environment.</p> Source code in <code>ammico/utils.py</code> <pre><code>def is_interactive():\n    \"\"\"Check if we are running in an interactive environment.\"\"\"\n    import __main__ as main\n\n    return not hasattr(main, \"__file__\")\n</code></pre>"},{"location":"api/#ammico.utils.load_image","title":"<code>load_image(image_path)</code>","text":"<p>Load image from file path or return if already PIL Image.</p> Source code in <code>ammico/utils.py</code> <pre><code>def load_image(image_path: Union[str, Path, Image.Image]) -&gt; Image.Image:\n    \"\"\"Load image from file path or return if already PIL Image.\"\"\"\n    if isinstance(image_path, Image.Image):\n        return image_path\n\n    image_path = Path(image_path)\n    if not image_path.exists():\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n\n    return Image.open(image_path).convert(\"RGB\")\n</code></pre>"},{"location":"api/#ammico.utils.prepare_image","title":"<code>prepare_image(image, target_size=(512, 512), resize_mode='resize')</code>","text":"<p>Prepare image for model input with optimal resolution.</p> Source code in <code>ammico/utils.py</code> <pre><code>def prepare_image(\n    image: Image.Image,\n    target_size: Tuple[int, int] = (512, 512),\n    resize_mode: str = \"resize\",\n) -&gt; Image.Image:\n    \"\"\"Prepare image for model input with optimal resolution.\"\"\"\n    width, height = image.size\n    target_w, target_h = target_size\n\n    if resize_mode == \"center_crop\":\n        scale = max(target_w / width, target_h / height)\n        new_width = int(width * scale)\n        new_height = int(height * scale)\n        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n\n        left = (new_width - target_w) // 2\n        top = (new_height - target_h) // 2\n        image = image.crop((left, top, left + target_w, top + target_h))\n    else:\n        image = image.resize(target_size, Image.Resampling.LANCZOS)\n\n    return image\n</code></pre>"},{"location":"api/#display","title":"Display","text":""},{"location":"api/#ammico.display.AnalysisExplorer","title":"<code>AnalysisExplorer</code>","text":"Source code in <code>ammico/display.py</code> <pre><code>class AnalysisExplorer:\n    def __init__(self, mydict: dict) -&gt; None:\n        \"\"\"Initialize the AnalysisExplorer class to create an interactive\n        visualization of the analysis results.\n\n        Args:\n            mydict (dict): A nested dictionary containing image data for all images.\n\n        \"\"\"\n        self.app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n        self.mydict = mydict\n        self.theme = {\n            \"scheme\": \"monokai\",\n            \"author\": \"wimer hazenberg (http://www.monokai.nl)\",\n            \"base00\": \"#272822\",\n            \"base01\": \"#383830\",\n            \"base02\": \"#49483e\",\n            \"base03\": \"#75715e\",\n            \"base04\": \"#a59f85\",\n            \"base05\": \"#f8f8f2\",\n            \"base06\": \"#f5f4f1\",\n            \"base07\": \"#f9f8f5\",\n            \"base08\": \"#f92672\",\n            \"base09\": \"#fd971f\",\n            \"base0A\": \"#f4bf75\",\n            \"base0B\": \"#a6e22e\",\n            \"base0C\": \"#a1efe4\",\n            \"base0D\": \"#66d9ef\",\n            \"base0E\": \"#ae81ff\",\n            \"base0F\": \"#cc6633\",\n        }\n\n        # Setup the layout\n        app_layout = html.Div(\n            [\n                # Top row, only file explorer\n                dbc.Row(\n                    [dbc.Col(self._top_file_explorer(mydict))],\n                    id=\"Div_top\",\n                    style={\n                        \"width\": \"30%\",\n                    },\n                ),\n                # second row, middle picture and right output\n                dbc.Row(\n                    [\n                        # first column: picture\n                        dbc.Col(self._middle_picture_frame()),\n                        dbc.Col(self._right_output_json()),\n                    ]\n                ),\n            ],\n            # style={\"width\": \"95%\", \"display\": \"inline-block\"},\n        )\n        self.app.layout = app_layout\n\n        # Add callbacks to the app\n        self.app.callback(\n            Output(\"img_middle_picture_id\", \"src\"),\n            Input(\"left_select_id\", \"value\"),\n            prevent_initial_call=True,\n        )(self.update_picture)\n\n        self.app.callback(\n            Output(\"right_json_viewer\", \"children\"),\n            Input(\"button_run\", \"n_clicks\"),\n            State(\"left_select_id\", \"options\"),\n            State(\"left_select_id\", \"value\"),\n            State(\"Dropdown_select_Detector\", \"value\"),\n            State(\"Dropdown_analysis_type\", \"value\"),\n            State(\"textarea_questions\", \"value\"),\n            State(\"setting_privacy_env_var\", \"value\"),\n            State(\"setting_Color_delta_e_method\", \"value\"),\n            prevent_initial_call=True,\n        )(self._right_output_analysis)\n\n        self.app.callback(\n            Output(\"settings_TextDetector\", \"style\"),\n            Output(\"settings_ColorDetector\", \"style\"),\n            Output(\"settings_VQA\", \"style\"),\n            Input(\"Dropdown_select_Detector\", \"value\"),\n        )(self._update_detector_setting)\n\n        self.app.callback(\n            Output(\"textarea_questions\", \"style\"),\n            Input(\"Dropdown_analysis_type\", \"value\"),\n        )(self._show_questions_textarea_on_demand)\n\n    # I split the different sections into subfunctions for better clarity\n    def _top_file_explorer(self, mydict: dict) -&gt; html.Div:\n        \"\"\"Initialize the file explorer dropdown for selecting the file to be analyzed.\n\n        Args:\n            mydict (dict): A dictionary containing image data.\n\n        Returns:\n            html.Div: The layout for the file explorer dropdown.\n        \"\"\"\n        left_layout = html.Div(\n            [\n                dcc.Dropdown(\n                    options={value[\"filename\"]: key for key, value in mydict.items()},\n                    id=\"left_select_id\",\n                )\n            ]\n        )\n        return left_layout\n\n    def _middle_picture_frame(self) -&gt; html.Div:\n        \"\"\"Initialize the picture frame to display the image.\n\n        Returns:\n            html.Div: The layout for the picture frame.\n        \"\"\"\n        middle_layout = html.Div(\n            [\n                html.Img(\n                    id=\"img_middle_picture_id\",\n                    style={\n                        \"width\": \"80%\",\n                    },\n                )\n            ]\n        )\n        return middle_layout\n\n    def _create_setting_layout(self):\n        settings_layout = html.Div(\n            [\n                # text summary start\n                html.Div(\n                    id=\"settings_TextDetector\",\n                    style={\"display\": \"none\"},\n                    children=[\n                        # row 1\n                        dbc.Row(\n                            dbc.Col(\n                                [\n                                    html.P(\n                                        \"Privacy disclosure acceptance environment variable\"\n                                    ),\n                                    dcc.Input(\n                                        type=\"text\",\n                                        value=\"PRIVACY_AMMICO\",\n                                        id=\"setting_privacy_env_var\",\n                                        style={\"width\": \"100%\"},\n                                    ),\n                                ],\n                                align=\"start\",\n                            ),\n                        ),\n                    ],\n                ),  # text summary end\n                html.Div(\n                    id=\"settings_ColorDetector\",\n                    style={\"display\": \"none\"},\n                    children=[\n                        html.Div(\n                            [\n                                dcc.Dropdown(\n                                    options=COLOR_SCHEMES,\n                                    value=\"CIE 1976\",\n                                    id=\"setting_Color_delta_e_method\",\n                                )\n                            ],\n                            style={\n                                \"width\": \"49%\",\n                                \"display\": \"inline-block\",\n                                \"margin-top\": \"10px\",\n                            },\n                        )\n                    ],\n                ),\n                # start VQA settings\n                html.Div(\n                    id=\"settings_VQA\",\n                    style={\"display\": \"none\"},\n                    children=[\n                        dbc.Card(\n                            [\n                                dbc.CardBody(\n                                    [\n                                        dbc.Row(\n                                            dbc.Col(\n                                                dcc.Dropdown(\n                                                    id=\"Dropdown_analysis_type\",\n                                                    options=[\n                                                        {\"label\": v, \"value\": v}\n                                                        for v in SUMMARY_ANALYSIS_TYPE\n                                                    ],\n                                                    value=\"summary_and_questions\",\n                                                    clearable=False,\n                                                    style={\n                                                        \"width\": \"100%\",\n                                                        \"minWidth\": \"240px\",\n                                                        \"maxWidth\": \"520px\",\n                                                    },\n                                                ),\n                                            ),\n                                            justify=\"start\",\n                                        ),\n                                        html.Div(style={\"height\": \"8px\"}),\n                                        dbc.Row(\n                                            [\n                                                dbc.Col(\n                                                    dcc.Textarea(\n                                                        id=\"textarea_questions\",\n                                                        value=\"Are there people in the image?\\nWhat is this picture about?\",\n                                                        placeholder=\"One question per line...\",\n                                                        style={\n                                                            \"width\": \"100%\",\n                                                            \"minHeight\": \"160px\",\n                                                            \"height\": \"220px\",\n                                                            \"resize\": \"vertical\",\n                                                            \"overflow\": \"auto\",\n                                                        },\n                                                        rows=8,\n                                                    ),\n                                                    width=12,\n                                                ),\n                                            ],\n                                            justify=\"start\",\n                                        ),\n                                    ]\n                                )\n                            ],\n                            style={\n                                \"width\": \"100%\",\n                                \"marginTop\": \"10px\",\n                                \"zIndex\": 2000,\n                            },\n                        )\n                    ],\n                ),\n            ],\n            style={\"width\": \"100%\", \"display\": \"inline-block\", \"overflow\": \"visible\"},\n        )\n        return settings_layout\n\n    def _right_output_json(self) -&gt; html.Div:\n        \"\"\"Initialize the DetectorDropdown, argument Div and JSON viewer for displaying the analysis output.\n\n        Returns:\n            html.Div: The layout for the JSON viewer.\n        \"\"\"\n        right_layout = html.Div(\n            [\n                dbc.Col(\n                    [\n                        dbc.Row(\n                            dcc.Dropdown(\n                                options=[\n                                    \"TextDetector\",\n                                    \"ColorDetector\",\n                                    \"VQA\",\n                                ],\n                                value=\"TextDetector\",\n                                id=\"Dropdown_select_Detector\",\n                                style={\"width\": \"60%\"},\n                            ),\n                            justify=\"start\",\n                        ),\n                        dbc.Row(\n                            children=[self._create_setting_layout()],\n                            id=\"div_detector_args\",\n                            justify=\"start\",\n                        ),\n                        dbc.Row(\n                            html.Button(\n                                \"Run Detector\",\n                                id=\"button_run\",\n                                style={\n                                    \"margin-top\": \"15px\",\n                                    \"margin-bottom\": \"15px\",\n                                    \"margin-left\": \"11px\",\n                                    \"width\": \"30%\",\n                                },\n                            ),\n                            justify=\"start\",\n                        ),\n                        dbc.Row(\n                            dcc.Loading(\n                                id=\"loading-2\",\n                                children=[\n                                    # This is where the json is shown.\n                                    html.Div(id=\"right_json_viewer\"),\n                                ],\n                                type=\"circle\",\n                            ),\n                            justify=\"start\",\n                        ),\n                    ],\n                    align=\"start\",\n                )\n            ]\n        )\n        return right_layout\n\n    def run_server(self, port: int = 8050) -&gt; None:\n        \"\"\"Run the Dash server to start the analysis explorer.\n\n\n        Args:\n            port (int, optional): The port number to run the server on (default: 8050).\n        \"\"\"\n\n        self.app.run(debug=True, port=port)\n\n    # Dash callbacks\n    def update_picture(self, img_path: str) -&gt; Optional[Image.Image]:\n        \"\"\"Callback function to update the displayed image.\n\n        Args:\n            img_path (str): The path of the selected image.\n\n        Returns:\n            Union[PIL.PngImagePlugin, None]: The image object to be displayed\n                or None if the image path is\n\n        \"\"\"\n        if img_path is not None:\n            image = Image.open(img_path)\n            return image\n        else:\n            return None\n\n    def _update_detector_setting(self, setting_input):\n        # return settings_TextDetector -&gt; style,\n        display_none = {\"display\": \"none\"}\n        display_flex = {\n            \"display\": \"flex\",\n            \"flexWrap\": \"wrap\",\n            \"width\": 400,\n            \"margin-top\": \"20px\",\n        }\n\n        if setting_input == \"TextDetector\":\n            return display_flex, display_none, display_none, display_none\n        if setting_input == \"ColorDetector\":\n            return display_none, display_none, display_flex, display_none\n        if setting_input == \"VQA\":\n            return display_none, display_none, display_none, display_flex\n        else:\n            return display_none, display_none, display_none, display_none\n\n    def _parse_questions(self, text: Optional[str]) -&gt; Optional[List[str]]:\n        if not text:\n            return None\n        qs = [q.strip() for q in text.splitlines() if q.strip()]\n        return qs if qs else None\n\n    def _right_output_analysis(\n        self,\n        n_clicks,\n        all_img_options: dict,\n        current_img_value: str,\n        detector_value: str,\n        analysis_type_value: str,\n        textarea_questions_value: str,\n        setting_privacy_env_var: str,\n        setting_color_delta_e_method: str,\n    ) -&gt; dict:\n        \"\"\"Callback function to perform analysis on the selected image and return the output.\n\n        Args:\n            all_options (dict): The available options in the file explorer dropdown.\n            current_value (str): The current selected value in the file explorer dropdown.\n\n        Returns:\n            dict: The analysis output for the selected image.\n        \"\"\"\n        identify_dict = {\n            \"TextDetector\": text.TextDetector,\n            \"ColorDetector\": colors.ColorDetector,\n            \"VQA\": image_summary.ImageSummaryDetector,\n        }\n\n        # Get image ID from dropdown value, which is the filepath\n        if current_img_value is None:\n            return {}\n        image_id = all_img_options[current_img_value]\n        image_copy = self.mydict.get(image_id, {}).copy()\n\n        analysis_dict: Dict[str, Any] = {}\n        if detector_value == \"VQA\":\n            try:\n                qwen_model = MultimodalSummaryModel(\n                    model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\"\n                )  # TODO: allow user to specify model\n                vqa_cls = identify_dict.get(\"VQA\")\n                vqa_detector = vqa_cls(qwen_model, subdict={})\n                questions_list = self._parse_questions(textarea_questions_value)\n                analysis_result = vqa_detector.analyse_image(\n                    image_copy,\n                    analysis_type=analysis_type_value,\n                    list_of_questions=questions_list,\n                    is_concise_summary=True,\n                    is_concise_answer=True,\n                )\n                analysis_dict = analysis_result or {}\n            except Exception as e:\n                warnings.warn(f\"VQA/Image tasks failed: {e}\")\n                analysis_dict = {\"image_tasks_error\": str(e)}\n        else:\n            # detector value is the string name of the chosen detector\n            identify_function = identify_dict[detector_value]\n\n            if detector_value == \"TextDetector\":\n                detector_class = identify_function(\n                    image_copy,\n                    accept_privacy=(\n                        setting_privacy_env_var\n                        if setting_privacy_env_var\n                        else \"PRIVACY_AMMICO\"\n                    ),\n                )\n            elif detector_value == \"ColorDetector\":\n                detector_class = identify_function(\n                    image_copy,\n                    delta_e_method=setting_color_delta_e_method,\n                )\n            else:\n                detector_class = identify_function(image_copy)\n\n            analysis_dict = detector_class.analyse_image()\n\n        new_analysis_dict: Dict[str, Any] = {}\n\n        # Iterate over the items in the original dictionary\n        for k, v in analysis_dict.items():\n            # Check if the value is a list\n            if isinstance(v, list):\n                # If it is, convert each item in the list to a string and join them with a comma\n                new_value = \", \".join([str(f) for f in v])\n            else:\n                # If it's not a list, keep the value as it is\n                new_value = v\n\n            # Add the new key-value pair to the new dictionary\n            new_analysis_dict[k] = new_value\n\n        df = pd.DataFrame([new_analysis_dict]).set_index(\"filename\").T\n        df.index.rename(\"filename\", inplace=True)\n        return dbc.Table.from_dataframe(\n            df, striped=True, bordered=True, hover=True, index=True\n        )\n\n    def _show_questions_textarea_on_demand(self, analysis_type_value: str) -&gt; dict:\n        if analysis_type_value in (\"questions\", \"summary_and_questions\"):\n            return {\"display\": \"block\", \"width\": \"100%\"}\n        else:\n            return {\"display\": \"none\"}\n</code></pre>"},{"location":"api/#ammico.display.AnalysisExplorer.__init__","title":"<code>__init__(mydict)</code>","text":"<p>Initialize the AnalysisExplorer class to create an interactive visualization of the analysis results.</p> <p>Parameters:</p> Name Type Description Default <code>mydict</code> <code>dict</code> <p>A nested dictionary containing image data for all images.</p> required Source code in <code>ammico/display.py</code> <pre><code>def __init__(self, mydict: dict) -&gt; None:\n    \"\"\"Initialize the AnalysisExplorer class to create an interactive\n    visualization of the analysis results.\n\n    Args:\n        mydict (dict): A nested dictionary containing image data for all images.\n\n    \"\"\"\n    self.app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n    self.mydict = mydict\n    self.theme = {\n        \"scheme\": \"monokai\",\n        \"author\": \"wimer hazenberg (http://www.monokai.nl)\",\n        \"base00\": \"#272822\",\n        \"base01\": \"#383830\",\n        \"base02\": \"#49483e\",\n        \"base03\": \"#75715e\",\n        \"base04\": \"#a59f85\",\n        \"base05\": \"#f8f8f2\",\n        \"base06\": \"#f5f4f1\",\n        \"base07\": \"#f9f8f5\",\n        \"base08\": \"#f92672\",\n        \"base09\": \"#fd971f\",\n        \"base0A\": \"#f4bf75\",\n        \"base0B\": \"#a6e22e\",\n        \"base0C\": \"#a1efe4\",\n        \"base0D\": \"#66d9ef\",\n        \"base0E\": \"#ae81ff\",\n        \"base0F\": \"#cc6633\",\n    }\n\n    # Setup the layout\n    app_layout = html.Div(\n        [\n            # Top row, only file explorer\n            dbc.Row(\n                [dbc.Col(self._top_file_explorer(mydict))],\n                id=\"Div_top\",\n                style={\n                    \"width\": \"30%\",\n                },\n            ),\n            # second row, middle picture and right output\n            dbc.Row(\n                [\n                    # first column: picture\n                    dbc.Col(self._middle_picture_frame()),\n                    dbc.Col(self._right_output_json()),\n                ]\n            ),\n        ],\n        # style={\"width\": \"95%\", \"display\": \"inline-block\"},\n    )\n    self.app.layout = app_layout\n\n    # Add callbacks to the app\n    self.app.callback(\n        Output(\"img_middle_picture_id\", \"src\"),\n        Input(\"left_select_id\", \"value\"),\n        prevent_initial_call=True,\n    )(self.update_picture)\n\n    self.app.callback(\n        Output(\"right_json_viewer\", \"children\"),\n        Input(\"button_run\", \"n_clicks\"),\n        State(\"left_select_id\", \"options\"),\n        State(\"left_select_id\", \"value\"),\n        State(\"Dropdown_select_Detector\", \"value\"),\n        State(\"Dropdown_analysis_type\", \"value\"),\n        State(\"textarea_questions\", \"value\"),\n        State(\"setting_privacy_env_var\", \"value\"),\n        State(\"setting_Color_delta_e_method\", \"value\"),\n        prevent_initial_call=True,\n    )(self._right_output_analysis)\n\n    self.app.callback(\n        Output(\"settings_TextDetector\", \"style\"),\n        Output(\"settings_ColorDetector\", \"style\"),\n        Output(\"settings_VQA\", \"style\"),\n        Input(\"Dropdown_select_Detector\", \"value\"),\n    )(self._update_detector_setting)\n\n    self.app.callback(\n        Output(\"textarea_questions\", \"style\"),\n        Input(\"Dropdown_analysis_type\", \"value\"),\n    )(self._show_questions_textarea_on_demand)\n</code></pre>"},{"location":"api/#ammico.display.AnalysisExplorer.run_server","title":"<code>run_server(port=8050)</code>","text":"<p>Run the Dash server to start the analysis explorer.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>The port number to run the server on (default: 8050).</p> <code>8050</code> Source code in <code>ammico/display.py</code> <pre><code>def run_server(self, port: int = 8050) -&gt; None:\n    \"\"\"Run the Dash server to start the analysis explorer.\n\n\n    Args:\n        port (int, optional): The port number to run the server on (default: 8050).\n    \"\"\"\n\n    self.app.run(debug=True, port=port)\n</code></pre>"},{"location":"api/#ammico.display.AnalysisExplorer.update_picture","title":"<code>update_picture(img_path)</code>","text":"<p>Callback function to update the displayed image.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>The path of the selected image.</p> required <p>Returns:</p> Type Description <code>Optional[Image]</code> <p>Union[PIL.PngImagePlugin, None]: The image object to be displayed or None if the image path is</p> Source code in <code>ammico/display.py</code> <pre><code>def update_picture(self, img_path: str) -&gt; Optional[Image.Image]:\n    \"\"\"Callback function to update the displayed image.\n\n    Args:\n        img_path (str): The path of the selected image.\n\n    Returns:\n        Union[PIL.PngImagePlugin, None]: The image object to be displayed\n            or None if the image path is\n\n    \"\"\"\n    if img_path is not None:\n        image = Image.open(img_path)\n        return image\n    else:\n        return None\n</code></pre>"},{"location":"api/#model","title":"Model","text":""},{"location":"api/#ammico.model.AudioToTextModel","title":"<code>AudioToTextModel</code>","text":"Source code in <code>ammico/model.py</code> <pre><code>class AudioToTextModel:\n    def __init__(\n        self,\n        model_size: str = \"large\",\n        device: Optional[str] = None,\n        language: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for WhisperX model loading and inference.\n        Args:\n            model_size: Size of Whisper model to load (small, base, large).\n            device: \"cuda\" or \"cpu\" (auto-detected when None).\n            language: ISO-639-1 language code (e.g., \"en\", \"fr\", \"de\").\n                     If None, language will be detected automatically.\n                     Set this to avoid unreliable detection on small clips.\n        \"\"\"\n        self.device = resolve_model_device(device)\n\n        self.model_size = resolve_model_size(model_size)\n\n        self.model = None\n\n        self.language = self._validate_language(language)\n\n        self._load_model()\n\n    def _validate_language(self, language: Optional[str]) -&gt; Optional[str]:\n        \"\"\"\n\n        Validate the provided language code against whisperx's supported languages.\n        Args:\n            language: ISO-639-1 language code (e.g., \"en\", \"fr\", \"de\").\n        Returns:\n            Validated language code or None.\n        Raises:\n            ValueError: If the language code is invalid or unsupported.\n        \"\"\"\n\n        if not language:\n            return None\n\n        language = language.strip().lower()\n        supported_languages = get_supported_whisperx_languages()\n\n        if len(language) != 2:\n            raise ValueError(\n                f\"Invalid language code: '{language}'. Language codes must be 2 letters.\"\n            )\n\n        if not language.isalpha():\n            raise ValueError(\n                f\"Invalid language code: '{language}'. Language codes must contain only alphabetic characters.\"\n            )\n\n        if language not in supported_languages:\n            raise ValueError(\n                f\"Unsupported language code: '{language}'. Supported: {sorted(supported_languages)}\"\n            )\n\n        return language\n\n    def _load_model(self):\n        if self.device == \"cuda\":\n            self.model = whisperx.load_model(\n                self.model_size,\n                device=self.device,\n                compute_type=\"float16\",\n                language=self.language,\n            )\n        else:\n            self.model = whisperx.load_model(\n                self.model_size,\n                device=self.device,\n                compute_type=\"int8\",\n                language=self.language,\n            )\n\n    def close(self) -&gt; None:\n        \"\"\"Free model resources (helpful in long-running processes).\"\"\"\n        try:\n            if self.model is not None:\n                del self.model\n                self.model = None\n        finally:\n            try:\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            except Exception as e:\n                warnings.warn(\n                    \"Failed to empty CUDA cache. This is not critical, but may lead to memory lingering: \"\n                    f\"{e!r}\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n</code></pre>"},{"location":"api/#ammico.model.AudioToTextModel.__init__","title":"<code>__init__(model_size='large', device=None, language=None)</code>","text":"<p>Class for WhisperX model loading and inference. Args:     model_size: Size of Whisper model to load (small, base, large).     device: \"cuda\" or \"cpu\" (auto-detected when None).     language: ISO-639-1 language code (e.g., \"en\", \"fr\", \"de\").              If None, language will be detected automatically.              Set this to avoid unreliable detection on small clips.</p> Source code in <code>ammico/model.py</code> <pre><code>def __init__(\n    self,\n    model_size: str = \"large\",\n    device: Optional[str] = None,\n    language: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Class for WhisperX model loading and inference.\n    Args:\n        model_size: Size of Whisper model to load (small, base, large).\n        device: \"cuda\" or \"cpu\" (auto-detected when None).\n        language: ISO-639-1 language code (e.g., \"en\", \"fr\", \"de\").\n                 If None, language will be detected automatically.\n                 Set this to avoid unreliable detection on small clips.\n    \"\"\"\n    self.device = resolve_model_device(device)\n\n    self.model_size = resolve_model_size(model_size)\n\n    self.model = None\n\n    self.language = self._validate_language(language)\n\n    self._load_model()\n</code></pre>"},{"location":"api/#ammico.model.AudioToTextModel.close","title":"<code>close()</code>","text":"<p>Free model resources (helpful in long-running processes).</p> Source code in <code>ammico/model.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Free model resources (helpful in long-running processes).\"\"\"\n    try:\n        if self.model is not None:\n            del self.model\n            self.model = None\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception as e:\n            warnings.warn(\n                \"Failed to empty CUDA cache. This is not critical, but may lead to memory lingering: \"\n                f\"{e!r}\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n</code></pre>"},{"location":"api/#ammico.model.MultimodalEmbeddingsModel","title":"<code>MultimodalEmbeddingsModel</code>","text":"Source code in <code>ammico/model.py</code> <pre><code>class MultimodalEmbeddingsModel:\n    def __init__(\n        self,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for Multimodal Embeddings model loading and inference. Uses Jina CLIP-V2 model.\n        Args:\n            device: \"cuda\" or \"cpu\" (auto-detected when None).\n        \"\"\"\n        self.device = resolve_model_device(device)\n\n        model_id = \"jinaai/jina-clip-v2\"\n\n        self.model = SentenceTransformer(\n            model_id,\n            device=self.device,\n            trust_remote_code=True,\n            model_kwargs={\"torch_dtype\": \"auto\"},\n        )\n\n        self.model.eval()\n\n        self.embedding_dim = 1024\n\n    @torch.inference_mode()\n    def encode_text(\n        self,\n        texts: Union[str, List[str]],\n        batch_size: int = 64,\n        truncate_dim: Optional[int] = None,\n    ) -&gt; Union[torch.Tensor, np.ndarray]:\n        if isinstance(texts, str):\n            texts = [texts]\n\n        convert_to_tensor = self.device == \"cuda\"\n        convert_to_numpy = not convert_to_tensor\n\n        embeddings = self.model.encode(\n            texts,\n            batch_size=batch_size,\n            convert_to_tensor=convert_to_tensor,\n            convert_to_numpy=convert_to_numpy,\n            normalize_embeddings=True,\n        )\n\n        if truncate_dim is not None:\n            if not (64 &lt;= truncate_dim &lt;= self.embedding_dim):\n                raise ValueError(\n                    f\"truncate_dim must be between 64 and {self.embedding_dim}\"\n                )\n            embeddings = embeddings[:, :truncate_dim]\n        return embeddings\n\n    @torch.inference_mode()\n    def encode_image(\n        self,\n        images: Union[Image.Image, List[Image.Image]],\n        batch_size: int = 32,\n        truncate_dim: Optional[int] = None,\n    ) -&gt; Union[torch.Tensor, np.ndarray]:\n        if not isinstance(images, (Image.Image, list)):\n            raise ValueError(\n                \"images must be a PIL.Image or a list of PIL.Image objects. Please load images properly.\"\n            )\n\n        convert_to_tensor = self.device == \"cuda\"\n        convert_to_numpy = not convert_to_tensor\n\n        embeddings = self.model.encode(\n            images if isinstance(images, list) else [images],\n            batch_size=batch_size,\n            convert_to_tensor=convert_to_tensor,\n            convert_to_numpy=convert_to_numpy,\n            normalize_embeddings=True,\n        )\n\n        if truncate_dim is not None:\n            if not (64 &lt;= truncate_dim &lt;= self.embedding_dim):\n                raise ValueError(\n                    f\"truncate_dim must be between 64 and {self.embedding_dim}\"\n                )\n            embeddings = embeddings[:, :truncate_dim]\n\n        return embeddings\n\n    def close(self) -&gt; None:\n        \"\"\"Free model resources (helpful in long-running processes).\"\"\"\n        try:\n            if self.model is not None:\n                del self.model\n                self.model = None\n        finally:\n            try:\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            except Exception as e:\n                warnings.warn(\n                    \"Failed to empty CUDA cache. This is not critical, but may lead to memory lingering: \"\n                    f\"{e!r}\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n</code></pre>"},{"location":"api/#ammico.model.MultimodalEmbeddingsModel.__init__","title":"<code>__init__(device=None)</code>","text":"<p>Class for Multimodal Embeddings model loading and inference. Uses Jina CLIP-V2 model. Args:     device: \"cuda\" or \"cpu\" (auto-detected when None).</p> Source code in <code>ammico/model.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Class for Multimodal Embeddings model loading and inference. Uses Jina CLIP-V2 model.\n    Args:\n        device: \"cuda\" or \"cpu\" (auto-detected when None).\n    \"\"\"\n    self.device = resolve_model_device(device)\n\n    model_id = \"jinaai/jina-clip-v2\"\n\n    self.model = SentenceTransformer(\n        model_id,\n        device=self.device,\n        trust_remote_code=True,\n        model_kwargs={\"torch_dtype\": \"auto\"},\n    )\n\n    self.model.eval()\n\n    self.embedding_dim = 1024\n</code></pre>"},{"location":"api/#ammico.model.MultimodalEmbeddingsModel.close","title":"<code>close()</code>","text":"<p>Free model resources (helpful in long-running processes).</p> Source code in <code>ammico/model.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Free model resources (helpful in long-running processes).\"\"\"\n    try:\n        if self.model is not None:\n            del self.model\n            self.model = None\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception as e:\n            warnings.warn(\n                \"Failed to empty CUDA cache. This is not critical, but may lead to memory lingering: \"\n                f\"{e!r}\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n</code></pre>"},{"location":"api/#ammico.model.MultimodalSummaryModel","title":"<code>MultimodalSummaryModel</code>","text":"Source code in <code>ammico/model.py</code> <pre><code>class MultimodalSummaryModel:\n    DEFAULT_CUDA_MODEL = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n    DEFAULT_CPU_MODEL = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n\n    def __init__(\n        self,\n        model_id: Optional[str] = None,\n        device: Optional[str] = None,\n        cache_dir: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for QWEN-2.5-VL model loading and inference.\n        Args:\n            model_id: Type of model to load, defaults to a smaller version for CPU if device is \"cpu\".\n            device: \"cuda\" or \"cpu\" (auto-detected when None).\n            cache_dir: huggingface cache dir (optional).\n        \"\"\"\n        self.device = resolve_model_device(device)\n\n        if model_id is not None and model_id not in (\n            self.DEFAULT_CUDA_MODEL,\n            self.DEFAULT_CPU_MODEL,\n        ):\n            raise ValueError(\n                f\"model_id must be one of {self.DEFAULT_CUDA_MODEL} or {self.DEFAULT_CPU_MODEL}\"\n            )\n\n        self.model_id = model_id or (\n            self.DEFAULT_CUDA_MODEL if self.device == \"cuda\" else self.DEFAULT_CPU_MODEL\n        )\n\n        self.cache_dir = cache_dir\n        self._trust_remote_code = True\n        self._quantize = True\n\n        self.model = None\n        self.processor = None\n        self.tokenizer = None\n\n        self._load_model_and_processor()\n\n    def _load_model_and_processor(self):\n        load_kwargs = {\"trust_remote_code\": self._trust_remote_code, \"use_cache\": True}\n        if self.cache_dir:\n            load_kwargs[\"cache_dir\"] = self.cache_dir\n\n        self.processor = AutoProcessor.from_pretrained(\n            self.model_id, padding_side=\"left\", **load_kwargs\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, **load_kwargs)\n\n        if self.device == \"cuda\":\n            compute_dtype = (\n                torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n            )\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=compute_dtype,\n            )\n            load_kwargs[\"quantization_config\"] = bnb_config\n            load_kwargs[\"device_map\"] = \"auto\"\n\n        else:\n            load_kwargs.pop(\"quantization_config\", None)\n            load_kwargs.pop(\"device_map\", None)\n\n        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            self.model_id, **load_kwargs\n        )\n        self.model.eval()\n\n    def close(self) -&gt; None:\n        \"\"\"Free model resources (helpful in long-running processes).\"\"\"\n        try:\n            if self.model is not None:\n                del self.model\n                self.model = None\n            if self.processor is not None:\n                del self.processor\n                self.processor = None\n            if self.tokenizer is not None:\n                del self.tokenizer\n                self.tokenizer = None\n        finally:\n            try:\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            except Exception as e:\n                warnings.warn(\n                    \"Failed to empty CUDA cache. This is not critical, but may lead to memory lingering: \"\n                    f\"{e!r}\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n</code></pre>"},{"location":"api/#ammico.model.MultimodalSummaryModel.__init__","title":"<code>__init__(model_id=None, device=None, cache_dir=None)</code>","text":"<p>Class for QWEN-2.5-VL model loading and inference. Args:     model_id: Type of model to load, defaults to a smaller version for CPU if device is \"cpu\".     device: \"cuda\" or \"cpu\" (auto-detected when None).     cache_dir: huggingface cache dir (optional).</p> Source code in <code>ammico/model.py</code> <pre><code>def __init__(\n    self,\n    model_id: Optional[str] = None,\n    device: Optional[str] = None,\n    cache_dir: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Class for QWEN-2.5-VL model loading and inference.\n    Args:\n        model_id: Type of model to load, defaults to a smaller version for CPU if device is \"cpu\".\n        device: \"cuda\" or \"cpu\" (auto-detected when None).\n        cache_dir: huggingface cache dir (optional).\n    \"\"\"\n    self.device = resolve_model_device(device)\n\n    if model_id is not None and model_id not in (\n        self.DEFAULT_CUDA_MODEL,\n        self.DEFAULT_CPU_MODEL,\n    ):\n        raise ValueError(\n            f\"model_id must be one of {self.DEFAULT_CUDA_MODEL} or {self.DEFAULT_CPU_MODEL}\"\n        )\n\n    self.model_id = model_id or (\n        self.DEFAULT_CUDA_MODEL if self.device == \"cuda\" else self.DEFAULT_CPU_MODEL\n    )\n\n    self.cache_dir = cache_dir\n    self._trust_remote_code = True\n    self._quantize = True\n\n    self.model = None\n    self.processor = None\n    self.tokenizer = None\n\n    self._load_model_and_processor()\n</code></pre>"},{"location":"api/#ammico.model.MultimodalSummaryModel.close","title":"<code>close()</code>","text":"<p>Free model resources (helpful in long-running processes).</p> Source code in <code>ammico/model.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Free model resources (helpful in long-running processes).\"\"\"\n    try:\n        if self.model is not None:\n            del self.model\n            self.model = None\n        if self.processor is not None:\n            del self.processor\n            self.processor = None\n        if self.tokenizer is not None:\n            del self.tokenizer\n            self.tokenizer = None\n    finally:\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        except Exception as e:\n            warnings.warn(\n                \"Failed to empty CUDA cache. This is not critical, but may lead to memory lingering: \"\n                f\"{e!r}\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n</code></pre>"},{"location":"api/#prompt-builder","title":"Prompt Builder","text":""},{"location":"api/#ammico.prompt_builder.ProcessingLevel","title":"<code>ProcessingLevel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Define the three processing levels in a pipeline. FRAME: individual frame analysis CLIP: video segment (multiple frames) VIDEO: full video (multiple clips)</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>class ProcessingLevel(Enum):\n    \"\"\"Define the three processing levels in a pipeline.\n    FRAME: individual frame analysis\n    CLIP: video segment (multiple frames)\n    VIDEO: full video (multiple clips)\"\"\"\n\n    FRAME = \"frame\"\n    CLIP = \"clip\"\n    VIDEO = \"video\"\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder","title":"<code>PromptBuilder</code>","text":"<p>Modular prompt builder for multi-level video analysis. Handles frame-level, clip-level, and video-level prompts.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>class PromptBuilder:\n    \"\"\"\n    Modular prompt builder for multi-level video analysis.\n    Handles frame-level, clip-level, and video-level prompts.\n    \"\"\"\n\n    ROLE_MODULE = \"\"\"You are a precise video analysis AI. Your purpose is to:\n    - Extract only information explicitly present in provided sources\n    - Never hallucinate or infer beyond what is shown\n    - Generate clear, concise, well-structured outputs\n    - Maintain logical coherence across visual and audio sources\"\"\"\n\n    CONSTRAINTS_MODULE = \"\"\"## Quality Requirements\n\n    - **Accuracy:** Use only explicitly provided information\n    - **Conciseness:** Eliminate redundancy; be direct\n    - **Clarity:** Use accessible language\n    - **Consistency:** Align audio and visual information when both exist\n    - **Format Compliance:** Follow output format exactly\"\"\"\n\n    @staticmethod\n    def visual_frames_module() -&gt; str:\n        \"\"\"For frame-level processing with actual images.\"\"\"\n        str_to_return = \"\"\"## Visual Information\n\n        Visual information is represented by keyframe extracted from the video segment at specified timestamp.\"\"\"\n        return str_to_return\n\n    @staticmethod\n    def visual_captions_module(frame_bullets: List[str]) -&gt; str:\n        \"\"\"For clip-level processing with frame summaries.\"\"\"\n        bullets_text = \"\\n\".join(frame_bullets)\n        str_to_return = f\"\"\"## Visual Information\n\n        The following are summary bullets extracted from video frames with timestamps.\n        These bullets represent the visual content detected in each frame of the video segment:\n\n        {bullets_text}\"\"\"\n\n        return str_to_return\n\n    @staticmethod\n    def visual_captions_final_module(clip_summaries: List[str]) -&gt; str:\n        \"\"\"For video-level processing with clip summaries.\"\"\"\n        str_to_return = f\"\"\"## Visual Information\n\n        The following are brief summaries obtained for each segment of the video.\n        These summaries are associated with the timestamp of each segment's beginning:\n\n        {clip_summaries}\"\"\"\n\n        return str_to_return\n\n    @staticmethod\n    def audio_module(audio_transcription: List[Dict[str, Any]]) -&gt; str:\n        \"\"\"Audio transcription with timestamps.\"\"\"\n        audio_text = \"\\n\".join(\n            [\n                f\"[{a['start_time']:.2f}s - {a['end_time']:.2f}s]: {a['text'].strip()}\"\n                for a in audio_transcription\n            ]\n        )\n        str_to_return = f\"\"\"## Audio Information\n\n        The following is the audio transcription for the same video segment,\n        with precise timestamps for each spoken element:\n\n        {audio_text}\"\"\"\n        return str_to_return\n\n    @staticmethod\n    def summary_task(has_audio: bool = False) -&gt; str:\n        \"\"\"Generate summary task (with or without audio).\"\"\"\n        sources = \"visual and audio information\" if has_audio else \"visual information\"\n        str_to_return = f\"\"\"## Task: Generate Concise Summary\n\n        Based on the {sources} provided, generate a brief summary that:\n        - Captures and summarizes the main events and themes\n        - Uses clear, accessible language\n        - Is between 1-3 sentences\n        - Contains no unsupported claims\n\n        Return ONLY this format:\n\n        Summary: &lt;your summary here&gt;\"\"\"\n        return str_to_return\n\n    @staticmethod\n    def summary_vqa_task(\n        level: ProcessingLevel,\n        has_audio: bool = False,\n    ) -&gt; str:\n        \"\"\"Generate summary+VQA task (adapts based on level and audio). For Frame and Clip levels.\"\"\"\n\n        if level == ProcessingLevel.FRAME:\n            vqa_task = \"\"\"Answer the provided questions based ONLY on information from the visual information. Answers must be brief and direct.\n\n            **Critical Rule:** If you cannot answer a question from the provided sources,\n            respond with: \"Cannot be determined from provided information.\"\n            \"\"\"\n        elif level == ProcessingLevel.CLIP:\n            if has_audio:\n                priority_list = \"\"\"\n                    1. **Frame-Level Answers** - Pre-computed per-frame answers (auxiliary reference only)\n                    2. **Audio Information** - Spoken content from audio transcription\n                    3. **Visual Information** - Direct visual content from video frames\"\"\"\n            else:\n                priority_list = \"\"\"1. **Frame-Level Answers** - Pre-computed per-frame answers (auxiliary reference only)\n                    2. **Visual Information** - Direct visual content from video frames\"\"\"\n            vqa_task = f\"\"\"For each question, use the BEST available source in this priority:\n                {priority_list}\n\n                **Critical Logic:**\n                - If frame-level answer is a REAL answer (not \"Cannot be determined from provided information.\") \u2192 use it\n                - If frame-level answer is \"Cannot be determined\" \u2192 SKIP IT and check audio/visual instead\n                - If answer is found in visual OR audio information \u2192 use that\n                - ONLY respond \"Cannot be determined\" if truly no information exists anywhere\n\n                \"\"\"\n\n        str_to_return = f\"\"\"## You have two tasks:\n\n        ### task 1: Concise Summary\n        Generate a brief summary that captures and summarizes main events and themes from the visual information (1-3 sentences).\n\n        ### task 2: Question Answering\n        {vqa_task}\n\n        Return ONLY this format:\n\n        Summary: &lt;your summary here&gt;\n\n        VQA Answers:\n        1. &lt;answer to question 1&gt;\n        2. &lt;answer to question 2&gt;\n        [etc.]\"\"\"\n\n        return str_to_return\n\n    @staticmethod\n    def vqa_only_task() -&gt; str:\n        \"\"\"VQA-only task for video-level processing.\"\"\"\n        str_to_return = \"\"\"## Task: Answer Questions\n\n        For each question, use the BEST available source in this priority:\n            1. **Segment-Level Answers** - Pre-computed per-frame answers (auxiliary reference only)\n            2. **Visual Information** - Direct visual content from video frames\n\n        **Critical Logic:**\n                - If segment-level answer is a REAL answer (not \"Cannot be determined from provided information.\") \u2192 use it\n                - If frame-level answer is \"Cannot be determined\" \u2192 SKIP IT and check visual information instead\n                - If answer is found in visual information \u2192 use that\n                - ONLY respond \"Cannot be determined\" if truly no information exists anywhere\n\n        Return ONLY this format:\n\n        VQA Answers:\n        1. &lt;answer to question 1&gt;\n        2. &lt;answer to question 2&gt;\n        [etc.]\"\"\"\n\n        return str_to_return\n\n    @staticmethod\n    def questions_module(questions: List[str]) -&gt; str:\n        \"\"\"Format questions list.\"\"\"\n        questions_text = \"\\n\".join(\n            [f\"{i + 1}. {q.strip()}\" for i, q in enumerate(questions)]\n        )\n        str_to_return = f\"\"\"## Questions to Answer\n\n        {questions_text}\n        \"\"\"\n        return str_to_return\n\n    @staticmethod\n    def vqa_context_module(vqa_bullets: List[str], is_final: bool = False) -&gt; str:\n        \"\"\"VQA context (frame-level or clip-level answers).\"\"\"\n        if is_final:\n            header = \"\"\"## SEGMENT-Level Answer Context (Reference Only)\n\n            The following are answers to above questions obtained for each segment of the video.\n            These answers are associated with the timestamp of each segment's beginning:\"\"\"\n        else:\n            header = \"\"\"## FRAME-Level Answer Context (Reference Only)\n\n            For each question, the following are frame-level answers provided as reference.\n            If these answers are \"Cannot be determined\", do not accept that as final\u2014instead, \n            use visual and audio information to answer the question:\"\"\"\n\n        bullets_text = \"\\n\".join(vqa_bullets)\n        return f\"{header}\\n\\n{bullets_text}\"\n\n    @classmethod\n    def build_frame_prompt(\n        cls, include_vqa: bool = False, questions: Optional[List[str]] = None\n    ) -&gt; str:\n        \"\"\"Build prompt for frame-level analysis.\"\"\"\n        modules = [cls.ROLE_MODULE, cls.visual_frames_module()]\n\n        if include_vqa and not questions:\n            raise ValueError(\"Questions must be provided when VQA should be included.\")\n\n        if include_vqa:\n            modules.append(cls.summary_vqa_task(ProcessingLevel.FRAME))\n            modules.append(cls.questions_module(questions))\n        else:\n            modules.append(cls.summary_task())\n\n        modules.append(cls.CONSTRAINTS_MODULE)\n        return \"\\n\\n\".join(modules)\n\n    @classmethod\n    def build_clip_prompt(\n        cls,\n        frame_bullets: List[str],\n        include_audio: bool = False,\n        audio_transcription: Optional[List[Dict]] = None,\n        include_vqa: bool = False,\n        questions: Optional[List[str]] = None,\n        vqa_bullets: Optional[List[str]] = None,\n    ) -&gt; str:\n        \"\"\"Build prompt for clip-level analysis.\"\"\"\n        modules = [cls.ROLE_MODULE, cls.visual_captions_module(frame_bullets)]\n\n        if include_audio and audio_transcription:\n            modules.append(cls.audio_module(audio_transcription))\n\n        if include_vqa and not questions:\n            raise ValueError(\"Questions must be provided when VQA should be included.\")\n\n        if include_vqa:\n            modules.append(\n                cls.summary_vqa_task(ProcessingLevel.CLIP, has_audio=include_audio)\n            )\n            modules.append(cls.questions_module(questions))\n            if vqa_bullets:\n                modules.append(cls.vqa_context_module(vqa_bullets, is_final=False))\n        else:\n            modules.append(cls.summary_task(has_audio=include_audio))\n\n        modules.append(cls.CONSTRAINTS_MODULE)\n        return \"\\n\\n\".join(modules)\n\n    @classmethod\n    def build_video_prompt(\n        cls,\n        include_vqa: bool = False,\n        clip_summaries: Optional[List[str]] = None,\n        questions: Optional[List[str]] = None,\n        vqa_bullets: Optional[List[str]] = None,\n    ) -&gt; str:\n        \"\"\"Build prompt for video-level analysis.\"\"\"\n        modules = [cls.ROLE_MODULE]\n\n        if not include_vqa:\n            modules.append(cls.visual_captions_final_module(clip_summaries))\n            modules.append(cls.summary_task())\n        else:\n            if not questions:\n                raise ValueError(\n                    \"Questions must be provided when VQA should be included.\"\n                )\n            if not vqa_bullets:\n                raise ValueError(\"Vqa_bullets must be provided for video-level VQA.\")\n\n            modules.append(cls.visual_captions_final_module(clip_summaries))\n            modules.append(cls.vqa_context_module(vqa_bullets, is_final=True))\n            modules.append(cls.vqa_only_task())\n            modules.append(cls.questions_module(questions))\n\n        modules.append(cls.CONSTRAINTS_MODULE)\n        return \"\\n\\n\".join(modules)\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.audio_module","title":"<code>audio_module(audio_transcription)</code>  <code>staticmethod</code>","text":"<p>Audio transcription with timestamps.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef audio_module(audio_transcription: List[Dict[str, Any]]) -&gt; str:\n    \"\"\"Audio transcription with timestamps.\"\"\"\n    audio_text = \"\\n\".join(\n        [\n            f\"[{a['start_time']:.2f}s - {a['end_time']:.2f}s]: {a['text'].strip()}\"\n            for a in audio_transcription\n        ]\n    )\n    str_to_return = f\"\"\"## Audio Information\n\n    The following is the audio transcription for the same video segment,\n    with precise timestamps for each spoken element:\n\n    {audio_text}\"\"\"\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.build_clip_prompt","title":"<code>build_clip_prompt(frame_bullets, include_audio=False, audio_transcription=None, include_vqa=False, questions=None, vqa_bullets=None)</code>  <code>classmethod</code>","text":"<p>Build prompt for clip-level analysis.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@classmethod\ndef build_clip_prompt(\n    cls,\n    frame_bullets: List[str],\n    include_audio: bool = False,\n    audio_transcription: Optional[List[Dict]] = None,\n    include_vqa: bool = False,\n    questions: Optional[List[str]] = None,\n    vqa_bullets: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"Build prompt for clip-level analysis.\"\"\"\n    modules = [cls.ROLE_MODULE, cls.visual_captions_module(frame_bullets)]\n\n    if include_audio and audio_transcription:\n        modules.append(cls.audio_module(audio_transcription))\n\n    if include_vqa and not questions:\n        raise ValueError(\"Questions must be provided when VQA should be included.\")\n\n    if include_vqa:\n        modules.append(\n            cls.summary_vqa_task(ProcessingLevel.CLIP, has_audio=include_audio)\n        )\n        modules.append(cls.questions_module(questions))\n        if vqa_bullets:\n            modules.append(cls.vqa_context_module(vqa_bullets, is_final=False))\n    else:\n        modules.append(cls.summary_task(has_audio=include_audio))\n\n    modules.append(cls.CONSTRAINTS_MODULE)\n    return \"\\n\\n\".join(modules)\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.build_frame_prompt","title":"<code>build_frame_prompt(include_vqa=False, questions=None)</code>  <code>classmethod</code>","text":"<p>Build prompt for frame-level analysis.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@classmethod\ndef build_frame_prompt(\n    cls, include_vqa: bool = False, questions: Optional[List[str]] = None\n) -&gt; str:\n    \"\"\"Build prompt for frame-level analysis.\"\"\"\n    modules = [cls.ROLE_MODULE, cls.visual_frames_module()]\n\n    if include_vqa and not questions:\n        raise ValueError(\"Questions must be provided when VQA should be included.\")\n\n    if include_vqa:\n        modules.append(cls.summary_vqa_task(ProcessingLevel.FRAME))\n        modules.append(cls.questions_module(questions))\n    else:\n        modules.append(cls.summary_task())\n\n    modules.append(cls.CONSTRAINTS_MODULE)\n    return \"\\n\\n\".join(modules)\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.build_video_prompt","title":"<code>build_video_prompt(include_vqa=False, clip_summaries=None, questions=None, vqa_bullets=None)</code>  <code>classmethod</code>","text":"<p>Build prompt for video-level analysis.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@classmethod\ndef build_video_prompt(\n    cls,\n    include_vqa: bool = False,\n    clip_summaries: Optional[List[str]] = None,\n    questions: Optional[List[str]] = None,\n    vqa_bullets: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"Build prompt for video-level analysis.\"\"\"\n    modules = [cls.ROLE_MODULE]\n\n    if not include_vqa:\n        modules.append(cls.visual_captions_final_module(clip_summaries))\n        modules.append(cls.summary_task())\n    else:\n        if not questions:\n            raise ValueError(\n                \"Questions must be provided when VQA should be included.\"\n            )\n        if not vqa_bullets:\n            raise ValueError(\"Vqa_bullets must be provided for video-level VQA.\")\n\n        modules.append(cls.visual_captions_final_module(clip_summaries))\n        modules.append(cls.vqa_context_module(vqa_bullets, is_final=True))\n        modules.append(cls.vqa_only_task())\n        modules.append(cls.questions_module(questions))\n\n    modules.append(cls.CONSTRAINTS_MODULE)\n    return \"\\n\\n\".join(modules)\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.questions_module","title":"<code>questions_module(questions)</code>  <code>staticmethod</code>","text":"<p>Format questions list.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef questions_module(questions: List[str]) -&gt; str:\n    \"\"\"Format questions list.\"\"\"\n    questions_text = \"\\n\".join(\n        [f\"{i + 1}. {q.strip()}\" for i, q in enumerate(questions)]\n    )\n    str_to_return = f\"\"\"## Questions to Answer\n\n    {questions_text}\n    \"\"\"\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.summary_task","title":"<code>summary_task(has_audio=False)</code>  <code>staticmethod</code>","text":"<p>Generate summary task (with or without audio).</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef summary_task(has_audio: bool = False) -&gt; str:\n    \"\"\"Generate summary task (with or without audio).\"\"\"\n    sources = \"visual and audio information\" if has_audio else \"visual information\"\n    str_to_return = f\"\"\"## Task: Generate Concise Summary\n\n    Based on the {sources} provided, generate a brief summary that:\n    - Captures and summarizes the main events and themes\n    - Uses clear, accessible language\n    - Is between 1-3 sentences\n    - Contains no unsupported claims\n\n    Return ONLY this format:\n\n    Summary: &lt;your summary here&gt;\"\"\"\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.summary_vqa_task","title":"<code>summary_vqa_task(level, has_audio=False)</code>  <code>staticmethod</code>","text":"<p>Generate summary+VQA task (adapts based on level and audio). For Frame and Clip levels.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef summary_vqa_task(\n    level: ProcessingLevel,\n    has_audio: bool = False,\n) -&gt; str:\n    \"\"\"Generate summary+VQA task (adapts based on level and audio). For Frame and Clip levels.\"\"\"\n\n    if level == ProcessingLevel.FRAME:\n        vqa_task = \"\"\"Answer the provided questions based ONLY on information from the visual information. Answers must be brief and direct.\n\n        **Critical Rule:** If you cannot answer a question from the provided sources,\n        respond with: \"Cannot be determined from provided information.\"\n        \"\"\"\n    elif level == ProcessingLevel.CLIP:\n        if has_audio:\n            priority_list = \"\"\"\n                1. **Frame-Level Answers** - Pre-computed per-frame answers (auxiliary reference only)\n                2. **Audio Information** - Spoken content from audio transcription\n                3. **Visual Information** - Direct visual content from video frames\"\"\"\n        else:\n            priority_list = \"\"\"1. **Frame-Level Answers** - Pre-computed per-frame answers (auxiliary reference only)\n                2. **Visual Information** - Direct visual content from video frames\"\"\"\n        vqa_task = f\"\"\"For each question, use the BEST available source in this priority:\n            {priority_list}\n\n            **Critical Logic:**\n            - If frame-level answer is a REAL answer (not \"Cannot be determined from provided information.\") \u2192 use it\n            - If frame-level answer is \"Cannot be determined\" \u2192 SKIP IT and check audio/visual instead\n            - If answer is found in visual OR audio information \u2192 use that\n            - ONLY respond \"Cannot be determined\" if truly no information exists anywhere\n\n            \"\"\"\n\n    str_to_return = f\"\"\"## You have two tasks:\n\n    ### task 1: Concise Summary\n    Generate a brief summary that captures and summarizes main events and themes from the visual information (1-3 sentences).\n\n    ### task 2: Question Answering\n    {vqa_task}\n\n    Return ONLY this format:\n\n    Summary: &lt;your summary here&gt;\n\n    VQA Answers:\n    1. &lt;answer to question 1&gt;\n    2. &lt;answer to question 2&gt;\n    [etc.]\"\"\"\n\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.visual_captions_final_module","title":"<code>visual_captions_final_module(clip_summaries)</code>  <code>staticmethod</code>","text":"<p>For video-level processing with clip summaries.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef visual_captions_final_module(clip_summaries: List[str]) -&gt; str:\n    \"\"\"For video-level processing with clip summaries.\"\"\"\n    str_to_return = f\"\"\"## Visual Information\n\n    The following are brief summaries obtained for each segment of the video.\n    These summaries are associated with the timestamp of each segment's beginning:\n\n    {clip_summaries}\"\"\"\n\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.visual_captions_module","title":"<code>visual_captions_module(frame_bullets)</code>  <code>staticmethod</code>","text":"<p>For clip-level processing with frame summaries.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef visual_captions_module(frame_bullets: List[str]) -&gt; str:\n    \"\"\"For clip-level processing with frame summaries.\"\"\"\n    bullets_text = \"\\n\".join(frame_bullets)\n    str_to_return = f\"\"\"## Visual Information\n\n    The following are summary bullets extracted from video frames with timestamps.\n    These bullets represent the visual content detected in each frame of the video segment:\n\n    {bullets_text}\"\"\"\n\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.visual_frames_module","title":"<code>visual_frames_module()</code>  <code>staticmethod</code>","text":"<p>For frame-level processing with actual images.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef visual_frames_module() -&gt; str:\n    \"\"\"For frame-level processing with actual images.\"\"\"\n    str_to_return = \"\"\"## Visual Information\n\n    Visual information is represented by keyframe extracted from the video segment at specified timestamp.\"\"\"\n    return str_to_return\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.vqa_context_module","title":"<code>vqa_context_module(vqa_bullets, is_final=False)</code>  <code>staticmethod</code>","text":"<p>VQA context (frame-level or clip-level answers).</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef vqa_context_module(vqa_bullets: List[str], is_final: bool = False) -&gt; str:\n    \"\"\"VQA context (frame-level or clip-level answers).\"\"\"\n    if is_final:\n        header = \"\"\"## SEGMENT-Level Answer Context (Reference Only)\n\n        The following are answers to above questions obtained for each segment of the video.\n        These answers are associated with the timestamp of each segment's beginning:\"\"\"\n    else:\n        header = \"\"\"## FRAME-Level Answer Context (Reference Only)\n\n        For each question, the following are frame-level answers provided as reference.\n        If these answers are \"Cannot be determined\", do not accept that as final\u2014instead, \n        use visual and audio information to answer the question:\"\"\"\n\n    bullets_text = \"\\n\".join(vqa_bullets)\n    return f\"{header}\\n\\n{bullets_text}\"\n</code></pre>"},{"location":"api/#ammico.prompt_builder.PromptBuilder.vqa_only_task","title":"<code>vqa_only_task()</code>  <code>staticmethod</code>","text":"<p>VQA-only task for video-level processing.</p> Source code in <code>ammico/prompt_builder.py</code> <pre><code>@staticmethod\ndef vqa_only_task() -&gt; str:\n    \"\"\"VQA-only task for video-level processing.\"\"\"\n    str_to_return = \"\"\"## Task: Answer Questions\n\n    For each question, use the BEST available source in this priority:\n        1. **Segment-Level Answers** - Pre-computed per-frame answers (auxiliary reference only)\n        2. **Visual Information** - Direct visual content from video frames\n\n    **Critical Logic:**\n            - If segment-level answer is a REAL answer (not \"Cannot be determined from provided information.\") \u2192 use it\n            - If frame-level answer is \"Cannot be determined\" \u2192 SKIP IT and check visual information instead\n            - If answer is found in visual information \u2192 use that\n            - ONLY respond \"Cannot be determined\" if truly no information exists anywhere\n\n    Return ONLY this format:\n\n    VQA Answers:\n    1. &lt;answer to question 1&gt;\n    2. &lt;answer to question 2&gt;\n    [etc.]\"\"\"\n\n    return str_to_return\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#compatibility-problems-solving","title":"Compatibility problems solving","text":"<p>Some ammico components require <code>tensorflow</code> (e.g. Emotion detector), some <code>pytorch</code> (e.g. Summary detector). Sometimes there are compatibility problems between these two frameworks. To avoid these problems on your machines, you can prepare proper environment before installing the package (you need conda on your machine):</p>"},{"location":"faq/#1-first-install-tensorflow-httpswwwtensorfloworginstallpip","title":"1. First, install tensorflow (https://www.tensorflow.org/install/pip)","text":"<ul> <li> <p>create a new environment with python and activate it</p> <p><code>conda create -n ammico_env python=3.13</code></p> <p><code>conda activate ammico_env</code> - install cudatoolkit from conda-forge</p> <p><code>conda install -c conda-forge cudatoolkit=11.8.0</code> - install nvidia-cudnn-cu11 from pip</p> <p><code>python -m pip install nvidia-cudnn-cu11==8.6.0.163</code> - add script that runs when conda environment <code>ammico_env</code> is activated to put the right libraries on your LD_LIBRARY_PATH</p> <p><pre><code>mkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\necho 'export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$CONDA_PREFIX/lib/:$LD_LIBRARY_PATH' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nsource $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n</code></pre> - deactivate and re-activate conda environment to call script above</p> <p><code>conda deactivate</code></p> <p><code>conda activate ammico_env</code></p> </li> <li> <p>install tensorflow</p> <p><code>python -m pip install tensorflow==2.15</code></p> </li> </ul>"},{"location":"faq/#2-second-install-pytorch","title":"2. Second, install pytorch","text":"<ul> <li> <p>install pytorch for same cuda version as above</p> <p><code>python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code></p> </li> </ul>"},{"location":"faq/#3-after-we-prepared-right-environment-we-can-install-the-ammico-package","title":"3. After we prepared right environment we can install the <code>ammico</code> package","text":"<ul> <li><code>python -m pip install ammico</code> </li> </ul> <p>It is done.</p>"},{"location":"faq/#micromamba","title":"Micromamba","text":"<p>If you are using micromamba you can prepare environment with just one command: </p> <p><code>micromamba create --no-channel-priority -c nvidia -c pytorch -c conda-forge -n ammico_env \"python=3.10\" pytorch torchvision torchaudio pytorch-cuda \"tensorflow-gpu&lt;=2.12.3\" \"numpy&lt;=1.23.4\"</code> </p>"},{"location":"faq/#windows","title":"Windows","text":"<p>To make pycocotools work on Windows OS you may need to install <code>vs_BuildTools.exe</code> from https://visualstudio.microsoft.com/visual-cpp-build-tools/ and choose following elements: - <code>Visual Studio extension development</code> - <code>MSVC v143 - VS 2022 C++ x64/x86 build tools</code> - <code>Windows 11 SDK</code> for Windows 11 (or <code>Windows 10 SDK</code> for Windows 10)</p> <p>Be careful, it requires around 7 GB of disk space.</p> <p></p>"},{"location":"faq/#what-happens-to-the-images-that-are-sent-to-google-cloud-vision","title":"What happens to the images that are sent to google Cloud Vision?","text":"<p>You have to accept the privacy statement of ammico to run this type of analyis.</p> <p>According to the google Vision API, the images that are uploaded and analysed are not stored and not shared with third parties:</p> <p>We won't make the content that you send available to the public. We won't share the content with any third party. The content is only used by Google as necessary to provide the Vision API service. Vision API complies with the Cloud Data Processing Addendum.</p> <p>For online (immediate response) operations (<code>BatchAnnotateImages</code> and <code>BatchAnnotateFiles</code>), the image data is processed in memory and not persisted to disk. For asynchronous offline batch operations (<code>AsyncBatchAnnotateImages</code> and <code>AsyncBatchAnnotateFiles</code>), we must store that image for a short period of time in order to perform the analysis and return the results to you. The stored image is typically deleted right after the processing is done, with a failsafe Time to live (TTL) of a few hours. Google also temporarily logs some metadata about your Vision API requests (such as the time the request was received and the size of the request) to improve our service and combat abuse.</p>"},{"location":"faq/#what-happens-to-the-text-that-is-sent-to-google-translate","title":"What happens to the text that is sent to google Translate?","text":"<p>You have to accept the privacy statement of ammico to run this type of analyis.</p> <p>According to google Translate, the data is not stored after processing and not made available to third parties:</p> <p>We will not make the content of the text that you send available to the public. We will not share the content with any third party. The content of the text is only used by Google as necessary to provide the Cloud Translation API service. Cloud Translation API complies with the Cloud Data Processing Addendum.</p> <p>When you send text to Cloud Translation API, text is held briefly in-memory in order to perform the translation and return the results to you.</p>"},{"location":"faq/#what-happens-if-i-dont-have-internet-access-can-i-still-use-ammico","title":"What happens if I don't have internet access - can I still use ammico?","text":"<p>Some features of ammico require internet access; a general answer to this question is not possible, some services require an internet connection, others can be used offline:</p> <ul> <li>Text extraction: To extract text from images, and translate the text, the data needs to be processed by google Cloud Vision and google Translate, which run in the cloud. Without internet access, text extraction and translation is not possible.</li> <li>Image summary and query: After initial loading and caching of the model, image summarization and VQA can work fully offline.</li> <li>Video summary and query: After initial loading and caching of the model, video summarization and VQA can work fully offline.</li> <li>Video summary and query with audio: After the WhisperX model (and optional language assets) is downloaded, audio transcription and combined video+audio summarization also work offline.</li> <li>Multimodal search:  After initial loading and caching of the model, multimodal search can work fully offline.</li> <li>Color analysis: The <code>color</code> module does not require an internet connection.</li> </ul>"},{"location":"set_up_credentials/","title":"Instructions how to generate and enable a google Cloud Vision API key","text":"<ol> <li>Go to google-cloud-vision and click on \"Console\". Sign into your google account / create a new google account if prompted. This will bring you to the following page, where you click on \"project\" in the top of the screen. </li> <li>Select \"project\" from the top left drop-down menu. </li> <li>Click on \"NEW PROJECT\" on the left of the pop-up window. </li> <li>Enter a project name and click on \"CREATE\". </li> <li>Now you should be back on the dashboard. In the top right, click on the three vertical dots. </li> <li>In the drop-down menu, select \"Project settings\". </li> <li>In the menu on the left, click on \"Service Accounts\". </li> <li>Click on \"+ CREATE SERVICE ACCOUNT\". </li> <li>Select a service account ID (you can pick this as any name you wish). Click on \"DONE\". </li> <li>Now your service account should show up in the list of service accounts. </li> <li>Click on the three vertical dots to the right of your service account name and select \"Manage keys\". </li> <li>Click on \"Create new key\". </li> <li>In the pop-up window, select \"JSON\" and click \"CREATE\". </li> <li>The private key is directly downloaded to your computer. It should be in your downloads folder. </li> <li>The JSON key file will look something like this (any private information has been blanked out in the screenshot). </li> <li>Now go back to your browser window. Click on \"Google Cloud\" in the top left corner. </li> <li>Now select \"APIs &amp; Services\". </li> <li>From the selection of APIs, select \"Cloud Vision API\" or search for it and then select. </li> <li>Click on \"ENABLE\". </li> <li>Google Cloud Vision API is now enabled for your key. </li> <li>Place the JSON key in a selected folder on your computer and reference this key in your Jupyter Notebook / Python console when running ammico. Or, upload it to your google Drive to use it on google Colaboratory.</li> <li>Make sure that billing is enabled for your google account. You can get the first three month for free; after that, you will be charged if processing more than 1000 images / month (currently $1.50 per 1000 images, see here).</li> </ol>"},{"location":"modules/colors/","title":"Color detector","text":"<p>The Colors module analyzes color composition in images, extracting dominant colors and categorizing them into named color categories.</p> <p>Color detection is carried out using colorgram.py and colour for the distance metric. The colors can be classified into the main named colors/hues in the English language, that are red, green, blue, yellow, cyan, orange, purple, pink, brown, grey, white, black.</p>"},{"location":"modules/colors/#key-features","title":"Key Features","text":"<ul> <li>Color Extraction: Extracts the N most common colors from images using the <code>colorgram</code> library (default: 100 colors)</li> <li>Color Naming: Matches extracted RGB colors to the closest CSS3 color names using Delta-E color difference metrics</li> <li>Color Categorization: Reduces colors to 12 basic categories: red, green, blue, yellow, cyan, orange, purple, pink, brown, grey, white, black</li> <li>Multiple Delta-E Methods: Supports 12 different color difference calculation methods:</li> <li>CIE 1976 (default)</li> <li>CIE 1994</li> <li>CIE 2000</li> <li>CMC</li> <li>ITP</li> <li>CAM02-LCD, CAM02-SCD, CAM02-UCS</li> <li>CAM16-LCD, CAM16-SCD, CAM16-UCS</li> <li>DIN99</li> </ul>"},{"location":"modules/colors/#usage","title":"Usage","text":"<pre><code>from ammico.colors import ColorDetector\n\ndetector = ColorDetector(subdict={\"filename\": \"image.jpg\"}, delta_e_method=\"CIE 1976\")\nresults = detector.analyse_image()\n# Returns dict with color percentages: {\"red\": 0.15, \"blue\": 0.23, ...}\n</code></pre>"},{"location":"modules/colors/#output","title":"Output","text":"<p>Returns a dictionary with color names as keys and their percentage presence in the image as values (rounded to 2 decimal places).</p>"},{"location":"modules/colors/#workflow","title":"Workflow","text":"<pre><code>flowchart TD\n    Start([Start]) --&gt; Init[Initialize ColorDetector]\n    Init --&gt; CheckDelta{Valid Delta-E Method?}\n\n    CheckDelta -- No --&gt; Error[Raise ValueError]\n    CheckDelta -- Yes --&gt; Analyse[analyse_image]\n\n    Analyse --&gt; Extract[colorgram.extract]\n    Extract --&gt; TopColors[Get N most common colors]\n\n    TopColors --&gt; LoopStart{For each color}\n    LoopStart --&gt; Convert[rgb2name]\n\n    Convert --&gt; Hex[Convert to Hex]\n    Hex --&gt; ExactMatch{Exact match CSS3?}\n\n    ExactMatch -- Yes --&gt; MatchedName[Get Name]\n\n    ExactMatch -- No --&gt; CalcDelta[Calculate Delta-E]\n    CalcDelta --&gt; MinDelta[Find Minimum Delta-E]\n    MinDelta --&gt; MatchedName\n\n    MatchedName --&gt; Merge{Merge Colors?}\n\n    Merge -- No --&gt; SaveProp[Save Proportion]\n    Merge -- Yes --&gt; Reduce[Map to 12 Basic Colors]\n    Reduce --&gt; SaveProp\n\n    SaveProp --&gt; LoopEnd{More Colors?}\n    LoopEnd -- Yes --&gt; LoopStart\n    LoopEnd -- No --&gt; Round[Round Proportions]\n\n    Round --&gt; Return[Return Dictionary]\n    Return --&gt; End([End])</code></pre>"},{"location":"modules/display/","title":"Display interface for manual inspection","text":"<p>The Display module provides an interactive web-based dashboard for visualizing and analyzing image data using Dash and Bootstrap components.</p>"},{"location":"modules/display/#key-features","title":"Key Features","text":"<ul> <li>Interactive File Explorer: Dropdown interface to select images from a dictionary</li> <li>Image Display: Real-time image preview in the dashboard</li> <li>Multi-Detector Support: Run different analysis detectors:</li> <li>TextDetector (text extraction and translation)</li> <li>ColorDetector (color analysis)</li> <li>VQA (Visual Question Answering with image summaries)</li> <li>Dynamic Settings: Context-aware settings panels that appear based on selected detector</li> <li>Real-time Analysis: Execute analysis on selected images with loading indicators</li> <li>Results Visualization: JSON-style table display of analysis results</li> <li>Customizable Questions: Text area for entering custom VQA questions (one per line)</li> <li>Privacy Controls: Environment variable configuration for privacy disclosure acceptance</li> </ul>"},{"location":"modules/display/#usage","title":"Usage","text":"<pre><code>from ammico.display import AnalysisExplorer\n\nexplorer = AnalysisExplorer(mydict=image_dict)\nexplorer.run_server(port=8050)\n</code></pre>"},{"location":"modules/display/#features","title":"Features","text":"<ul> <li>TextDetector Settings: Privacy disclosure environment variable configuration</li> <li>ColorDetector Settings: Delta-E method selection dropdown</li> <li>VQA Settings: </li> <li>Analysis type selection (summary, questions, or both)</li> <li>Custom questions textarea (shown/hidden based on analysis type)</li> <li>Model selection (base/large)</li> </ul>"},{"location":"modules/display/#output","title":"Output","text":"<p>Displays analysis results in an interactive table format, showing all extracted features and metadata for the selected image.</p>"},{"location":"modules/display/#workflow","title":"Workflow","text":"<pre><code>flowchart TD\n    Start([Initialize AnalysisExplorer]) --&gt; RunServer[Explorer.run_server]\n    RunServer --&gt; Dashboard[Dash Dashboard Loaded]\n\n    subgraph Dashboard Interaction\n        Dashboard --&gt; SelectImg{Select Image}\n        SelectImg --&gt; UpdatePic[Update Picture View]\n\n        Dashboard --&gt; SelectDet{Select Detector}\n        SelectDet --&gt; UpdateSet[Update Settings Panel]\n\n        UpdateSet --&gt; Settings{Adjust Settings}\n\n        Settings --&gt; ClickRun{Click Run Detector}\n        ClickRun --&gt; CheckDet[Check Selected Detector]\n    end\n\n    CheckDet -- TextDetector --&gt; TextDet[Init TextDetector]\n    CheckDet -- ColorDetector --&gt; ColorDet[Init ColorDetector]\n    CheckDet -- VQA --&gt; VQADet[Init ImageSummaryDetector]\n\n    TextDet --&gt; RunMeasure[Run analyse_image]\n    ColorDet --&gt; RunMeasure\n    VQADet --&gt; RunMeasure\n\n    RunMeasure --&gt; Format[Format Output as Table]\n    Format --&gt; UpdateTable[Update JSON Viewer]\n\n    UpdateTable --&gt; Dashboard</code></pre>"},{"location":"modules/image_summary/","title":"Image detector: Summary and VQA","text":"<p>The <code>image_summary</code> module provides advanced image analysis capabilities using the Qwen2.5-VL multimodal model. Qwen2.5-VL is a multimodal large language model capable of understanding and generating content from both images and videos. With its help, <code>ammico</code> supports tasks such as image/video summarization and image/video visual question answering, where the model answers users' questions about the context of a media file. It combines functionality from the <code>model.py</code> and <code>prompt_builder.py</code> modules to offer comprehensive image understanding.</p>"},{"location":"modules/image_summary/#core-components","title":"Core Components","text":""},{"location":"modules/image_summary/#multimodalsummarymodel-modelpy","title":"MultimodalSummaryModel (<code>model.py</code>)","text":"<p>The underlying model wrapper that handles Qwen2.5-VL model loading and inference:</p> <ul> <li>Model Selection: </li> <li>CUDA: <code>Qwen/Qwen2.5-VL-7B-Instruct</code> (default for GPU)</li> <li>CPU: <code>Qwen/Qwen2.5-VL-3B-Instruct</code> (default for CPU)</li> <li>Automatic Device Detection: Auto-detects CUDA availability and falls back to CPU</li> <li>Quantization: Automatic 4-bit quantization for CUDA devices using BitsAndBytesConfig</li> <li>Memory Management: Resource cleanup methods for long-running processes</li> <li>Model Components: Provides processor, tokenizer, and model objects</li> </ul>"},{"location":"modules/image_summary/#promptbuilder-prompt_builderpy","title":"PromptBuilder (<code>prompt_builder.py</code>)","text":"<p>Modular prompt construction system for multi-level analysis:</p> <ul> <li>Processing Levels: Frame, Clip, and Video level prompts</li> <li>Task Types: Summary generation, VQA, or combined tasks</li> <li>Audio Integration: Prompts that incorporate audio transcription when available</li> <li>Structured Output: Ensures consistent, well-formatted model outputs</li> </ul>"},{"location":"modules/image_summary/#key-features","title":"Key Features","text":"<ul> <li>Image Captioning: Generate concise or detailed captions for images</li> <li>Visual Question Answering (VQA): Answer custom questions about image content</li> <li>Batch Processing: Process multiple images efficiently with configurable batch sizes</li> <li>Flexible Input: Supports file paths, PIL Images, or sequences of images</li> <li>Analysis Types:</li> <li><code>summary</code>: Generate image captions only</li> <li><code>questions</code>: Answer questions only</li> <li><code>summary_and_questions</code>: Both caption and Q&amp;A (default)</li> <li>Concise Mode: Option to generate shorter, more focused summaries and answers</li> <li>Question Chunking: Automatically processes questions in batches (default: 8 per batch)</li> <li>Error Handling: Robust error handling with retry logic for CUDA operations</li> </ul>"},{"location":"modules/image_summary/#usage","title":"Usage","text":"<pre><code>from ammico.image_summary import ImageSummaryDetector\nfrom ammico.model import MultimodalSummaryModel\n\n# Initialize model\nmodel = MultimodalSummaryModel(device=\"cpu\")\n\n# Create detector\ndetector = ImageSummaryDetector(summary_model=model, subdict={})\n\n# Analyze single image\nresults = detector.analyse_image(\n    entry={\"filename\": \"image.jpg\"},\n    analysis_type=\"summary_and_questions\",\n    list_of_questions=[\"What is in this image?\", \"Are there people?\"],\n    is_concise_summary=True,\n    is_concise_answer=True\n)\n\n# Batch processing\ndetector.subdict = image_dict\nresults = detector.analyse_images_from_dict(\n    analysis_type=\"summary\",\n    keys_batch_size=16\n)\n</code></pre>"},{"location":"modules/image_summary/#configuration","title":"Configuration","text":"<ul> <li>Max Questions: Default 32 questions per image (configurable)</li> <li>Batch Size: Default 16 images per batch (configurable)</li> <li>Token Limits: </li> <li>Concise summary: 64 tokens</li> <li>Detailed summary: 256 tokens</li> <li>Concise answers: 64 tokens</li> <li>Detailed answers: 128 tokens</li> </ul>"},{"location":"modules/image_summary/#output","title":"Output","text":"<ul> <li><code>vqa</code>: List of answers corresponding to questions (if questions requested)</li> </ul>"},{"location":"modules/image_summary/#workflow","title":"Workflow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Initialize_Detector\n    Initialize_Detector --&gt; Check_Analysis_Type\n\n    state Check_Analysis_Type {\n        [*] --&gt; Decide\n        Decide --&gt; Summary : Type=Summary\n        Decide --&gt; Questions : Type=Questions\n        Decide --&gt; Both : Type=Summary+Questions\n    }\n\n    Summary --&gt; Generate_Caption\n    Questions --&gt; Answer_Questions\n    Both --&gt; Generate_Caption\n    Both --&gt; Answer_Questions\n\n    Generate_Caption --&gt; Prepare_Inputs\n    Answer_Questions --&gt; Clean_Questions\n    Clean_Questions --&gt; Prepare_Inputs\n\n    state MultimodalSummaryModel {\n        [*] --&gt; Initialize_Model\n        Initialize_Model --&gt; Load_Qwen2.5_VL\n        Load_Qwen2.5_VL --&gt; Quantize_4bit\n        Quantize_4bit --&gt; Processor\n        Processor --&gt; Model_Inference\n        Model_Inference --&gt; Tokenizer_Decode\n    }\n\n    Prepare_Inputs --&gt; Processor\n    Tokenizer_Decode --&gt; Return_Result\n    Return_Result --&gt; [*]\n\n    note right of Processor : Uses PromptBuilder</code></pre>"},{"location":"modules/text/","title":"Text detector","text":"<p>The text is extracted from the images using google-cloud-vision. For this, you need an API key. Set up your google account following the instructions on the google Vision AI website or as described here. You then need to export the location of the API key as an environment variable: <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"location of your .json\"\n</code></pre> The extracted text is then stored under the <code>text</code> key (column when exporting a csv).</p> <p>Googletrans is used to recognize the language automatically and translate into English. The text language and translated text is then stored under the <code>text_language</code> and <code>text_english</code> key (column when exporting a csv).</p> <p>The English text is cleaned from numbers and unrecognized words (<code>text_clean</code>).</p>"},{"location":"modules/text/#key-features","title":"Key Features","text":"<ul> <li>Text Extraction: Detects and extracts text from images using Google Cloud Vision API</li> <li>Language Detection: Automatically detects the language of extracted text</li> <li>Translation: Translates text to English using Google Translate</li> <li>Text Preprocessing: </li> <li>Handles formatting issues (adds spaces after periods for translation compatibility)</li> <li>Truncates very long texts (default: 5000 characters)</li> <li>Removes line breaks</li> <li>NLP Analysis: Integrates with spaCy for advanced text analysis</li> <li>CSV Support: <code>TextAnalyzer</code> class for reading text from CSV files</li> <li>Privacy Controls: Requires explicit privacy disclosure acceptance before processing</li> </ul>"},{"location":"modules/text/#usage","title":"Usage","text":"<p><pre><code>detector = ammico.TextDetector(\n    subdict={\"filename\": \"image.jpg\"},\n    skip_extraction=False,  # Set True if text already provided\n    accept_privacy=\"PRIVACY_AMMICO\"\n)\nresults = detector.analyse_image()\n</code></pre> It is also possible to read in a csv file for translation and processing: <pre><code>detector = ammico.TextDetector(\n    csv_path=\"./text_input.csv\",\n    column_key=\"Text column\",\n    csv_encoding=\"utf-8\",\n    skip_extraction=True,  \n    accept_privacy=\"PRIVACY_AMMICO\"\n)\nresults = detector.analyse_image()\n</code></pre> In this case, the extraction of the text from the image is skipped and the text is translated directly (no Google Cloud Vision API key required).</p>"},{"location":"modules/text/#output","title":"Output","text":"<p>Returns dictionary with:</p> <ul> <li><code>text</code>: Original extracted text</li> <li><code>text_language</code>: Detected source language code</li> <li><code>text_english</code>: Translated English text</li> <li><code>text_truncated</code>: Truncated version if original exceeds length limit</li> </ul>"},{"location":"modules/text/#requirements","title":"Requirements","text":"<ul> <li>Google Cloud Vision API credentials</li> <li>Privacy disclosure acceptance (via environment variable or interactive prompt)</li> <li>spaCy English model (<code>en_core_web_md</code>)</li> </ul>"},{"location":"modules/text/#workflow","title":"Workflow","text":"<pre><code>flowchart TD\n    Start([Start]) --&gt; Init[Initialize TextDetector]\n    Init --&gt; CheckPrivacy{Privacy Accepted?}\n\n    CheckPrivacy -- No --&gt; Error[Raise ValueError]\n    CheckPrivacy -- Yes --&gt; CheckSkip{Skip Extraction?}\n\n    CheckSkip -- No --&gt; GetText[get_text_from_image]\n    GetText --&gt; CloudVision[Call Google Cloud Vision API]\n    CloudVision --&gt; CheckResp{Response?}\n    CheckResp -- Error --&gt; LogError[Log Error]\n    CheckResp -- Success --&gt; Extract[Extract Text]\n\n    CheckSkip -- Yes --&gt; ReadDict[Read text from subdict]\n\n    Extract --&gt; HasText{Text Found?}\n    ReadDict --&gt; HasText\n\n    HasText -- No --&gt; End([End])\n    HasText -- Yes --&gt; Preprocess[Preprocess Text]\n\n    Preprocess --&gt; AddSpc[_check_add_space_after_full_stop]\n    AddSpc --&gt; Trunc[_truncate_text]\n    Trunc --&gt; Trans[translate_text]\n    Trans --&gt; GTrans[Call Google Translate]\n    GTrans --&gt; Clean[remove_linebreaks]\n\n    Clean --&gt; RunSpacy[_run_spacy]\n    RunSpacy --&gt; RetDict[Return Updated Dictionary]\n    RetDict --&gt; End</code></pre>"},{"location":"modules/video_summary/","title":"Video detector: Summary and VQA","text":"<p>The Video Summary module provides comprehensive video analysis combining visual and audio information using multimodal AI models.</p>"},{"location":"modules/video_summary/#key-features","title":"Key Features","text":"<ul> <li>Multi-Modal Analysis: Combines visual frames and audio transcription</li> <li>Scene Detection: Automatically detects scene cuts using frame differencing</li> <li>Frame Extraction: Extracts keyframes at optimal timestamps using ffmpeg</li> <li>Audio Transcription: Transcribes audio using WhisperX models with word-level alignment</li> <li>Segment Merging: Intelligently merges audio phrases and video scenes into coherent segments</li> <li>Hierarchical Processing: Three-level analysis pipeline:</li> <li>Frame Level: Individual frame analysis with captions and VQA</li> <li>Clip Level: Segment-level summaries combining frames and audio</li> <li>Video Level: Final video summary and Q&amp;A</li> <li>Parallel Processing: Multi-threaded frame extraction for performance</li> <li>Adaptive Frame Sampling: Adjusts frame rate based on segment duration</li> <li>Memory Management: Efficient resource cleanup for audio models</li> </ul>"},{"location":"modules/video_summary/#audio-features","title":"Audio Features","text":"<ul> <li>Automatic Audio Detection: Checks for audio streams in video files</li> <li>Audio Extraction: Extracts audio using ffmpeg (16kHz mono WAV)</li> <li>Transcription: Uses WhisperX for accurate transcription with timestamps</li> <li>Word Alignment: Provides precise timing for each transcribed segment</li> <li>Language Support: Supports all WhisperX languages with optional language specification</li> </ul>"},{"location":"modules/video_summary/#visual-features","title":"Visual Features","text":"<ul> <li>Scene Cut Detection: Uses adaptive thresholding based on frame differences</li> <li>Smart Frame Selection: </li> <li>2 frames/second for short segments (&lt;2s)</li> <li>4 frames/second for medium segments (2-20s)</li> <li>6 frames/second for long segments (&gt;20s)</li> <li>Aspect Ratio Preservation: Maintains video aspect ratio during frame extraction</li> <li>Multiple Extraction Strategies: Fallback strategies (MJPEG, PNG) for robust frame extraction</li> <li>Segment Splitting: Automatically splits segments longer than 25 seconds</li> </ul>"},{"location":"modules/video_summary/#usage","title":"Usage","text":"<pre><code>from ammico.video_summary import VideoSummaryDetector\nfrom ammico.model import MultimodalSummaryModel, AudioToTextModel\n\n# Initialize models\nvideo_model = MultimodalSummaryModel(device=\"cuda\")\naudio_model = AudioToTextModel(model_size=\"large\", device=\"cuda\")\n\n# Create detector\ndetector = VideoSummaryDetector(\n    summary_model=video_model,\n    audio_model=audio_model,\n    subdict={\"video1\": {\"filename\": \"video.mp4\"}}\n)\n\n# Analyze video\nresults = detector.analyse_videos_from_dict(\n    analysis_type=\"summary_and_questions\",\n    list_of_questions=[\"What is happening?\", \"Who is speaking?\"]\n)\n</code></pre>"},{"location":"modules/video_summary/#analysis-pipeline","title":"Analysis Pipeline","text":"<ol> <li>Audio Processing (if available):</li> <li>Extract audio track</li> <li>Transcribe with WhisperX</li> <li> <p>Generate timestamped audio segments</p> </li> <li> <p>Visual Processing:</p> </li> <li>Detect scene cuts</li> <li>Extract frame timestamps for each scene</li> <li> <p>Extract frames using parallel ffmpeg processes</p> </li> <li> <p>Segment Merging:</p> </li> <li>Merge audio and visual boundaries</li> <li>Create coherent temporal segments</li> <li> <p>Reassign frame timestamps to merged segments</p> </li> <li> <p>Frame-Level Analysis:</p> </li> <li>Generate captions for each extracted frame</li> <li>Answer questions about individual frames</li> <li> <p>Create summary bullets and VQA bullets</p> </li> <li> <p>Clip-Level Analysis:</p> </li> <li>Combine frame summaries with audio transcription</li> <li>Generate segment-level summaries</li> <li> <p>Answer questions using combined audio/visual context</p> </li> <li> <p>Video-Level Analysis:</p> </li> <li>Aggregate all segment summaries</li> <li>Generate final video summary</li> <li>Answer questions about the entire video</li> </ol>"},{"location":"modules/video_summary/#configuration","title":"Configuration","text":"<ul> <li>Segment Threshold: Default 8 seconds for creating new segments</li> <li>Max Segment Duration: Segments longer than 25 seconds are split</li> <li>Frame Extraction Workers: Default uses half of available CPU cores (max 8)</li> <li>Output Dimensions: Max 720px while preserving aspect ratio</li> </ul>"},{"location":"modules/video_summary/#output","title":"Output","text":"<p>Returns dictionaries with:</p> <ul> <li><code>summary</code>: Final video summary (if summary requested)</li> <li><code>vqa_answers</code>: List of answers to questions (if questions requested)</li> <li><code>audio_descriptions</code>: Timestamped audio transcription segments</li> <li>Per-segment data with <code>summary_bullets</code> and <code>vqa_bullets</code></li> </ul>"},{"location":"modules/video_summary/#requirements","title":"Requirements","text":"<ul> <li>ffmpeg and ffprobe for video processing</li> <li>CUDA support recommended for performance</li> <li>WhisperX models for audio transcription</li> </ul>"},{"location":"modules/video_summary/#workflow","title":"Workflow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Initialize_Detector\n    Initialize_Detector --&gt; Check_Audio_Model\n\n    state Check_Audio_Model {\n        [*] --&gt; Decisions\n        Decisions --&gt; Extract_Audio : Has Audio Model\n        Decisions --&gt; Visual_Processing_Only : No Audio Model\n    }\n\n    Extract_Audio --&gt; Transcribe_WhisperX\n    Transcribe_WhisperX --&gt; Audio_Segments\n\n    state Visual_Processing {\n        [*] --&gt; Scene_Cut_Detection\n        Scene_Cut_Detection --&gt; Calculate_Timestamps\n        Calculate_Timestamps --&gt; Extract_Frames_ffmpeg\n    }\n\n    Visual_Processing_Only --&gt; Visual_Processing\n    Audio_Segments --&gt; Visual_Processing\n    Extract_Frames_ffmpeg --&gt; Merge_Audio_Visual_Segments\n\n    state Analysis_Levels {\n        [*] --&gt; Frame_Level\n        Frame_Level --&gt; Clip_Level\n        Clip_Level --&gt; Video_Level\n    }\n\n    Merge_Audio_Visual_Segments --&gt; Analysis_Levels\n\n    state MultimodalSummaryModel {\n        [*] --&gt; Model_Inference\n    }\n\n    state PromptBuilder {\n        [*] --&gt; Build_Prompts\n        Build_Prompts --&gt; Frame_Prompt\n        Build_Prompts --&gt; Clip_Prompt\n        Build_Prompts --&gt; Video_Prompt\n    }\n\n    Frame_Level --&gt; Frame_Prompt\n    Frame_Prompt --&gt; Model_Inference\n\n    Clip_Level --&gt; Clip_Prompt\n    Clip_Prompt --&gt; Model_Inference\n\n    Video_Level --&gt; Video_Prompt\n    Video_Prompt --&gt; Model_Inference\n\n    Model_Inference --&gt; Final_Result\n    Final_Result --&gt; [*]</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/","title":"Getting started with <code>ammico</code>","text":"<p>Version date: 28.01.2026</p> <p>With <code>ammico</code>, you can analyze text on images and visual content (image and video) at the same time. This tutorial notebook shows you how, after importing ammico (Step 1) and uploading your data (Step 2), you can use all three modules: text extraction (Step 3), image analysis (Step 4) and video analysis (Step 5). The modules are independent, so you can just perform, for example, image analysis or video analysis without needing to go through the others. </p> <p>You can run this notebook on google colab or locally / on your own HPC resource. For production data processing, it is recommended to run the analysis locally on a GPU-supported machine. You can also make use of the colab GPU runtime, or purchase additional runtime. However, google colab comes with pre-installed libraries that can lead to dependency conflicts.</p> <p>This first cell only runs on google colab; on all other machines, you need to create a conda environment first and install ammico from the Python Package Index using <code>pip install ammico</code></p> <p>Alternatively you can install the development version from the GitHub repository <code>pip install git+https://github.com/ssciwr/AMMICO.git</code></p> <p>On google colab, select \"TPU\" as runtime, otherwise the notebook may not run. To do so, go to: Runtime -&gt; Change runtime type -&gt; Select Hardware accelerator T4 GPU -&gt; Press Save</p> <p>Then you need to uninstall the already installed <code>transformers</code> version, and <code>peft</code>, since these lead to dependency conflicts. Then you can install <code>ammico</code>.</p> <p>Simply execute the cell below by pressing shift+enter.</p> <pre><code># CELL #1\n# when running on Google colab, otherwise the below cell is skipped\nif \"google.colab\" in str(get_ipython()):\n    # uv is a fast Python package manager, see https://github.com/astral-sh/uv\n    %pip install uv\n    # Uninstall conflicting packages\n    !uv pip uninstall peft transformers\n    # Install ammico as the latest version from GitHub, which will pull in the compatible dependencies\n    !uv pip install git+https://github.com/ssciwr/ammico.git\n</code></pre> <p>Now you need to restart the kernel to load the new dependencies. For this, click on \"Runtime -&gt; Restart Session\" or press Ctrl+M.</p>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#step-1-import-ammico","title":"Step 1: Import AMMICO","text":"<pre><code># CELL #2\nimport ammico\n</code></pre> <p>This imports all the functionality from <code>ammico</code>. To analyze images, you need to upload images to google colab or connect to your Google Drive. To connect to your google drive you need to run the cell below.</p> <pre><code># CELL #3\nfrom google.colab import drive\ndrive.mount('/content/drive')\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#step-2-read-your-image-data-into-ammico","title":"Step 2: Read your image data into AMMICO","text":"<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <p>For reading in the files, the ammico function <code>find_files</code> is used, with optional keywords:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the image files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\", \"tiff\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>int</code> the random seed for shuffling the images; applies when only a few images are read and the selection should be preserved (defaults to <code>None</code>) <pre><code># CELL #4\n# Define your data path\ndata_path = \"/content/drive/MyDrive/Test\"  # the current directory (make sure you specify the correct path of your folder!)\n\n# Find files and create the image dictionary\nimage_dict = ammico.find_files(\n    path=data_path,\n    limit=20,  # Limit the number of files to process (optional)\n)\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#step-3-extract-the-text-from-images-in-the-dataset","title":"Step 3: Extract the text from images in the dataset","text":"<p>In order to be able to extract the text, you will need a google cloud vision API key. To get such a key follow the instructions here: https://ssciwr.github.io/AMMICO/set_up_credentials/.</p> <pre><code># CELL #5\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Test/[KEYNAME].json\" # make sure you specify the correct file path of your cloud vision API key\n</code></pre> <p>To extract the text from images, you will need to run the cell below, and answer \"yes\" when prompted to accept the privacy disclosure.</p> <pre><code># CELL #6\nfor key in image_dict.keys():\n    image_dict[key] = ammico.TextDetector(\n        image_dict[key],\n    ).analyse_image()\n</code></pre> <pre><code>The Text Detector uses Google Cloud Vision\n    and Google Translate. Detailed information about how information\n    is being processed is provided here:\n    https://ssciwr.github.io/AMMICO/build/html/faq_link.html.\n    Google\u2019s privacy policy can be read here: https://policies.google.com/privacy.\n    By continuing to use this Detector, you agree to send the data you want analyzed\n    to the Google servers for extraction and translation.\nDo you accept the privacy disclosure? (yes/no): yes\nYou have accepted the privacy disclosure.\nText detection and translation will be performed.\n</code></pre> <pre><code># CELL #7\nimage_df = ammico.get_dataframe(image_dict)\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#31-inspect-and-save-the-extracted-text-data","title":"3.1 Inspect and save the extracted text data","text":"<p>To inspect the data run the cell below</p> <pre><code># CELL #8\nimage_df.head(20)\n</code></pre> <p>To save the data, run the cell below.</p> <pre><code># CELL #9\nimage_df.to_csv(\"/content/drive/MyDrive/Test/text_data_out.csv\") # make sure you specify the correct file path of your output file\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#step-4-perform-image-content-analysis","title":"Step 4. Perform image content analysis","text":""},{"location":"tutorials/ammico_demo_multimodal_analysis/#41-obtain-the-image-summary","title":"4.1. Obtain the image summary","text":"<p>We begin by creating an image caption (\"Summary\") using the QWEN 2.5 Vision-Language model family. Two variants are supported:</p> <p>This module is built on the Qwen2.5-VL model family. In this project, two model variants are supported:</p> <ol> <li><code>Qwen2.5-VL-3B-Instruct</code>, which requires approximately 3 GB of video memory to load.</li> <li><code>Qwen2.5-VL-7B-Instruct</code>, which requires 8.5 GB of VRAM for initialization (default).</li> </ol> <p>First, the model needs to be specified and loaded into memory. This will take several minutes.</p> <pre><code># CELL #10\nmodel = ammico.MultimodalSummaryModel()  # load the default model\n</code></pre> <p>Then, we create an instance of the Python class that handles the image summary and visual question answering tasks:</p> <pre><code># CELL #11\nimage_summary_vqa = ammico.ImageSummaryDetector(summary_model=model, subdict=image_dict)\n</code></pre> <p>After this, we can create the image captions. Depending on the number of images and the hardware provided, this can take several minutes.</p> <pre><code># CELL #12\nsummary = image_summary_vqa.analyse_images_from_dict(\n    analysis_type=\"summary\", is_concise_summary=True\n)\n</code></pre> <p>The results are provided in the updated dictionary. For your convenience, you can execute the cell below to see the image that was analyzed together with the generated caption (summary). This works best for a limited number of images; for larger datasets it is better if you save the data in a .csv file and inspect that file directly (see step 4.3, which you can run without going through 4.2).</p> <pre><code># CELL #13\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfor key in summary.keys():\n    # Load and display the image\n    image_path = summary[key][\"filename\"]\n    img = Image.open(image_path)\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(img)\n    plt.axis(\"off\")  # Hide axes\n    plt.title(f\"Summary: {summary[key]['caption']}\", fontsize=12)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#42-visual-question-answering-obtain-answers-to-user-defined-questions-about-the-images","title":"4.2. Visual question answering: obtain answers to user-defined questions about the images","text":"<p>You can also ask questions about the images in the dataset. For this, you need to provide a list of questions and pass it to the Python class instantiated above; to do so, run the two cells below. </p> <pre><code># CELL #14\nlist_of_questions = [\n    \"Who is in the picture?\",\n    \"Does the image show a man, a woman, both, none, or you can't tell?\",\n    \"Does the picture show a flag, and if yes, what colors does it have, and to which country or group does it belong?\",\n]  # add or replace with your own questions\n</code></pre> <pre><code># CELL #15\nsummary_and_answers = image_summary_vqa.analyse_images_from_dict(\n    analysis_type=\"summary_and_questions\",\n    list_of_questions=list_of_questions,\n    is_concise_summary=True,\n    is_concise_answer=True,\n)\n</code></pre> <p>For your convenience, the images and the answers to the questions can be displayed together by running the cell below. This works best for a limited number of images (for larger datasets it is recommended that you save them to a .csv file, see step 4.3 below).</p> <pre><code># CELL #16\nfrom pprint import pprint\n\nfor key in summary_and_answers.keys():\n    # Load and display the image\n    image_path = summary_and_answers[key][\"filename\"]\n    img = Image.open(image_path)\n    for answer in summary_and_answers[key][\"vqa\"]:\n        pprint(answer, width=100, compact=True)\n    plt.figure(figsize=(8, 6))\n    plt.imshow(img)\n    plt.axis(\"off\")  # Hide axes\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#43-export-the-results-from-the-image-content-analysis","title":"4.3. Export the results from the image content analysis","text":"<p>To export the results for further processing, convert the image dictionary into a pandas dataframe.</p> <pre><code># CELL #17\nimage_df = ammico.get_dataframe(image_dict)\n</code></pre> <p>To examine the data, run the cell below.</p> <pre><code># CELL #18\nimage_df.head(20)\n</code></pre> <p>To save the data, run the cell below.</p> <pre><code># CELL #19\nimage_df.to_csv(\"/content/drive/MyDrive/Test/image_summary_questions_data_out.csv\") # make sure you specify the correct file path of your output file\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#step-5-perform-video-content-analysis","title":"Step 5: Perform video content analysis","text":"<p>Depending on whether you arrive at this step after performing image content analysis or not, and depending on your computer resource capabilities, you may have to restart the session and import ammico again. To do so, run the cell below.</p> <pre><code># CELL #20\nimport ammico\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#51-read-your-video-data-into-ammico","title":"5.1 Read your video data into AMMICO","text":"<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>video_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <p>For reading in the files, the ammico function <code>find_videos</code> is used, with optional keywords:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the video files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"mp4\", \"mov\", \"avi\", \"mkv\", \"webm\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>int</code> the random seed for shuffling the videos; applies when only a few videos are read and the selection should be preserved (defaults to <code>None</code>) <pre><code># CELL #21\n# Define your data path\ndata_path = \"/content/drive/MyDrive/Test\"  # the current directory (make sure you specify the correct path of your folder!)\n\n# Find files and create the image dictionary\nvideo_dict = ammico.find_videos(\n    path=data_path,\n    limit=2,  # Limit the number of files to process (optional)\n)\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#42-obtain-the-video-summary","title":"4.2. Obtain the video summary","text":"<p>We begin by creating a video caption (\"Summary\") using the QWEN 2.5 Vision-Language model family. Two variants are supported:</p> <p>This module is built on the Qwen2.5-VL model family. In this project, two model variants are supported:</p> <ol> <li><code>Qwen2.5-VL-3B-Instruct</code>, which requires approximately 3 GB of video memory to load.</li> <li><code>Qwen2.5-VL-7B-Instruct</code>, which requires 8.5 GB of VRAM for initialization (default).</li> </ol> <p>The optimal length of the video is more than 30s and less than ~2-3 minutes. The former is due to possible inaccuracies with the automated language detection from the audio, which requires sufficient data to be accurate (however, you may also specify the language). The latter is due to the high compute demand for long videos.</p> <p>First, the model needs to be specified and loaded into memory. This will take several minutes.</p> <pre><code># CELL #22\nmodel = ammico.MultimodalSummaryModel()  # load the default model\n</code></pre> <p>To analyze the audio content from the video, <code>ammico</code> uses the WhisperX model family for audio transcription as developed by OpenAI. The available flavors available are:</p> <ol> <li><code>small</code></li> <li><code>base</code></li> <li><code>large</code></li> </ol> <p>These models can also detect many languages and provide translations, however are more accurate for longer videos.</p> <pre><code># CELL #23\naudio_model = ammico.model.AudioToTextModel(model_size=\"small\", device=\"cuda\")\n</code></pre> <p>Then, we create an instance of the Python class that handles the image summary and visual question answering tasks:</p> <pre><code># CELL #24\nvid_summary_vqa = ammico.VideoSummaryDetector(\n    summary_model=model, audio_model=audio_model, subdict=video_dict\n)\n</code></pre> <p>After this, we can create the video captions. Depending on the length and number of videos and the hardware provided, this can take several minutes.</p> <pre><code># CELL #25\nsummary = vid_summary_vqa.analyse_videos_from_dict(analysis_type=\"summary\")\n</code></pre> <p>The results are provided in the updated dictionary. For your convenience, you can see the first frame of the video that was analyzed together with the generated caption (summary) by executing the cell below. This works best for a limited number of videos (for larger datasets it is recommended that you save the results to a csv file, see below at step 5.4, which you can run without needing to go through 5.3).</p> <pre><code># CELL #26\nimport cv2\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\n\n\ndef display_first_frame(video_path):\n    cap = cv2.VideoCapture(video_path)\n    ok, frame = cap.read()\n    cap.release()\n    if not ok:\n        raise RuntimeError(f\"Could not read first frame from {video_path}\")\n    # Convert BGR -&gt; RGB for matplotlib\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    plt.figure(figsize=(8, 6))\n    plt.imshow(frame_rgb)\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\nfor key in summary.keys():\n    # Load and display the image\n    video_path = summary[key][\"filename\"]\n    display_first_frame(video_path)\n    pprint(summary[key][\"summary\"], width=100, compact=True)\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#53-visual-question-answering-obtain-answers-to-user-defined-questions-about-the-videos","title":"5.3. Visual question answering: obtain answers to user-defined questions about the videos","text":"<p>You may also ask questions about the videos. For this, provide a list of questions and pass it to the Python class that you have instantiated above. Note that the question answering takes longer than video summarization. Ideally you would carry out both tasks together in one exection as below:</p> <pre><code># CELL #27\nlist_of_questions = [\n    \"Who are the people in the video?\",\n    \"Is Donald Trump in the video, answer with only yes or no?\",\n    \"Are people in the video displaying any emotion? If yes, which ones?\",\n]  # add or replace with your own questions\n</code></pre> <pre><code># CELL #28\nsummary_and_answers = vid_summary_vqa.analyse_videos_from_dict(\n    analysis_type=\"summary_and_questions\", list_of_questions=list_of_questions\n)\n</code></pre> <p>For your convenience, the first frame of the videos and the answers to the questions can be displayed together by running the cell below. This works best for a limited number of videos (for larger datasets it is recommended that you save the results to a csv file, see below).</p> <pre><code># CELL #29\nfor key in summary_and_answers.keys():\n    # Load and display the image\n    video_path = summary_and_answers[key][\"filename\"]\n    display_first_frame(video_path)\n\n    for answer in summary_and_answers[key][\"vqa_answers\"]:\n        pprint(answer, width=100, compact=True)\n</code></pre>"},{"location":"tutorials/ammico_demo_multimodal_analysis/#54-export-the-results-from-the-video-content-analysis","title":"5.4. Export the results from the video content analysis","text":"<p>To export the results for further processing, convert the image dictionary into a pandas dataframe.</p> <pre><code># CELL #30\nvideo_df = ammico.get_dataframe(video_dict)\n</code></pre> <p>Inspect the dataframe:</p> <pre><code># CELL #31\nvideo_df.head(5)\n</code></pre> <p>Export the dataframe to a csv file:</p> <pre><code># CELL #32\nvideo_df.to_csv(\"/content/drive/MyDrive/Test/video_summary_questions_data_out.csv\") # make sure you specify the correct file path of your output file\n</code></pre>"},{"location":"tutorials/colors/","title":"Color composition analysis","text":"<p>Tutorial coming soon!</p> <p>The color composition of the images can be extracted using the <code>ColorDetector</code> class (<code>colors</code> module). </p>"},{"location":"tutorials/colors/#read-your-image-data-into-ammico","title":"Read your image data into <code>ammico</code>","text":"<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <p>For reading in the files, the ammico function <code>find_files</code> is used, with optional keywords:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the image files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\", \"tiff\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>int</code> the random seed for shuffling the images; applies when only a few images are read and the selection should be preserved (defaults to <code>None</code>)"},{"location":"tutorials/colors/#example-usage","title":"Example usage","text":"<p>The color detection is carried out using the following method call: <pre><code>for key in image_dict.keys():\n    image_dict[key] = ammico.ColorDetector(\n        image_dict[key],  \n    ).analyse_image()\n</code></pre> This returns a dictionary with color names as keys and their percentage presence in the image as values (rounded to 2 decimal places).</p>"},{"location":"tutorials/display/","title":"The interactive display","text":"<p>Tutorial coming soon!</p> <p>The Display module provides an interactive web-based dashboard for visualizing and analyzing image data using Dash.</p>"},{"location":"tutorials/display/#read-your-image-data-into-ammico","title":"Read your image data into <code>ammico</code>","text":"<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <p>For reading in the files, the ammico function <code>find_files</code> is used, with optional keywords:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the image files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\", \"tiff\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>int</code> the random seed for shuffling the images; applies when only a few images are read and the selection should be preserved (defaults to <code>None</code>)"},{"location":"tutorials/display/#example-usage","title":"Example usage","text":"<pre><code>explorer = ammico.AnalysisExplorer(mydict=image_dict)\nexplorer.run_server(port=8050)\n</code></pre>"},{"location":"tutorials/getting_started/","title":"Tutorial","text":"<p>This tutorial demonstrates how to use AMMICO to analyze text on images and image content.</p> <p></p> <p>Tutorial notebook</p>"},{"location":"tutorials/getting_started/#installation","title":"Installation","text":"<p>First, install the package using pip:</p> <pre><code>pip install ammico\n</code></pre> <p>Or install the development version from GitHub (currently recommended for the new features):</p> <pre><code>pip install git+https://github.com/ssciwr/AMMICO.git\n</code></pre>"},{"location":"tutorials/getting_started/#step-0-set-up-credentials","title":"Step 0: Set up Credentials","text":"<p>For text extraction using the Google Cloud Vision API, you need to set your API key environment variable.</p> <p><pre><code>import os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/key.json\"\n</code></pre> How to obtain this key is described in setting up credentials. However, you only need this if you plan to extract text from images, not for image summary, VQA or video analysis.</p>"},{"location":"tutorials/getting_started/#step-1-read-data","title":"Step 1: Read Data","text":"<p>AMMICO reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <pre><code>import ammico\n\n# Define your data path\ndata_path = \"./data-test\"\n\n# Find files and create the image dictionary\nimage_dict = ammico.find_files(\n    path=data_path,\n    limit=20, # Limit the number of files to process (optional)\n)\n</code></pre>"},{"location":"tutorials/getting_started/#step-2-interactive-analysis-optional","title":"Step 2: Interactive Analysis (Optional)","text":"<p>You can launch an interactive Dash interface to inspect your data and test different detector settings before running a full analysis. This is mostly useful if you want to try out different models or settings for the analysis.</p> <pre><code># Launch the explorer\nanalysis_explorer = ammico.AnalysisExplorer(image_dict)\nanalysis_explorer.run_server(port=8055)\n</code></pre>"},{"location":"tutorials/getting_started/#step-3-run-detectors","title":"Step 3: Run Detectors","text":"<p>You can run various detectors on your images. The results are stored in the <code>image_dict</code>. For example, running the text detector will add the text to the dictionary: <pre><code>for key in image_dict:\n    image_dict[key] = ammico.TextDetector(\n        image_dict[key],\n    ).analyse_image()\n</code></pre> This will iterate over all images in the dictionary and run the text detector on each one. For batching mode, there are also advanced options that are described in the more focused tutorials. You can run all detectors on the <code>image_dict</code>, and the order does not matter. <pre><code>for key in image_dict:\n    image_dict[key] = ammico.TextDetector(\n        image_dict[key],\n    ).analyse_image()\n    image_dict[key] = ammico.ColorDetector(\n        image_dict[key],\n    ).analyse_image()\n</code></pre> Note that for the image summary detector, you need to initialize the model first and create an instance of the detector class: <pre><code># Initialize the model once\nmodel = ammico.MultimodalSummaryModel(device=\"cuda\") # Use \"cpu\" if no GPU available\n\n# Initialize the detector\nimage_summary_detector = ammico.ImageSummaryDetector(\n    subdict=image_dict, \n    summary_model=model\n)\n</code></pre> Now you can run the inference task: <pre><code>summary = image_summary_detector.analyse_images_from_dict(\n    analysis_type=\"summary\", is_concise_summary=True\n)\n</code></pre> For more information, consult the demonstration notebook  or the more in-depth sections about each analysis tool.</p>"},{"location":"tutorials/getting_started/#privacy-disclosure","title":"Privacy Disclosure","text":"<p>The text detector requires you to accept a disclosure statement, since it sends data to the Google Cloud for processing. You will be prompted for this automatically when you invoke the detector, or you may run it separately using</p> <pre><code># For TextDetector (uses Google Cloud)\nammico.privacy_disclosure(accept_privacy=\"PRIVACY_AMMICO\")\n</code></pre>"},{"location":"tutorials/getting_started/#step-4-export-results","title":"Step 4: Export Results","text":"<p>Convert the results dictionary to a pandas DataFrame and save it to a CSV file.</p> <pre><code># Convert to DataFrame\nimage_df = ammico.get_dataframe(image_dict)\n\n# Save to CSV\nimage_df.to_csv(\"ammico_results.csv\")\n</code></pre> <p>You can now inspect <code>ammico_results.csv</code> to see all extracted features, including text, translations, image summaries and so on, and perform further analysis on the data.</p>"},{"location":"tutorials/image_summary/","title":"Multimodal Summary and Visual Question Answering","text":"<p>Tutorial notebook</p> <p>This detector is built on the QWEN 2.5 Vision-Language model family. In this project, two model variants are supported: </p> <ol> <li><code>Qwen2.5-VL-3B-Instruct</code>, which requires approximately 3 GB of video memory to load.</li> <li><code>Qwen2.5-VL-7B-Instruct</code>, which requires up to 8 GB of VRAM for initialization.</li> </ol> <p>Each version can be run on the CPU, but this will significantly increase the operating time, so we cannot recommend it, but we retain this option.  The model type can be specified when initializing the <code>MultimodalSummaryModel</code> class: <pre><code>model_id = (\n    \"Qwen/Qwen2.5-VL-7B-Instruct\"  # or \"Qwen/Qwen2.5-VL-3B-Instruct\" respectively\n)\nmodel = ammico.MultimodalSummaryModel(model_id=model_id)\n</code></pre> You can also define the preferred device type (\"cpu\" or \"cuda\") explicitly during initialization: <pre><code>model = ammico.MultimodalSummaryModel(model_id=model_id, device=\"cuda\")\n</code></pre> By default, the initialization follows this logic:</p> <p>If a GPU is available, it is automatically detected and the model defaults to Qwen2.5-VL-7B-Instruct on \"cuda\".</p> <p>If no GPU is detected, the system falls back to the Qwen2.5-VL-3B-Instruct model on the \"cpu\" device. <pre><code>model = ammico.MultimodalSummaryModel()\n</code></pre> To instantiate class it is required to provide <code>MultimodalSummaryModel</code> and dictionary <pre><code>image_summary_vqa = ammico.ImageSummaryDetector(summary_model=model, subdict=image_dict)\n</code></pre> To perform image analysis, use the <code>analyse_images_from_dict()</code> method. This function provides flexible options for generating summaries and performing visual question answering. </p> <ol> <li><code>analysis_type</code> \u2013 defines the type of analysis to perform. Setting it to <code>summary</code> will generate a caption (summary), <code>questions</code> will prepare answers (VQA) to a list of questions as set by the user, <code>summary_and_questions</code> will do both.</li> <li><code>list_of_questions</code> a list of text questions to be answered by the model. This parameter is required when analysis_type is set to \"questions\" or \"summary_and_questions\".</li> <li><code>keys_batch_size</code> controls the number of images processed per batch. Increasing this value may slightly improve performance, depending on your system. The default is <code>16</code>, which provides a good balance between speed and stability on most setups.</li> <li><code>is_concise_summary</code> \u2013 determines the level of detail in generated captions:<ul> <li><code>True</code> \u2192 produces short, concise summaries.</li> <li><code>False</code> \u2192 produces longer, more descriptive captions that may include additional context or atmosphere, but take more time to compute.</li> </ul> </li> <li><code>is_concise_answer</code>\u2013 similar to the previous flag, but for controlling the level of detail in question answering responses.</li> </ol>"},{"location":"tutorials/image_summary/#read-your-image-data-into-ammico","title":"Read your image data into <code>ammico</code>","text":"<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <p>For reading in the files, the ammico function <code>find_files</code> is used, with optional keywords:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the image files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\", \"tiff\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>int</code> the random seed for shuffling the images; applies when only a few images are read and the selection should be preserved (defaults to <code>None</code>)"},{"location":"tutorials/image_summary/#example-usage","title":"Example usage","text":"<p>To generate a concise image summary only: <pre><code>summary = image_summary_vqa.analyse_images_from_dict(\n    analysis_type=\"summary\", is_concise_summary=True\n)\n</code></pre> To generate detailed summaries and answer multiple questions:</p> <p>First, define a list of questions: <pre><code>list_of_questions = [\n    \"How many persons on the picture?\",\n    \"Are there any politicians in the picture?\",\n    \"Does the picture show something from medicine?\",\n]\n</code></pre> Then call the function: <pre><code>summary_and_answers = ammico.analyse_images_from_dict(\n    analysis_type=\"summary_and_questions\",\n    list_of_questions=list_of_questions,\n    is_concise_summary=False,\n    is_concise_answer=False,\n)\n</code></pre> The output of the <code>analyse_images_from_dict()</code> method is a dictionary, where each key corresponds to an input image identifier. Each entry in this dictionary contains the processed results for that image.</p> output key output type output value <code>caption</code> <code>str</code> when <code>analysis_type=\"summary\"</code> or <code>\"summary_and_questions\"</code>, constant image caption <code>vqa</code> <code>list[str]</code> when <code>analysis_type=\"questions\"</code> or <code>summary_and_questions</code>, the answers to the user-defined input question"},{"location":"tutorials/jupyter4nfdi/","title":"Jupyter4NFDI","text":"<p>Run on Jupyter4NDFI: - Select Helmholtz as login - Select ORCID</p> <p>source activate base conda activate /home/jovyan/miniforge3/envs/ammico to run in shell, otherwise just select the existing kernel</p>"},{"location":"tutorials/text/","title":"Text detector","text":"<p>Tutorial coming soon!</p> <p>Text on the images can be extracted using the <code>TextDetector</code> class (<code>text</code> module). The text is initally extracted using the Google Cloud Vision API and then translated into English with googletrans. The translated text is cleaned of whitespace, linebreaks, and numbers using Python syntax and spaCy. </p> <p>Please note that for the Google Cloud Vision API (the TextDetector class) you need to set a key in order to process the images. This key is ideally set as an environment variable using for example <pre><code>os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"&lt;path_to_your_service_account_key&gt;.json\"\n</code></pre> where you place the key on your Google Drive if running on colab, or place it in a local folder on your machine.</p>"},{"location":"tutorials/text/#read-your-image-data-into-ammico","title":"Read your image data into <code>ammico</code>","text":"<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p> <p>For reading in the files, the ammico function <code>find_files</code> is used, with optional keywords:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the image files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"png\", \"jpg\", \"jpeg\", \"gif\", \"webp\", \"avif\", \"tiff\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>int</code> the random seed for shuffling the images; applies when only a few images are read and the selection should be preserved (defaults to <code>None</code>)"},{"location":"tutorials/text/#example-usage","title":"Example usage","text":"<p>The text detection is carried out using the following method call: <pre><code>for key in image_dict.keys():\n    image_dict[key] = ammico.TextDetector(\n        image_dict[key],  \n    ).analyse_image()\n</code></pre></p> <p>A detailed description of the output keys and data types is given in the following table.</p> output key output type output value <code>text</code> <code>str</code> the extracted text in the original language <code>text_language</code> <code>str</code> the detected dominant language of the extracted text <code>text_english</code> <code>str</code> the text translated into English <code>text_clean</code> <code>str</code> the text after cleaning from numbers and unrecognizable words"},{"location":"tutorials/video_summary/","title":"Video summary and VQA module","text":"<p>Tutorial notebook</p> <p>Also the <code>MultimodalSummaryDetector</code> can be used to generate video captions (<code>summary</code>) as well as visual question answering (<code>VQA</code>) for visual part of video file. This again uses the QWEN 2.5 Vision-Language model family</p> <p><pre><code>model = ammico.MultimodalSummaryModel(model_id=model_id)\n</code></pre> To analyze the audio content from the video, <code>ammico</code> uses the WhisperX model family for audio transcription as developed by OpenAI. This will lead to higher accuracy. The <code>AudioToTextModel</code> model is responsible for this in <code>ammico</code>. By default, it loads a small model on the GPU (if your device supports CUDA), also you can specify size of the audio model (\"small\", \"base\", \"large\"), or device (\"cuda\" or \"cpu\") if you want. Increasing the model size can improve the result of converting an audio track to text, but consumes more RAM or VRAM. <pre><code>audio_model = ammico.AudioToTextModel(model_size=\"small\", device=\"cuda\")\n</code></pre></p>"},{"location":"tutorials/video_summary/#read-your-video-data-into-ammico","title":"Read your video data into AMMICO","text":"<p>The ammico package reads in one or several input video files given in a folder for processing. The user can select to read in all videos in a folder, to include subfolders via the <code>recursive</code> option, and can select the file extensions that should be considered (i.e. \"mp4\"). For reading in the files, the ammico function <code>find_videos</code> is used, with supported extensions supported:</p> input key input type possible input values <code>path</code> <code>str</code> the directory containing the video files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>) <code>pattern</code> <code>str\\|list</code> the file extensions to consider (defaults to \"mp4\", \"mov\", \"avi\", \"mkv\", \"webm\") <code>recursive</code> <code>bool</code> include subdirectories recursively (defaults to <code>True</code>) <code>limit</code> <code>int</code> maximum number of files to read (defaults to <code>5</code>, for all videos set to <code>None</code> or <code>-1</code>) <code>random_seed</code> <code>str</code> the random seed for shuffling the videos; applies when only a few videos are read and the selection should be preserved (defaults to <code>None</code>) <p>The <code>find_videos</code> function returns a nested dictionary that contains the file ids and the paths to the files and is empty otherwise.  <pre><code>video_dict = ammico.find_videos(\n    path=str(\"/insert/your/path/here/\"),  # path to the folder with videos\n    limit=-1,  # -1 means no limit on the number of files, by default it is set to 20\n    pattern=\"mp4\",  # file extensions to look for\n)\n</code></pre></p>"},{"location":"tutorials/video_summary/#example-usage","title":"Example usage","text":"<p>To instantiate class it is required to provide <code>MultimodalSummaryModel</code> and <code>video_dict</code>. Optionally you may provide <code>AudioToTextModel</code> for more precise results. <pre><code>vid_summary_vqa = ammico.VideoSummaryDetector(\n    summary_model=model, audio_model=audio_model, subdict=video_dict\n)\n</code></pre> To perform video analysis, use the <code>analyse_videos_from_dict()</code> method. This function provides flexible options for generating summaries and performing visual question answering. </p> <ol> <li><code>analysis_type</code> \u2013 defines the type of analysis to perform. Setting it to <code>summary</code> will generate a caption (summary), <code>questions</code> will prepare answers (VQA) to a list of questions as set by the user, <code>summary_and_questions</code> will do both.</li> <li><code>list_of_questions</code> a list of text questions to be answered by the model. This parameter is required when analysis_type is set to \"questions\" or \"summary_and_questions\".</li> </ol> <p>To generate a concise video summary only:</p> <p><pre><code>summary_dict = vid_summary_vqa.analyse_videos_from_dict(analysis_type=\"summary\")\n</code></pre> To generate detailed summaries and answer multiple questions:</p> <p>First, define a list of questions: <pre><code>questions = [\"What did people in the frame say?\"]\n</code></pre></p> <p>Then call the function: <pre><code>vqa_results = vid_summary_vqa.analyse_videos_from_dict(\n    analysis_type=\"questions\",\n    list_of_questions=questions,\n)\n</code></pre> or, in case of both summary and VQA: <pre><code>vqa_results = vid_summary_vqa.analyse_videos_from_dict(\n    analysis_type=\"summary_and_questions\",\n    list_of_questions=questions,\n)\n</code></pre></p>"}]}