
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="AI-based Media and Misinformation Content Analysis Tool">
      
      
      
        <link rel="canonical" href="https://ssciwr.github.io/AMMICO/tutorials/ammico_demo_multimodal_analysis/">
      
      
        <link rel="prev" href="../text/">
      
      
        <link rel="next" href="../colors/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Multimodal analysis - AMMICO</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#getting-started-with-ammico" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AMMICO" class="md-header__button md-logo" aria-label="AMMICO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AMMICO
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multimodal analysis
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ssciwr/AMMICO" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    ssciwr/AMMICO
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AMMICO" class="md-nav__button md-logo" aria-label="AMMICO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AMMICO
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ssciwr/AMMICO" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    ssciwr/AMMICO
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Getting started
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image_summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Image summary and VQA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../video_summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Video summary and VQA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../text/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Text analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Multimodal analysis
  

    
  </span>
  
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../colors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Color analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../display/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Interactive display
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Modules
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Modules
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/text/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Text analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/image_summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Image summary and VQA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/video_summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Video summary and VQA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/colors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Color analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../modules/display/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Interactive display
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    More information
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    More information
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../set_up_credentials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Set up credentials for Google Cloud API
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FAQ
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="getting-started-with-ammico">Getting started with <code>ammico</code></h1>
<p>Version date: 28.01.2026</p>
<p>With <code>ammico</code>, you can analyze text on images and visual content (image and video) at the same time. This tutorial notebook shows you how, after importing ammico (Step 1) and uploading your data (Step 2), you can use all three modules: text extraction (Step 3), image analysis (Step 4) and video analysis (Step 5). The modules are independent, so you can just perform, for example, image analysis or video analysis without needing to go through the others. </p>
<p>You can run this notebook on google colab or locally / on your own HPC resource. For production data processing, it is recommended to run the analysis locally on a GPU-supported machine. You can also make use of the colab GPU runtime, or purchase additional runtime. However, google colab comes with pre-installed libraries that can lead to dependency conflicts.</p>
<p>This first cell only runs on google colab; on all other machines, you need to create a conda environment first and install ammico from the Python Package Index using<br />
<code>pip install ammico</code></p>
<p>Alternatively you can install the development version from the GitHub repository<br />
<code>pip install git+https://github.com/ssciwr/AMMICO.git</code></p>
<p><b>On google colab, select "TPU" as runtime, otherwise the notebook may not run. To do so, go to:<br />
Runtime -&gt; Change runtime type -&gt; Select Hardware accelerator T4 GPU -&gt; Press Save</b></p>
<!--
<div style="display: flex; justify-content: space-around; align-items: center;">
  <img src="https://drive.google.com/file/d/1JBRS2mZ4uy1rlvrNrt1PDdRjx6OxU8Ch/view?usp=drive_link" alt="Select Runtime" style="width: 45%;">
  <img src="https://drive.google.com/file/d/1NSH6eOeYX-wKrlljfjpwuCr1b5ngAinW/view?usp=drive_link" alt="Runtime Options" style="width: 45%;">
</div> -->

<p>Then you need to uninstall the already installed <code>transformers</code> version, and <code>peft</code>, since these lead to dependency conflicts. Then you can install <code>ammico</code>.</p>
<p>Simply execute the cell below by pressing shift+enter.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #1</span>
<span class="c1"># when running on Google colab, otherwise the below cell is skipped</span>
<span class="k">if</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_ipython</span><span class="p">()):</span>
    <span class="c1"># uv is a fast Python package manager, see https://github.com/astral-sh/uv</span>
    <span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">uv</span>
    <span class="c1"># Uninstall conflicting packages</span>
    <span class="err">!</span><span class="n">uv</span> <span class="n">pip</span> <span class="n">uninstall</span> <span class="n">peft</span> <span class="n">transformers</span>
    <span class="c1"># Install ammico as the latest version from GitHub, which will pull in the compatible dependencies</span>
    <span class="err">!</span><span class="n">uv</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ssciwr</span><span class="o">/</span><span class="n">ammico</span><span class="o">.</span><span class="n">git</span>
</code></pre></div>
<p>Now you need to restart the kernel to load the new dependencies. <strong>For this, click on "Runtime -&gt; Restart Session" or press Ctrl+M</strong>.</p>
<h1 id="step-1-import-ammico">Step 1: Import AMMICO</h1>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #2</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ammico</span>
</code></pre></div>
<p>This imports all the functionality from <code>ammico</code>. To analyze images, you need to upload images to google colab or <a href="https://colab.research.google.com/notebooks/io.ipynb">connect to your Google Drive</a>. To connect to your google drive you need to run the cell below.</p>
<!-- To upload files (note that these will not persist over the runtime of the notebook), click on the folder symbol ("Files") on the left navbar and press the upload button.

<div style="display: flex; justify-content: space-around; align-items: center;">
  <img src="../_static/select_files.png" alt="Select Files" style="width: 45%;">
  <img src="../_static/upload_files.png" alt="Upload Files" style="width: 45%;">
</div> -->

<div class="highlight"><pre><span></span><code><span class="c1"># CELL #3</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">google.colab</span><span class="w"> </span><span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</code></pre></div>
<h1 id="step-2-read-your-image-data-into-ammico">Step 2: Read your image data into AMMICO</h1>
<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>image_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p>
<p>For reading in the files, the ammico function <code>find_files</code> is used, with optional keywords:</p>
<table>
<thead>
<tr>
<th>input key</th>
<th>input type</th>
<th>possible input values</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>path</code></td>
<td><code>str</code></td>
<td>the directory containing the image files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>)</td>
</tr>
<tr>
<td><code>pattern</code></td>
<td><code>str\|list</code></td>
<td>the file extensions to consider (defaults to "png", "jpg", "jpeg", "gif", "webp", "avif", "tiff")</td>
</tr>
<tr>
<td><code>recursive</code></td>
<td><code>bool</code></td>
<td>include subdirectories recursively (defaults to <code>True</code>)</td>
</tr>
<tr>
<td><code>limit</code></td>
<td><code>int</code></td>
<td>maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>)</td>
</tr>
<tr>
<td><code>random_seed</code></td>
<td><code>int</code></td>
<td>the random seed for shuffling the images; applies when only a few images are read and the selection should be preserved (defaults to <code>None</code>)</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #4</span>
<span class="c1"># Define your data path</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;/content/drive/MyDrive/Test&quot;</span>  <span class="c1"># the current directory (make sure you specify the correct path of your folder!)</span>

<span class="c1"># Find files and create the image dictionary</span>
<span class="n">image_dict</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">find_files</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>  <span class="c1"># Limit the number of files to process (optional)</span>
<span class="p">)</span>
</code></pre></div>
<h1 id="step-3-extract-the-text-from-images-in-the-dataset">Step 3: Extract the text from images in the dataset</h1>
<p>In order to be able to extract the text, you will need a google cloud vision API key. To get such a key follow the instructions here: https://ssciwr.github.io/AMMICO/set_up_credentials/.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #5</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;/content/drive/MyDrive/Test/[KEYNAME].json&quot;</span> <span class="c1"># make sure you specify the correct file path of your cloud vision API key</span>
</code></pre></div>
<p>To extract the text from images, you will need to run the cell below, and answer "yes" when prompted to accept the privacy disclosure.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #6</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">image_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">image_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">TextDetector</span><span class="p">(</span>
        <span class="n">image_dict</span><span class="p">[</span><span class="n">key</span><span class="p">],</span>
    <span class="p">)</span><span class="o">.</span><span class="n">analyse_image</span><span class="p">()</span>
</code></pre></div>
<pre><code>The Text Detector uses Google Cloud Vision
    and Google Translate. Detailed information about how information
    is being processed is provided here:
    https://ssciwr.github.io/AMMICO/build/html/faq_link.html.
    Googleâ€™s privacy policy can be read here: https://policies.google.com/privacy.
    By continuing to use this Detector, you agree to send the data you want analyzed
    to the Google servers for extraction and translation.
Do you accept the privacy disclosure? (yes/no): yes
You have accepted the privacy disclosure.
Text detection and translation will be performed.
</code></pre>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #7</span>
<span class="n">image_df</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">get_dataframe</span><span class="p">(</span><span class="n">image_dict</span><span class="p">)</span>
</code></pre></div>
<h1 id="31-inspect-and-save-the-extracted-text-data">3.1 Inspect and save the extracted text data</h1>
<p>To inspect the data run the cell below</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #8</span>
<span class="n">image_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div>
<p>To save the data, run the cell below.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #9</span>
<span class="n">image_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/Test/text_data_out.csv&quot;</span><span class="p">)</span> <span class="c1"># make sure you specify the correct file path of your output file</span>
</code></pre></div>
<h1 id="step-4-perform-image-content-analysis">Step 4. Perform image content analysis</h1>
<h1 id="41-obtain-the-image-summary">4.1. Obtain the image summary</h1>
<p>We begin by creating an image caption ("Summary") using the <a href="https://huggingface.co/collections/Qwen/qwen25-vl">QWEN 2.5 Vision-Language model family</a>. Two variants are supported:</p>
<p>This module is built on the Qwen2.5-VL model family. In this project, two model variants are supported:</p>
<ol>
<li><code>Qwen2.5-VL-3B-Instruct</code>, which requires approximately 3 GB of video memory to load.</li>
<li><code>Qwen2.5-VL-7B-Instruct</code>, which requires 8.5 GB of VRAM for initialization (default).</li>
</ol>
<p>First, the model needs to be specified and loaded into memory. This will take several minutes.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #10</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">MultimodalSummaryModel</span><span class="p">()</span>  <span class="c1"># load the default model</span>
</code></pre></div>
<p>Then, we create an instance of the Python class that handles the image summary and visual question answering tasks:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #11</span>
<span class="n">image_summary_vqa</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">ImageSummaryDetector</span><span class="p">(</span><span class="n">summary_model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">subdict</span><span class="o">=</span><span class="n">image_dict</span><span class="p">)</span>
</code></pre></div>
<p>After this, we can create the image captions. Depending on the number of images and the hardware provided, this can take several minutes.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #12</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">image_summary_vqa</span><span class="o">.</span><span class="n">analyse_images_from_dict</span><span class="p">(</span>
    <span class="n">analysis_type</span><span class="o">=</span><span class="s2">&quot;summary&quot;</span><span class="p">,</span> <span class="n">is_concise_summary</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<p>The results are provided in the updated dictionary. For your convenience, you can execute the cell below to see the image that was analyzed together with the generated caption (summary). This works best for a limited number of images; for larger datasets it is better if you save the data in a .csv file and inspect that file directly (see step 4.3, which you can run without going through 4.2).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #13</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">summary</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="c1"># Load and display the image</span>
    <span class="n">image_path</span> <span class="o">=</span> <span class="n">summary</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;filename&quot;</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>  <span class="c1"># Hide axes</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Summary: </span><span class="si">{</span><span class="n">summary</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s1">&#39;caption&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h1 id="42-visual-question-answering-obtain-answers-to-user-defined-questions-about-the-images">4.2. Visual question answering: obtain answers to user-defined questions about the images</h1>
<p>You can also ask questions about the images in the dataset. For this, you need to provide a list of questions and pass it to the Python class instantiated above; to do so, run the two cells below. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #14</span>
<span class="n">list_of_questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Who is in the picture?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Does the image show a man, a woman, both, none, or you can&#39;t tell?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Does the picture show a flag, and if yes, what colors does it have, and to which country or group does it belong?&quot;</span><span class="p">,</span>
<span class="p">]</span>  <span class="c1"># add or replace with your own questions</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #15</span>
<span class="n">summary_and_answers</span> <span class="o">=</span> <span class="n">image_summary_vqa</span><span class="o">.</span><span class="n">analyse_images_from_dict</span><span class="p">(</span>
    <span class="n">analysis_type</span><span class="o">=</span><span class="s2">&quot;summary_and_questions&quot;</span><span class="p">,</span>
    <span class="n">list_of_questions</span><span class="o">=</span><span class="n">list_of_questions</span><span class="p">,</span>
    <span class="n">is_concise_summary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">is_concise_answer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>For your convenience, the images and the answers to the questions can be displayed together by running the cell below. This works best for a limited number of images (for larger datasets it is recommended that you save them to a .csv file, see step 4.3 below).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #16</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span>

<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">summary_and_answers</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="c1"># Load and display the image</span>
    <span class="n">image_path</span> <span class="o">=</span> <span class="n">summary_and_answers</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;filename&quot;</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">answer</span> <span class="ow">in</span> <span class="n">summary_and_answers</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;vqa&quot;</span><span class="p">]:</span>
        <span class="n">pprint</span><span class="p">(</span><span class="n">answer</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">compact</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>  <span class="c1"># Hide axes</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<h1 id="43-export-the-results-from-the-image-content-analysis">4.3. Export the results from the image content analysis</h1>
<p>To export the results for further processing, convert the image dictionary into a pandas dataframe.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #17</span>
<span class="n">image_df</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">get_dataframe</span><span class="p">(</span><span class="n">image_dict</span><span class="p">)</span>
</code></pre></div>
<p>To examine the data, run the cell below.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #18</span>
<span class="n">image_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div>
<p>To save the data, run the cell below.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #19</span>
<span class="n">image_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/Test/image_summary_questions_data_out.csv&quot;</span><span class="p">)</span> <span class="c1"># make sure you specify the correct file path of your output file</span>
</code></pre></div>
<h1 id="step-5-perform-video-content-analysis">Step 5: Perform video content analysis</h1>
<p>Depending on whether you arrive at this step after performing image content analysis or not, and depending on your computer resource capabilities, you may have to restart the session and import ammico again. To do so, run the cell below.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #20</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ammico</span>
</code></pre></div>
<h1 id="51-read-your-video-data-into-ammico">5.1 Read your video data into AMMICO</h1>
<p><code>ammico</code> reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary <code>video_dict</code>, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.</p>
<p>For reading in the files, the ammico function <code>find_videos</code> is used, with optional keywords:</p>
<table>
<thead>
<tr>
<th>input key</th>
<th>input type</th>
<th>possible input values</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>path</code></td>
<td><code>str</code></td>
<td>the directory containing the video files (defaults to the location set by environment variable <code>AMMICO_DATA_HOME</code>)</td>
</tr>
<tr>
<td><code>pattern</code></td>
<td><code>str\|list</code></td>
<td>the file extensions to consider (defaults to "mp4", "mov", "avi", "mkv", "webm")</td>
</tr>
<tr>
<td><code>recursive</code></td>
<td><code>bool</code></td>
<td>include subdirectories recursively (defaults to <code>True</code>)</td>
</tr>
<tr>
<td><code>limit</code></td>
<td><code>int</code></td>
<td>maximum number of files to read (defaults to <code>20</code>, for all images set to <code>None</code> or <code>-1</code>)</td>
</tr>
<tr>
<td><code>random_seed</code></td>
<td><code>int</code></td>
<td>the random seed for shuffling the videos; applies when only a few videos are read and the selection should be preserved (defaults to <code>None</code>)</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #21</span>
<span class="c1"># Define your data path</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s2">&quot;/content/drive/MyDrive/Test&quot;</span>  <span class="c1"># the current directory (make sure you specify the correct path of your folder!)</span>

<span class="c1"># Find files and create the image dictionary</span>
<span class="n">video_dict</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">find_videos</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span>
    <span class="n">limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Limit the number of files to process (optional)</span>
<span class="p">)</span>
</code></pre></div>
<h1 id="42-obtain-the-video-summary">4.2. Obtain the video summary</h1>
<p>We begin by creating a video caption ("Summary") using the <a href="https://huggingface.co/collections/Qwen/qwen25-vl">QWEN 2.5 Vision-Language model family</a>. Two variants are supported:</p>
<p>This module is built on the Qwen2.5-VL model family. In this project, two model variants are supported:</p>
<ol>
<li><code>Qwen2.5-VL-3B-Instruct</code>, which requires approximately 3 GB of video memory to load.</li>
<li><code>Qwen2.5-VL-7B-Instruct</code>, which requires 8.5 GB of VRAM for initialization (default).</li>
</ol>
<p>The optimal length of the video is more than 30s and less than ~2-3 minutes. The former is due to possible inaccuracies with the automated language detection from the audio, which requires sufficient data to be accurate (however, you may also specify the language). The latter is due to the high compute demand for long videos.</p>
<p>First, the model needs to be specified and loaded into memory. This will take several minutes.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #22</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">MultimodalSummaryModel</span><span class="p">()</span>  <span class="c1"># load the default model</span>
</code></pre></div>
<p>To analyze the audio content from the video, <code>ammico</code> uses the <a href="https://github.com/m-bain/whisperX">WhisperX model family</a> for audio transcription as <a href="https://arxiv.org/abs/2303.00747">developed by OpenAI</a>. The available flavors available are:</p>
<ol>
<li><code>small</code></li>
<li><code>base</code></li>
<li><code>large</code></li>
</ol>
<p>These models can also detect many languages and provide translations, however are more accurate for longer videos.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #23</span>
<span class="n">audio_model</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">AudioToTextModel</span><span class="p">(</span><span class="n">model_size</span><span class="o">=</span><span class="s2">&quot;small&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Then, we create an instance of the Python class that handles the image summary and visual question answering tasks:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #24</span>
<span class="n">vid_summary_vqa</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">VideoSummaryDetector</span><span class="p">(</span>
    <span class="n">summary_model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">audio_model</span><span class="o">=</span><span class="n">audio_model</span><span class="p">,</span> <span class="n">subdict</span><span class="o">=</span><span class="n">video_dict</span>
<span class="p">)</span>
</code></pre></div>
<p>After this, we can create the video captions. Depending on the length and number of videos and the hardware provided, this can take several minutes.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #25</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">vid_summary_vqa</span><span class="o">.</span><span class="n">analyse_videos_from_dict</span><span class="p">(</span><span class="n">analysis_type</span><span class="o">=</span><span class="s2">&quot;summary&quot;</span><span class="p">)</span>
</code></pre></div>
<p>The results are provided in the updated dictionary. For your convenience, you can see the first frame of the video that was analyzed together with the generated caption (summary) by executing the cell below. This works best for a limited number of videos (for larger datasets it is recommended that you save the results to a csv file, see below at step 5.4, which you can run without needing to go through 5.3).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #26</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span>


<span class="k">def</span><span class="w"> </span><span class="nf">display_first_frame</span><span class="p">(</span><span class="n">video_path</span><span class="p">):</span>
    <span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>
    <span class="n">ok</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ok</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not read first frame from </span><span class="si">{</span><span class="n">video_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Convert BGR -&gt; RGB for matplotlib</span>
    <span class="n">frame_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">frame_rgb</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">summary</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="c1"># Load and display the image</span>
    <span class="n">video_path</span> <span class="o">=</span> <span class="n">summary</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;filename&quot;</span><span class="p">]</span>
    <span class="n">display_first_frame</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">summary</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;summary&quot;</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">compact</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h1 id="53-visual-question-answering-obtain-answers-to-user-defined-questions-about-the-videos">5.3. Visual question answering: obtain answers to user-defined questions about the videos</h1>
<p>You may also ask questions about the videos. For this, provide a list of questions and pass it to the Python class that you have instantiated above. Note that the question answering takes longer than video summarization. Ideally you would carry out both tasks together in one exection as below:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #27</span>
<span class="n">list_of_questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Who are the people in the video?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Is Donald Trump in the video, answer with only yes or no?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Are people in the video displaying any emotion? If yes, which ones?&quot;</span><span class="p">,</span>
<span class="p">]</span>  <span class="c1"># add or replace with your own questions</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #28</span>
<span class="n">summary_and_answers</span> <span class="o">=</span> <span class="n">vid_summary_vqa</span><span class="o">.</span><span class="n">analyse_videos_from_dict</span><span class="p">(</span>
    <span class="n">analysis_type</span><span class="o">=</span><span class="s2">&quot;summary_and_questions&quot;</span><span class="p">,</span> <span class="n">list_of_questions</span><span class="o">=</span><span class="n">list_of_questions</span>
<span class="p">)</span>
</code></pre></div>
<p>For your convenience, the first frame of the videos and the answers to the questions can be displayed together by running the cell below. This works best for a limited number of videos (for larger datasets it is recommended that you save the results to a csv file, see below).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #29</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">summary_and_answers</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="c1"># Load and display the image</span>
    <span class="n">video_path</span> <span class="o">=</span> <span class="n">summary_and_answers</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;filename&quot;</span><span class="p">]</span>
    <span class="n">display_first_frame</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">answer</span> <span class="ow">in</span> <span class="n">summary_and_answers</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s2">&quot;vqa_answers&quot;</span><span class="p">]:</span>
        <span class="n">pprint</span><span class="p">(</span><span class="n">answer</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">compact</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h1 id="54-export-the-results-from-the-video-content-analysis">5.4. Export the results from the video content analysis</h1>
<p>To export the results for further processing, convert the image dictionary into a pandas dataframe.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #30</span>
<span class="n">video_df</span> <span class="o">=</span> <span class="n">ammico</span><span class="o">.</span><span class="n">get_dataframe</span><span class="p">(</span><span class="n">video_dict</span><span class="p">)</span>
</code></pre></div>
<p>Inspect the dataframe:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #31</span>
<span class="n">video_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>
<p>Export the dataframe to a csv file:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># CELL #32</span>
<span class="n">video_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;/content/drive/MyDrive/Test/video_summary_questions_data_out.csv&quot;</span><span class="p">)</span> <span class="c1"># make sure you specify the correct file path of your output file</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.sections", "navigation.indexes", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>