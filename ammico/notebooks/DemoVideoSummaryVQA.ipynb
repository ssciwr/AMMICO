{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video summary and visual question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ammico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read your data into AMMICO\n",
    "The ammico package reads in one or several input video files given in a folder for processing. The user can select to read in all videos in a folder, to include subfolders via the `recursive` option, and can select the file extension that should be considered (i.e. \"mp4\"). For reading in the files, the ammico function `find_videos` is used, with supported extentions supported:\n",
    "\n",
    "| input key | input type | possible input values |\n",
    "| --------- | ---------- | --------------------- |\n",
    "`path` | `str` | the directory containing the video files (defaults to the location set by environment variable `AMMICO_DATA_HOME`) |\n",
    "| `pattern` | `str\\|list` | the file extensions to consider (defaults to \"mp4\", \"mov\", \"avi\", \"mkv\", \"webm\") |\n",
    "| `recursive` | `bool` | include subdirectories recursively (defaults to `True`) |\n",
    "| `limit` | `int` | maximum number of files to read (defaults to `5`, for all videos set to `None` or `-1`) |\n",
    "| `random_seed` | `str` | the random seed for shuffling the videos; applies when only a few videos are read and the selection should be preserved (defaults to `None`) |\n",
    "\n",
    "The `find_videos` function returns a nested dictionary that contains the file ids and the paths to the files and is empty otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = ammico.find_videos(\n",
    "    path=str(\"/insert/your/path/here/\"),  # path to the folder with videos\n",
    "    limit=-1,  # -1 means no limit on the number of files, by default it is set to 20\n",
    "    pattern=\"mp4\",  # file extensions to look for\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the model for VQA tasks. By default, it loads a large model on the GPU (if your device supports CUDA), otherwise it loads a relatively smaller model on the CPU. But you can specify other settings (e.g., a small model on the GPU) if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ammico.MultimodalSummaryModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the model for audio to text extraction, for more precise VQA results. By default, it loads a small model on the GPU (if your device supports CUDA), also you can specify size of the audio model (\"small\", \"base\", \"large\"), or device (\"cuda\" or \"cpu\") if you want. Increasing the model size can improve the result of converting an audio track to text, but consumes more RAM or VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = ammico.model.AudioToTextModel(model_size=\"small\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates an object that analyzes videos and generates a summary and/or answers questions using a specific vqa model and audio model (optional, since you may want to analyze only visual part of a video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_summary_model = ammico.VideoSummaryDetector(\n",
    "    summary_model=model, audio_model=audio_model, subdict=video_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start your work with videos, you should call the `analyse_videos_from_dict` method.\n",
    "\n",
    "You can specify what kind of analysis you want to perform with `analysis_type`. `\"summary\"` will generate a summary for all videos in your dictionary, `\"questions\"` will prepare answers to your questions for all videos, and `\"summary_and_questions\"` will do both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = vid_summary_model.analyse_videos_from_dict(analysis_type=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video VQA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to analyzing videos in `ammico`, the same model can be used in VQA mode. To do this, you need to define the questions that will be applied to all videos from your dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What did people in the frame say?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_results = vid_summary_model.analyse_videos_from_dict(\n",
    "    analysis_type=\"questions\",\n",
    "    list_of_questions=questions,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ammico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
