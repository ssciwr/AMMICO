{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Analyzing video content with `ammico`\n",
    "\n",
    "This is a tutorial notebook to get you started with video summarization and question answering (VQA).\n",
    "You can run this notebook on google colab or locally / on your own HPC resource. For production data processing, it is recommended to run the analysis locally on a GPU-supported machine. You can also make use of the colab GPU runtime, or purchase additional runtime. However, google colab comes with pre-installed libraries that can lead to dependency conflicts. \n",
    "\n",
    "This first cell only runs on google colab; on all other machines, you need to create a conda environment first and install ammico from the Python Package Index using  \n",
    "```pip install ammico```  \n",
    "Alternatively you can install the development version from the GitHub repository  \n",
    "```pip install git+https://github.com/ssciwr/AMMICO.git```\n",
    "\n",
    "On google colab, select \"TPU\" as runtime, otherwise the notebook may not run:  \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "  <img src=\"../_static/select_runtime.png\" alt=\"Select Runtime\" style=\"width: 45%;\">\n",
    "  <img src=\"../_static/runtime_options.png\" alt=\"Runtime Options\" style=\"width: 45%;\">\n",
    "</div>\n",
    "\n",
    "Then you need to uninstall the already installed `transformers` version, and `peft`, since these lead to dependency conflicts. Then you can install `ammico`. Simply execute the cell below by pressing shift+enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when running on Google colab, otherwise the below cell is skipped\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # uv is a fast Python package manager, see https://github.com/astral-sh/uv\n",
    "    %pip install uv\n",
    "    # Uninstall conflicting packages\n",
    "    !uv pip uninstall peft transformers\n",
    "    # Install ammico as the latest version from GitHub, which will pull in the compatible dependencies\n",
    "    !uv pip install git+https://github.com/ssciwr/ammico.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Now you need to restart the kernel to load the new dependencies. For this, click on \"Runtime -> Restart Session\" or press Ctrl+M.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "  <img src=\"../_static/restart_session.png\" alt=\"Restart Session\" style=\"width: 45%;\">\n",
    "</div>\n",
    "\n",
    "Now you are ready to import ammico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ammico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "This imports all the functionality from `ammico`. To analyze images, you need to upload images to google colab or [connect to your Google Drive](https://colab.research.google.com/notebooks/io.ipynb). To upload files (note that these will not persist over the runtime of the notebook), click on the folder symbol (\"Files\") on the left navbar and press the upload button.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "  <img src=\"../_static/select_files.png\" alt=\"Select Files\" style=\"width: 45%;\">\n",
    "  <img src=\"../_static/upload_files.png\" alt=\"Upload Files\" style=\"width: 45%;\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Step 1: Read your data into AMMICO\n",
    "\n",
    "`ammico` reads in files from a directory. You can iterate through directories in a recursive manner and filter by extensions. Note that the order of the files may vary on different OS. Reading in these files creates a dictionary `video_dict`, with one entry per image file, containing the file path and filename. This dictionary is the main data structure that ammico operates on and is extended successively with each detector run as explained below.\n",
    "\n",
    "For reading in the files, the ammico function `find_videos` is used, with optional keywords:\n",
    "\n",
    "| input key | input type | possible input values |\n",
    "| --------- | ---------- | --------------------- |\n",
    "| `path` | `str` | the directory containing the video files (defaults to the location set by environment variable `AMMICO_DATA_HOME`) |\n",
    "| `pattern` | `str\\|list` | the file extensions to consider (defaults to \"mp4\", \"mov\", \"avi\", \"mkv\", \"webm\") |\n",
    "| `recursive` | `bool` | include subdirectories recursively (defaults to `True`) |\n",
    "| `limit` | `int` | maximum number of files to read (defaults to `20`, for all images set to `None` or `-1`) |\n",
    "| `random_seed` | `int` | the random seed for shuffling the videos; applies when only a few videos are read and the selection should be preserved (defaults to `None`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data path\n",
    "data_path = \".\"  # the current directory\n",
    "\n",
    "# Find files and create the image dictionary\n",
    "video_dict = ammico.find_videos(\n",
    "    path=data_path,\n",
    "    limit=10,  # Limit the number of files to process (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Run the content analysis: Video summary\n",
    "\n",
    "As an example we will create a video caption (\"Summary\") using the [QWEN 2.5 Vision-Language model family](https://huggingface.co/collections/Qwen/qwen25-vl). Two variants are supported:\n",
    "\n",
    "This module is built on the Qwen2.5-VL model family. In this project, two model variants are supported: \n",
    "\n",
    "1. `Qwen2.5-VL-3B-Instruct`, which requires approximately 3 GB of video memory to load.\n",
    "2. `Qwen2.5-VL-7B-Instruct`, which requires 8.5 GB of VRAM for initialization (default).\n",
    "\n",
    "The optimal length of the video is more than 30s and less than ~2-3 minutes. The former is due to possible inaccuracies with the automated language detection from the audio, which requires sufficient data to be accurate (however, you may also specify the language). The latter is due to the high compute demand for long videos.\n",
    "\n",
    "First, the model needs to be specified and loaded into memory. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ammico.MultimodalSummaryModel()  # load the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "To analyze the audio content from the video, `ammico` uses the [WhisperX model family](https://github.com/m-bain/whisperX) for audio transcription as [developed by OpenAI](https://arxiv.org/abs/2303.00747). The available flavors available are:\n",
    "\n",
    "1. `small`\n",
    "2. `base`\n",
    "2. `large`\n",
    "\n",
    "These models can also detect many languages and provide translations, hwowever are more accurate for longer videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = ammico.model.AudioToTextModel(model_size=\"small\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Then, we create an instance of the Python class that handles the image summary and visual question answering tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_summary_vqa = ammico.VideoSummaryDetector(\n",
    "    summary_model=model, audio_model=audio_model, subdict=video_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "After this, we can create the video captions. Depending on the length and number of videos and the hardware provided, this can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = vid_summary_vqa.analyse_videos_from_dict(analysis_type=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The results are provided in the updated dictionary. Execute the cell below to see the first frame of the video that was analyzed together with the generated caption (summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def display_first_frame(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ok, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ok:\n",
    "        raise RuntimeError(f\"Could not read first frame from {video_path}\")\n",
    "    # Convert BGR -> RGB for matplotlib\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in summary.keys():\n",
    "    # Load and display the image\n",
    "    video_path = summary[key][\"filename\"]\n",
    "    display_first_frame(video_path)\n",
    "    pprint(summary[key][\"summary\"], width=100, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# 3. Run the content analysis: Visual question answering\n",
    "\n",
    "You may also ask questions about the videos. For this, provide a list of questions and pass it to the Python class that you have instantiated above. Note that the question answering takes longer than video summarization. Ideally you would carry out both tasks together in one exection as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_questions = [\n",
    "    \"Who is in the picture?\",\n",
    "    \"Is Trump in the picture, answer with only yes or no?\",\n",
    "]  # add or replace with your own questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_and_answers = vid_summary_vqa.analyse_videos_from_dict(\n",
    "    analysis_type=\"summary_and_questions\", list_of_questions=list_of_questions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Again for your convenience we display the first frame of the videos and the answers to the questions together below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in summary_and_answers.keys():\n",
    "    # Load and display the image\n",
    "    video_path = summary_and_answers[key][\"filename\"]\n",
    "    display_first_frame(video_path)\n",
    "\n",
    "    for answer in summary_and_answers[key][\"vqa_answers\"]:\n",
    "        pprint(answer, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 4. Export the results\n",
    "\n",
    "To export the results for further processing, convert the image dictionary into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = ammico.get_dataframe(video_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Inspect the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Export the dataframe to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.to_csv(\"./data_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 5. Check out further notebooks or create your own!\n",
    "\n",
    "Congratulations! You have used `ammico` for a video analysis task. Check out [the documentation](https://github.com/ssciwr/AMMICO) for further tutorials on how to extract text from images or analyze image content! Do not hesitate to [get in touch](https://github.com/ssciwr/AMMICO/issues) with questions, feedback or any technical issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ammico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
