

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FAQ &mdash; AMMICO 0.2.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=000c92bf"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Instructions how to generate and enable a google Cloud Vision API key" href="create_API_key_link.html" />
    <link rel="prev" title="AMMICO - AI-based Media and Misinformation Content Analysis Tool" href="readme_link.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            AMMICO
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="readme_link.html">AMMICO - AI-based Media and Misinformation Content Analysis Tool</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#compatibility-problems-solving">Compatibility problems solving</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#first-install-tensorflow-https-www-tensorflow-org-install-pip">1. First, install tensorflow (https://www.tensorflow.org/install/pip)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#second-install-pytorch">2. Second, install pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#after-we-prepared-right-environment-we-can-install-the-ammico-package">3. After we prepared right environment we can install the <code class="docutils literal notranslate"><span class="pre">ammico</span></code> package</a></li>
<li class="toctree-l3"><a class="reference internal" href="#micromamba">Micromamba</a></li>
<li class="toctree-l3"><a class="reference internal" href="#windows">Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="#version-clashes-between-tensorflow-and-numpy">Version clashes between tensorflow and numpy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#what-happens-to-the-images-that-are-sent-to-google-cloud-vision">What happens to the images that are sent to google Cloud Vision?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-happens-to-the-text-that-is-sent-to-google-translate">What happens to the text that is sent to google Translate?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-happens-if-i-don-t-have-internet-access-can-i-still-use-ammico">What happens if I don’t have internet access - can I still use ammico?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-don-t-i-get-probabilistic-assessments-of-age-gender-and-race-when-running-the-emotion-detector">Why don’t I get probabilistic assessments of age, gender and race when running the Emotion Detector?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="create_API_key_link.html">Instructions how to generate and enable a google Cloud Vision API key</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html">AMMICO Demonstration Notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html#Step-0:-Create-and-set-a-Google-Cloud-Vision-Key">Step 0: Create and set a Google Cloud Vision Key</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html#Step-1:-Read-your-data-into-AMMICO">Step 1: Read your data into AMMICO</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DemoNotebook_ammico.html#The-detector-modules">The detector modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">AMMICO package modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="license_link.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMMICO</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">FAQ</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ssciwr/AMMICO/blob/main/docs/source/faq_link.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="faq">
<h1>FAQ<a class="headerlink" href="#faq" title="Link to this heading"></a></h1>
<section id="compatibility-problems-solving">
<h2>Compatibility problems solving<a class="headerlink" href="#compatibility-problems-solving" title="Link to this heading"></a></h2>
<p>Some ammico components require <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> (e.g. Emotion detector), some <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> (e.g. Summary detector). Sometimes there are compatibility problems between these two frameworks. To avoid these problems on your machines, you can prepare proper environment before installing the package (you need conda on your machine):</p>
<section id="first-install-tensorflow-https-www-tensorflow-org-install-pip">
<h3>1. First, install tensorflow (https://www.tensorflow.org/install/pip)<a class="headerlink" href="#first-install-tensorflow-https-www-tensorflow-org-install-pip" title="Link to this heading"></a></h3>
<ul>
<li><p>create a new environment with python and activate it</p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">create</span> <span class="pre">-n</span> <span class="pre">ammico_env</span> <span class="pre">python=3.10</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">ammico_env</span></code></p>
</li>
<li><p>install cudatoolkit from conda-forge</p>
<p><code class="docutils literal notranslate"> <span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">cudatoolkit=11.8.0</span></code></p>
</li>
<li><p>install nvidia-cudnn-cu11 from pip</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">nvidia-cudnn-cu11==8.6.0.163</span></code></p>
</li>
<li><p>add script that runs when conda environment <code class="docutils literal notranslate"><span class="pre">ammico_env</span></code> is activated to put the right libraries on your LD_LIBRARY_PATH</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo &#39;CUDNN_PATH=$(dirname $(python -c &quot;import nvidia.cudnn;print(nvidia.cudnn.__file__)&quot;))&#39; &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo &#39;export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$CONDA_PREFIX/lib/:$LD_LIBRARY_PATH&#39; &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
</pre></div>
</div>
</li>
<li><p>deactivate and re-activate conda environment to call script above</p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">deactivate</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">ammico_env</span> </code></p>
</li>
<li><p>install tensorflow</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">tensorflow==2.12.1</span></code></p>
</li>
</ul>
</section>
<section id="second-install-pytorch">
<h3>2. Second, install pytorch<a class="headerlink" href="#second-install-pytorch" title="Link to this heading"></a></h3>
<ul>
<li><p>install pytorch for same cuda version as above</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch</span> <span class="pre">torchvision</span> <span class="pre">torchaudio</span> <span class="pre">--index-url</span> <span class="pre">https://download.pytorch.org/whl/cu118</span></code></p>
</li>
</ul>
</section>
<section id="after-we-prepared-right-environment-we-can-install-the-ammico-package">
<h3>3. After we prepared right environment we can install the <code class="docutils literal notranslate"><span class="pre">ammico</span></code> package<a class="headerlink" href="#after-we-prepared-right-environment-we-can-install-the-ammico-package" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">ammico</span></code></p></li>
</ul>
<p>It is done.</p>
</section>
<section id="micromamba">
<h3>Micromamba<a class="headerlink" href="#micromamba" title="Link to this heading"></a></h3>
<p>If you are using micromamba you can prepare environment with just one command:</p>
<p><code class="docutils literal notranslate"><span class="pre">micromamba</span> <span class="pre">create</span> <span class="pre">--no-channel-priority</span> <span class="pre">-c</span> <span class="pre">nvidia</span> <span class="pre">-c</span> <span class="pre">pytorch</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">-n</span> <span class="pre">ammico_env</span> <span class="pre">&quot;python=3.10&quot;</span> <span class="pre">pytorch</span> <span class="pre">torchvision</span> <span class="pre">torchaudio</span> <span class="pre">pytorch-cuda</span> <span class="pre">&quot;tensorflow-gpu&lt;=2.12.3&quot;</span> <span class="pre">&quot;numpy&lt;=1.23.4&quot;</span></code></p>
</section>
<section id="windows">
<h3>Windows<a class="headerlink" href="#windows" title="Link to this heading"></a></h3>
<p>To make pycocotools work on Windows OS you may need to install <code class="docutils literal notranslate"><span class="pre">vs_BuildTools.exe</span></code> from https://visualstudio.microsoft.com/visual-cpp-build-tools/ and choose following elements:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Visual</span> <span class="pre">Studio</span> <span class="pre">extension</span> <span class="pre">development</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MSVC</span> <span class="pre">v143</span> <span class="pre">-</span> <span class="pre">VS</span> <span class="pre">2022</span> <span class="pre">C++</span> <span class="pre">x64/x86</span> <span class="pre">build</span> <span class="pre">tools</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Windows</span> <span class="pre">11</span> <span class="pre">SDK</span></code> for Windows 11 (or <code class="docutils literal notranslate"><span class="pre">Windows</span> <span class="pre">10</span> <span class="pre">SDK</span></code> for Windows 10)</p></li>
</ul>
<p>Be careful, it requires around 7 GB of disk space.</p>
<p><img alt="Screenshot 2023-06-01 165712" src="https://github.com/ssciwr/AMMICO/assets/8105097/3dfb302f-c390-46a7-a700-4e044f56c30f" /></p>
</section>
<section id="version-clashes-between-tensorflow-and-numpy">
<h3>Version clashes between tensorflow and numpy<a class="headerlink" href="#version-clashes-between-tensorflow-and-numpy" title="Link to this heading"></a></h3>
<p>Due to the <code class="docutils literal notranslate"><span class="pre">faces</span></code> module, the tensorflow version is currently fixed to at most <code class="docutils literal notranslate"><span class="pre">2.14.0</span></code>. This requires that <code class="docutils literal notranslate"><span class="pre">numpy</span></code> is restricted to <code class="docutils literal notranslate"><span class="pre">numpy==1.23.5</span></code>. If you experience issues with compatibility between tensorflow and numpy, you can try fixing the numpy version to this version.</p>
</section>
</section>
<section id="what-happens-to-the-images-that-are-sent-to-google-cloud-vision">
<h2>What happens to the images that are sent to google Cloud Vision?<a class="headerlink" href="#what-happens-to-the-images-that-are-sent-to-google-cloud-vision" title="Link to this heading"></a></h2>
<p>You have to accept the privacy statement of ammico to run this type of analyis.</p>
<p>According to the <a class="reference external" href="https://cloud.google.com/vision/docs/data-usage">google Vision API</a>, the images that are uploaded and analysed are not stored and not shared with third parties:</p>
<blockquote>
<div><p>We won’t make the content that you send available to the public. We won’t share the content with any third party. The content is only used by Google as necessary to provide the Vision API service. Vision API complies with the Cloud Data Processing Addendum.</p>
</div></blockquote>
<blockquote>
<div><p>For online (immediate response) operations (<code class="docutils literal notranslate"><span class="pre">BatchAnnotateImages</span></code> and <code class="docutils literal notranslate"><span class="pre">BatchAnnotateFiles</span></code>), the image data is processed in memory and not persisted to disk.
For asynchronous offline batch operations (<code class="docutils literal notranslate"><span class="pre">AsyncBatchAnnotateImages</span></code> and <code class="docutils literal notranslate"><span class="pre">AsyncBatchAnnotateFiles</span></code>), we must store that image for a short period of time in order to perform the analysis and return the results to you. The stored image is typically deleted right after the processing is done, with a failsafe Time to live (TTL) of a few hours.
Google also temporarily logs some metadata about your Vision API requests (such as the time the request was received and the size of the request) to improve our service and combat abuse.</p>
</div></blockquote>
</section>
<section id="what-happens-to-the-text-that-is-sent-to-google-translate">
<h2>What happens to the text that is sent to google Translate?<a class="headerlink" href="#what-happens-to-the-text-that-is-sent-to-google-translate" title="Link to this heading"></a></h2>
<p>You have to accept the privacy statement of ammico to run this type of analyis.</p>
<p>According to <a class="reference external" href="https://cloud.google.com/translate/data-usage">google Translate</a>, the data is not stored after processing and not made available to third parties:</p>
<blockquote>
<div><p>We will not make the content of the text that you send available to the public. We will not share the content with any third party. The content of the text is only used by Google as necessary to provide the Cloud Translation API service. Cloud Translation API complies with the Cloud Data Processing Addendum.</p>
</div></blockquote>
<blockquote>
<div><p>When you send text to Cloud Translation API, text is held briefly in-memory in order to perform the translation and return the results to you.</p>
</div></blockquote>
</section>
<section id="what-happens-if-i-don-t-have-internet-access-can-i-still-use-ammico">
<h2>What happens if I don’t have internet access - can I still use ammico?<a class="headerlink" href="#what-happens-if-i-don-t-have-internet-access-can-i-still-use-ammico" title="Link to this heading"></a></h2>
<p>Some features of ammico require internet access; a general answer to this question is not possible, some services require an internet connection, others can be used offline:</p>
<ul class="simple">
<li><p>Text extraction: To extract text from images, and translate the text, the data needs to be processed by google Cloud Vision and google Translate, which run in the cloud. Without internet access, text extraction and translation is not possible.</p></li>
<li><p>Image summary and query: After an initial download of the models, the <code class="docutils literal notranslate"><span class="pre">summary</span></code> module does not require an internet connection.</p></li>
<li><p>Facial expressions: After an initial download of the models, the <code class="docutils literal notranslate"><span class="pre">faces</span></code> module does not require an internet connection.</p></li>
<li><p>Multimodal search: After an initial download of the models, the <code class="docutils literal notranslate"><span class="pre">multimodal_search</span></code> module does not require an internet connection.</p></li>
<li><p>Color analysis: The <code class="docutils literal notranslate"><span class="pre">color</span></code> module does not require an internet connection.</p></li>
</ul>
</section>
<section id="why-don-t-i-get-probabilistic-assessments-of-age-gender-and-race-when-running-the-emotion-detector">
<h2>Why don’t I get probabilistic assessments of age, gender and race when running the Emotion Detector?<a class="headerlink" href="#why-don-t-i-get-probabilistic-assessments-of-age-gender-and-race-when-running-the-emotion-detector" title="Link to this heading"></a></h2>
<p>Due to well documented biases in the detection of minorities with computer vision tools, and to the ethical implications of such detection, these parts of the tool are not directly made available to users. To access these capabilities, users must first agree with a ethical disclosure statement that reads:</p>
<p>“DeepFace and RetinaFace provide wrappers to trained models in face recognition and emotion detection. Age, gender and race/ethnicity models were trained on the backbone of VGG-Face with transfer learning.</p>
<p>ETHICAL DISCLOSURE STATEMENT:</p>
<p>The Emotion Detector uses DeepFace and RetinaFace to probabilistically assess the gender, age and race of the detected faces. Such assessments may not reflect how the individuals identify. Additionally, the classification is carried out in simplistic categories and contains only the most basic classes (for example, “male” and “female” for gender, and seven non-overlapping categories for ethnicity). To access these probabilistic assessments, you must therefore agree with the following statement: “I understand the ethical and privacy implications such assessments have for the interpretation of the results and that this analysis may result in personal and possibly sensitive data, and I wish to proceed.”</p>
<p>This disclosure statement is included as a separate line of code early in the flow of the Emotion Detector. Once the user has agreed with the statement, further data analyses will also include these assessments.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="readme_link.html" class="btn btn-neutral float-left" title="AMMICO - AI-based Media and Misinformation Content Analysis Tool" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="create_API_key_link.html" class="btn btn-neutral float-right" title="Instructions how to generate and enable a google Cloud Vision API key" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Scientific Software Center, Heidelberg University.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>